{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bahPjR3cTqaN"
   },
   "source": [
    "# Import and declaring certain arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 832,
     "status": "ok",
     "timestamp": 1606383032335,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "NZqzJsCjTXu_"
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "import data\n",
    "import model\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 907,
     "status": "ok",
     "timestamp": 1606382636549,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "ogqV14bhTkAV"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Args:\n",
    "  data = './data/wikitext-2'\n",
    "  model = 'FNNModel'\n",
    "  emsize = 200\n",
    "  context_size = 8\n",
    "  nhid = 200\n",
    "  nlayers = 2\n",
    "  lr = 20\n",
    "  clip = 0.25\n",
    "  epochs = 40\n",
    "  batch_size = 8\n",
    "  bptt = 35\n",
    "  dropout = 0.2\n",
    "  tied = True\n",
    "  seed = 1111\n",
    "  cuda = True\n",
    "  log_interval = 200\n",
    "  save = 'model.pt'\n",
    "  onnx_export = ''\n",
    "  nhead = 2\n",
    "  dry_run =  True\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 806,
     "status": "ok",
     "timestamp": 1606382638140,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "x1HKajQETmPN"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os7w9It9TzTz"
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 3158,
     "status": "ok",
     "timestamp": 1606382642237,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "EVggFpzQToVN"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "corpus = data.Corpus(args.data)\n",
    "\n",
    "# Starting from sequential data, batchify arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  ..., 15,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "print(corpus.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1606382644248,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "tUDx6VSxT4Dq"
   },
   "outputs": [],
   "source": [
    "# we want to return a tensor with ascending batch \n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(-1, bsz).contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "eval_batch_size = 8\n",
    "train_data = batchify(corpus.train, args.batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    data = data.numpy()\n",
    "    for i,word in enumerate(data):\n",
    "        if i+bsz>= len(data):\n",
    "            # sentence boundary reached\n",
    "            # ignoring sentence less than 3 words\n",
    "            break\n",
    "        # convert word to id\n",
    "        x_extract = []\n",
    "        for j in range(bsz):\n",
    "            x_extract.append(data[i+j])\n",
    "        y_extract = [data[i+bsz]]\n",
    "        x.append(x_extract)\n",
    "        y.append(y_extract)\n",
    "    x = torch.IntTensor(x)\n",
    "    y=torch.IntTensor(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = batchify(corpus.train, args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 1, 0, 0, 5], dtype=torch.int32)\n",
      "tensor([ 209, 3083,   23,  147, 2234, 2112,   15,    0], dtype=torch.int32)\n",
      "tensor([0], dtype=torch.int32)\n",
      "torch.Size([2088620, 8])\n",
      "torch.Size([2088620, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x[1])\n",
    "print(x[2088619])\n",
    "print(y[2088619])\n",
    "print(x.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2088628\n"
     ]
    }
   ],
   "source": [
    "data1 = corpus.train\n",
    "print(data1.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor(5), tensor(6), tensor(2), tensor(7), tensor(8), tensor(9), tensor(3), tensor(10)]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DkpDwi-w2Wf"
   },
   "source": [
    "# Declaring data loader functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1606383300302,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "6_R5kUkpw0x_"
   },
   "outputs": [],
   "source": [
    "# get the train and target for the train values\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len]\n",
    "    target = target.narrow(1,0,1).contiguous().view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 783,
     "status": "ok",
     "timestamp": 1606382653610,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "P1VORrg9xHkO",
    "outputId": "08d4d2b6-fac8-4fe1-b4fd-252d9f91da29"
   },
   "outputs": [],
   "source": [
    "data, target = get_batch(val_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 782,
     "status": "ok",
     "timestamp": 1606382656678,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "dgp2OOLGyQz1",
    "outputId": "2535b7bb-a261-428a-e8cf-d56d489cdccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1, 32966, 32967,     1,     0,     0, 32966],\n",
      "        [32967,    13,   406,    23,    17,  6253, 19902,   310],\n",
      "        [ 1444, 19902,    13,    26,    27,  2576,    16,     9],\n",
      "        [19902,   115,    17,  4929,  4121,  9611,    13,  4854],\n",
      "        [ 2429,    37,   651,    16,    17,   989,  2429,    15],\n",
      "        [  135,    26,  2763,   258,    22,    17,  1204, 19902],\n",
      "        [   13,  1796, 25170,    15,   135,  1575,  5357,    22],\n",
      "        [   27,   958,    16,  6318,  2584,    10,  2099,    43],\n",
      "        [   19,    37,    27,  4235,    16,  1326, 13129,    10],\n",
      "        [ 2034, 17346,    19,    13,    37,  2736,    27, 14429],\n",
      "        [ 1660,    16, 19195,    15,   652,  1127,    13,    17],\n",
      "        [19908,    78,  5710,    13,   278,  2765,    61, 19902],\n",
      "        [ 5706,    61,    93, 14706,    15, 24805,   234,    43],\n",
      "        [   17,   983,    13,  2624, 15743,   321,    78,   265],\n",
      "        [   35,    17,  2623,    39,   423,    22,    27,   152],\n",
      "        [  745, 15795,   154,     9, 15927,    15, 32966, 32967],\n",
      "        [   26,    27,   953,  9839,  5833,    13,    37,    26],\n",
      "        [ 1253,  2635,   269, 19902, 14089,    13,   464,   276],\n",
      "        [   17,  4853, 11528,    15,     0,     0,     1,     1],\n",
      "        [ 2712,     1,     1,     0,     0, 32966, 32967,    26],\n",
      "        [   27,    89,     9,    13,   119,    27,   674,   958],\n",
      "        [  423,    22,  6318,  4905,    10,  2099,    43,    19],\n",
      "        [   37, 15272,   423,    22,  1305,  1839,  1326, 13129],\n",
      "        [   10,  1721,  1839,  2034, 17346,    19,    13,   260],\n",
      "        [   17, 19908,  2635,    43, 19902, 14089,    78,  2752],\n",
      "        [  831,  1839,  3068,  2584,    10,  2585,  1839,  1215],\n",
      "        [   43,    19,  1362,    37, 15781,  1178,  1179,  2376],\n",
      "        [ 1839,  1450,  1179,  1450,  4014,    10,   998,  1179],\n",
      "        [ 1305,  1839,   173,  1179,  2585, 17346,    19,    15],\n",
      "        [  625,   225, 23965,    13, 19908,   348,    27,  4160],\n",
      "        [    9,   321,   212,  1773, 18418,    43,   472,    22],\n",
      "        [ 5357,    13,    43,    27,  5342,  1184,     9,    10],\n",
      "        [    9,    19,    15,   523,  1575,  7400,   765,   478],\n",
      "        [   27,   152,    39,   452, 19908,    13,   162, 16048],\n",
      "        [   22,   274,  3955,   998,  1839,  1450,  1193,    39]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1606382658230,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "9t6gVANMyUhE",
    "outputId": "092d5a18-839e-436f-c166-03b82ad0a3f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32967,  1444, 19902,  2429,   135,    13,    27,    19,  2034,  1660,\n",
      "        19908,  5706,    17,    35,   745,    26,  1253,    17,  2712,    27,\n",
      "          423,    37,    10,    17,   831,    43,  1839,  1305,   625,     9,\n",
      "         5357,     9,    27,    22,  1803], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixljmNNiwzci"
   },
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1060,
     "status": "ok",
     "timestamp": 1606382660333,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "tws-1kW5kT39"
   },
   "outputs": [],
   "source": [
    "# creating our FNN model \n",
    "\n",
    "# Trigram Neural Network Model\n",
    "class FNNModel(nn.Module):\n",
    "    # Here context_size should be 8( because its 8 gram model ), embedding dimension is 200, h is number of hidden layers , can set it to 200\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
    "        super(FNNModel, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
    "        # Linear 2 is the decoder that returns a variable based on vocab size \n",
    "        self.linear2 = nn.Linear(h, vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # compute x': concatenation of all the 8 words in 8 gram model \n",
    "        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))\n",
    "        # compute h: tanh(W_1.x' + b)\n",
    "        out = torch.tanh(self.linear1(embeds))\n",
    "        # compute W_2.h\n",
    "        out = self.linear2(out)\n",
    "        # compute y: log_softmax(W_2.h)\n",
    "        log_probs = torch.nn.functional.log_softmax(out, dim=1)\n",
    "        # return log probabilities\n",
    "        # BATCH_SIZE x len(vocab)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xn3SErKE8VWM"
   },
   "source": [
    "# Declaring the helper functions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 697,
     "status": "ok",
     "timestamp": 1606382660833,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "5b0MbgDyqde5"
   },
   "outputs": [],
   "source": [
    "# helper function to get accuracy from log probabilities\n",
    "def get_accuracy_from_log_probs(log_probs, labels):\n",
    "    probs = torch.exp(log_probs)\n",
    "    predicted_label = torch.argmax(probs, dim=1)\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "# helper function to evaluate model on dev data\n",
    "def evaluate(model, criterion, data_source):\n",
    "    model.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dev_st = time.time()\n",
    "        for it, data_tensor in enumerate(range(0, data_source.size(0) - 1, args.bptt)):\n",
    "            context_tensor, target_tensor = get_batch(data_source, data_tensor)\n",
    "#             context_tensor = data_tensor[:,0:2]\n",
    "#             target_tensor = data_tensor[:,2]\n",
    "            context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
    "            log_probs = model(context_tensor)\n",
    "            mean_loss += criterion(log_probs, target_tensor).item()\n",
    "            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "            count += 1\n",
    "            if it % 500 == 0: \n",
    "                print(\"Dev Iteration {} complete. Mean Loss: {}; Mean Acc:{}; Time taken (s): {}\".format(it, mean_loss / count, mean_acc / count, (time.time()-dev_st)))\n",
    "                dev_st = time.time()\n",
    "\n",
    "    return mean_acc / count, mean_loss / count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPvjarqo8cis"
   },
   "source": [
    "# Training the model and evaluating on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ReILiFEdqt3X",
    "outputId": "794c39a2-9b37-4f14-a567-0db7a7a94b42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training model Epoch: 1 ---\n",
      "Training Iteration 0 of epoch 0 complete. Loss: 10.475489616394043; Acc:0.0; Time taken (s): 0.01999974250793457| ppl 35436.22\n",
      "Training Iteration 500 of epoch 0 complete. Loss: 7.927534580230713; Acc:0.05714285746216774; Time taken (s): 9.917526960372925| ppl  2772.58\n",
      "Training Iteration 1000 of epoch 0 complete. Loss: 7.623171329498291; Acc:0.05714285746216774; Time taken (s): 9.886016368865967| ppl  2045.04\n",
      "Training Iteration 1500 of epoch 0 complete. Loss: 7.381105899810791; Acc:0.08571428805589676; Time taken (s): 9.878983497619629| ppl  1605.36\n",
      "Training Iteration 2000 of epoch 0 complete. Loss: 7.380171298980713; Acc:0.17142857611179352; Time taken (s): 9.873000621795654| ppl  1603.86\n",
      "Training Iteration 2500 of epoch 0 complete. Loss: 7.771772861480713; Acc:0.08571428805589676; Time taken (s): 9.797998666763306| ppl  2372.67\n",
      "Training Iteration 3000 of epoch 0 complete. Loss: 8.250960350036621; Acc:0.17142857611179352; Time taken (s): 9.830003499984741| ppl  3831.30\n",
      "Training Iteration 3500 of epoch 0 complete. Loss: 7.2603583335876465; Acc:0.11428571492433548; Time taken (s): 9.951995849609375| ppl  1422.77\n",
      "Training Iteration 4000 of epoch 0 complete. Loss: 9.037981986999512; Acc:0.0; Time taken (s): 9.813017129898071| ppl  8416.77\n",
      "Training Iteration 4500 of epoch 0 complete. Loss: 5.0528082847595215; Acc:0.2857142984867096; Time taken (s): 9.854002475738525| ppl   156.46\n",
      "Training Iteration 5000 of epoch 0 complete. Loss: 7.245643138885498; Acc:0.05714285746216774; Time taken (s): 9.877620220184326| ppl  1401.98\n",
      "Training Iteration 5500 of epoch 0 complete. Loss: 6.8809919357299805; Acc:0.11428571492433548; Time taken (s): 9.872999668121338| ppl   973.59\n",
      "Training Iteration 6000 of epoch 0 complete. Loss: 6.885570526123047; Acc:0.17142857611179352; Time taken (s): 9.915998697280884| ppl   978.06\n",
      "Training Iteration 6500 of epoch 0 complete. Loss: 6.9877400398254395; Acc:0.17142857611179352; Time taken (s): 9.940016508102417| ppl  1083.27\n",
      "Training Iteration 7000 of epoch 0 complete. Loss: 8.43585205078125; Acc:0.1428571492433548; Time taken (s): 10.004982948303223| ppl  4609.40\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 6.082859039306641; Mean Acc:0.1428571492433548; Time taken (s): 0.001001119613647461\n",
      "Dev Iteration 500 complete. Mean Loss: 6.811961672739116; Mean Acc:0.13664096593856812; Time taken (s): 0.8370161056518555\n",
      "Epoch 0 complete! Development Accuracy: 0.13377422094345093; Development Loss: 6.860857359241398\n",
      "Best development accuracy improved from 0 to 0.13377422094345093, saving model...\n",
      "\n",
      "--- Training model Epoch: 2 ---\n",
      "Training Iteration 0 of epoch 1 complete. Loss: 6.209114074707031; Acc:0.1428571492433548; Time taken (s): 0.02199840545654297| ppl   497.26\n",
      "Training Iteration 500 of epoch 1 complete. Loss: 6.998218059539795; Acc:0.08571428805589676; Time taken (s): 9.999001026153564| ppl  1094.68\n",
      "Training Iteration 1000 of epoch 1 complete. Loss: 5.933355808258057; Acc:0.17142857611179352; Time taken (s): 10.050999641418457| ppl   377.42\n",
      "Training Iteration 1500 of epoch 1 complete. Loss: 6.225021839141846; Acc:0.08571428805589676; Time taken (s): 9.991000652313232| ppl   505.23\n",
      "Training Iteration 2000 of epoch 1 complete. Loss: 6.0789899826049805; Acc:0.11428571492433548; Time taken (s): 9.821998834609985| ppl   436.59\n",
      "Training Iteration 2500 of epoch 1 complete. Loss: 6.345491409301758; Acc:0.08571428805589676; Time taken (s): 9.81200122833252| ppl   569.92\n",
      "Training Iteration 3000 of epoch 1 complete. Loss: 6.269874572753906; Acc:0.20000000298023224; Time taken (s): 9.837998628616333| ppl   528.41\n",
      "Training Iteration 3500 of epoch 1 complete. Loss: 6.304732322692871; Acc:0.08571428805589676; Time taken (s): 10.029001235961914| ppl   547.16\n",
      "Training Iteration 4000 of epoch 1 complete. Loss: 6.888190269470215; Acc:0.05714285746216774; Time taken (s): 9.8659987449646| ppl   980.63\n",
      "Training Iteration 4500 of epoch 1 complete. Loss: 4.353446960449219; Acc:0.34285715222358704; Time taken (s): 9.875000953674316| ppl    77.75\n",
      "Training Iteration 5000 of epoch 1 complete. Loss: 5.894924163818359; Acc:0.11428571492433548; Time taken (s): 10.023999452590942| ppl   363.19\n",
      "Training Iteration 5500 of epoch 1 complete. Loss: 5.856353759765625; Acc:0.17142857611179352; Time taken (s): 10.017000913619995| ppl   349.45\n",
      "Training Iteration 6000 of epoch 1 complete. Loss: 4.700886249542236; Acc:0.22857142984867096; Time taken (s): 9.929998874664307| ppl   110.04\n",
      "Training Iteration 6500 of epoch 1 complete. Loss: 5.729377269744873; Acc:0.22857142984867096; Time taken (s): 9.860998630523682| ppl   307.78\n",
      "Training Iteration 7000 of epoch 1 complete. Loss: 6.20017671585083; Acc:0.17142857611179352; Time taken (s): 9.926999568939209| ppl   492.84\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 6.2068562507629395; Mean Acc:0.17142857611179352; Time taken (s): 0.0020020008087158203\n",
      "Dev Iteration 500 complete. Mean Loss: 6.964040998927134; Mean Acc:0.1418304592370987; Time taken (s): 0.8580007553100586\n",
      "Epoch 1 complete! Development Accuracy: 0.13887068629264832; Development Loss: 7.013174018394058\n",
      "Best development accuracy improved from 0.13377422094345093 to 0.13887068629264832, saving model...\n",
      "\n",
      "--- Training model Epoch: 3 ---\n",
      "Training Iteration 0 of epoch 2 complete. Loss: 5.70053243637085; Acc:0.1428571492433548; Time taken (s): 0.02499675750732422| ppl   299.03\n",
      "Training Iteration 500 of epoch 2 complete. Loss: 5.67883825302124; Acc:0.0; Time taken (s): 10.038000345230103| ppl   292.61\n",
      "Training Iteration 1000 of epoch 2 complete. Loss: 5.010858535766602; Acc:0.20000000298023224; Time taken (s): 9.964999198913574| ppl   150.03\n",
      "Training Iteration 1500 of epoch 2 complete. Loss: 5.45624303817749; Acc:0.11428571492433548; Time taken (s): 9.860000133514404| ppl   234.22\n",
      "Training Iteration 2000 of epoch 2 complete. Loss: 4.957587718963623; Acc:0.20000000298023224; Time taken (s): 9.85999870300293| ppl   142.25\n",
      "Training Iteration 2500 of epoch 2 complete. Loss: 5.410556316375732; Acc:0.17142857611179352; Time taken (s): 9.794000148773193| ppl   223.76\n",
      "Training Iteration 3000 of epoch 2 complete. Loss: 5.221408843994141; Acc:0.17142857611179352; Time taken (s): 9.830000877380371| ppl   185.19\n",
      "Training Iteration 3500 of epoch 2 complete. Loss: 6.211071968078613; Acc:0.11428571492433548; Time taken (s): 9.998998880386353| ppl   498.24\n",
      "Training Iteration 4000 of epoch 2 complete. Loss: 5.4985151290893555; Acc:0.05714285746216774; Time taken (s): 9.808998107910156| ppl   244.33\n",
      "Training Iteration 4500 of epoch 2 complete. Loss: 3.735889434814453; Acc:0.3142857253551483; Time taken (s): 9.907000064849854| ppl    41.93\n",
      "Training Iteration 5000 of epoch 2 complete. Loss: 4.8600077629089355; Acc:0.17142857611179352; Time taken (s): 10.14100193977356| ppl   129.03\n",
      "Training Iteration 5500 of epoch 2 complete. Loss: 4.885392665863037; Acc:0.20000000298023224; Time taken (s): 9.956998825073242| ppl   132.34\n",
      "Training Iteration 6000 of epoch 2 complete. Loss: 3.643730640411377; Acc:0.34285715222358704; Time taken (s): 9.991999387741089| ppl    38.23\n",
      "Training Iteration 6500 of epoch 2 complete. Loss: 4.957082271575928; Acc:0.2571428716182709; Time taken (s): 10.034004211425781| ppl   142.18\n",
      "Training Iteration 7000 of epoch 2 complete. Loss: 4.904257297515869; Acc:0.20000000298023224; Time taken (s): 10.021997213363647| ppl   134.86\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 6.878748416900635; Mean Acc:0.11428571492433548; Time taken (s): 0.0010013580322265625\n",
      "Dev Iteration 500 complete. Mean Loss: 7.257984504966203; Mean Acc:0.13761042058467865; Time taken (s): 0.8339998722076416\n",
      "Epoch 2 complete! Development Accuracy: 0.13402725756168365; Development Loss: 7.3037078533810025\n",
      "\n",
      "--- Training model Epoch: 4 ---\n",
      "Training Iteration 0 of epoch 3 complete. Loss: 4.933244228363037; Acc:0.17142857611179352; Time taken (s): 0.019997835159301758| ppl   138.83\n",
      "Training Iteration 500 of epoch 3 complete. Loss: 4.548850059509277; Acc:0.1428571492433548; Time taken (s): 9.819999933242798| ppl    94.52\n",
      "Training Iteration 1000 of epoch 3 complete. Loss: 4.105903625488281; Acc:0.20000000298023224; Time taken (s): 9.802998304367065| ppl    60.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Iteration 1500 of epoch 3 complete. Loss: 4.771848201751709; Acc:0.1428571492433548; Time taken (s): 9.823998212814331| ppl   118.14\n",
      "Training Iteration 2000 of epoch 3 complete. Loss: 4.3330864906311035; Acc:0.1428571492433548; Time taken (s): 9.806999683380127| ppl    76.18\n",
      "Training Iteration 2500 of epoch 3 complete. Loss: 4.1960930824279785; Acc:0.20000000298023224; Time taken (s): 9.80699872970581| ppl    66.43\n",
      "Training Iteration 3000 of epoch 3 complete. Loss: 4.652804374694824; Acc:0.17142857611179352; Time taken (s): 9.810999155044556| ppl   104.88\n",
      "Training Iteration 3500 of epoch 3 complete. Loss: 5.601353645324707; Acc:0.11428571492433548; Time taken (s): 9.811997652053833| ppl   270.79\n",
      "Training Iteration 4000 of epoch 3 complete. Loss: 4.019041061401367; Acc:0.3142857253551483; Time taken (s): 9.804999589920044| ppl    55.65\n",
      "Training Iteration 4500 of epoch 3 complete. Loss: 3.3556549549102783; Acc:0.4000000059604645; Time taken (s): 9.806999683380127| ppl    28.66\n",
      "Training Iteration 5000 of epoch 3 complete. Loss: 4.599145412445068; Acc:0.17142857611179352; Time taken (s): 9.803998947143555| ppl    99.40\n",
      "Training Iteration 5500 of epoch 3 complete. Loss: 4.080943584442139; Acc:0.2857142984867096; Time taken (s): 9.827000379562378| ppl    59.20\n",
      "Training Iteration 6000 of epoch 3 complete. Loss: 2.9520134925842285; Acc:0.4285714328289032; Time taken (s): 9.934001445770264| ppl    19.14\n",
      "Training Iteration 6500 of epoch 3 complete. Loss: 4.041059494018555; Acc:0.34285715222358704; Time taken (s): 9.89399766921997| ppl    56.89\n",
      "Training Iteration 7000 of epoch 3 complete. Loss: 3.387401819229126; Acc:0.34285715222358704; Time taken (s): 9.805999755859375| ppl    29.59\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 7.017829895019531; Mean Acc:0.1428571492433548; Time taken (s): 0.0020020008087158203\n",
      "Dev Iteration 500 complete. Mean Loss: 7.559540076646025; Mean Acc:0.13464495539665222; Time taken (s): 0.8569989204406738\n",
      "Epoch 3 complete! Development Accuracy: 0.1314198225736618; Development Loss: 7.6142164588281\n",
      "\n",
      "--- Training model Epoch: 5 ---\n",
      "Training Iteration 0 of epoch 4 complete. Loss: 4.653195858001709; Acc:0.20000000298023224; Time taken (s): 0.01999807357788086| ppl   104.92\n",
      "Training Iteration 500 of epoch 4 complete. Loss: 3.545243978500366; Acc:0.2571428716182709; Time taken (s): 10.001999855041504| ppl    34.65\n",
      "Training Iteration 1000 of epoch 4 complete. Loss: 2.9958066940307617; Acc:0.4000000059604645; Time taken (s): 9.839998483657837| ppl    20.00\n",
      "Training Iteration 1500 of epoch 4 complete. Loss: 4.388042449951172; Acc:0.2571428716182709; Time taken (s): 9.840999364852905| ppl    80.48\n",
      "Training Iteration 2000 of epoch 4 complete. Loss: 3.8012263774871826; Acc:0.20000000298023224; Time taken (s): 9.825998783111572| ppl    44.76\n",
      "Training Iteration 2500 of epoch 4 complete. Loss: 3.8198368549346924; Acc:0.34285715222358704; Time taken (s): 9.848000049591064| ppl    45.60\n",
      "Training Iteration 3000 of epoch 4 complete. Loss: 3.8409359455108643; Acc:0.34285715222358704; Time taken (s): 9.874998331069946| ppl    46.57\n",
      "Training Iteration 3500 of epoch 4 complete. Loss: 5.303832054138184; Acc:0.20000000298023224; Time taken (s): 9.803998947143555| ppl   201.11\n",
      "Training Iteration 4000 of epoch 4 complete. Loss: 3.5701727867126465; Acc:0.37142857909202576; Time taken (s): 9.80699872970581| ppl    35.52\n",
      "Training Iteration 4500 of epoch 4 complete. Loss: 3.0905468463897705; Acc:0.37142857909202576; Time taken (s): 9.83400011062622| ppl    21.99\n",
      "Training Iteration 5000 of epoch 4 complete. Loss: 3.5142881870269775; Acc:0.2857142984867096; Time taken (s): 9.81499981880188| ppl    33.59\n",
      "Training Iteration 5500 of epoch 4 complete. Loss: 3.7507712841033936; Acc:0.2857142984867096; Time taken (s): 9.9760000705719| ppl    42.55\n",
      "Training Iteration 6000 of epoch 4 complete. Loss: 2.5188138484954834; Acc:0.48571428656578064; Time taken (s): 9.933998823165894| ppl    12.41\n",
      "Training Iteration 6500 of epoch 4 complete. Loss: 3.6155340671539307; Acc:0.2857142984867096; Time taken (s): 9.835998058319092| ppl    37.17\n",
      "Training Iteration 7000 of epoch 4 complete. Loss: 3.256556987762451; Acc:0.4285714328289032; Time taken (s): 9.806999206542969| ppl    25.96\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 7.629964828491211; Mean Acc:0.11428571492433548; Time taken (s): 0.002002239227294922\n",
      "Dev Iteration 500 complete. Mean Loss: 7.905534694770615; Mean Acc:0.12848564982414246; Time taken (s): 0.8459994792938232\n",
      "Epoch 4 complete! Development Accuracy: 0.12687411904335022; Development Loss: 7.961010166489373\n"
     ]
    }
   ],
   "source": [
    "# Using negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# create model\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = FNNModel(ntokens, args.emsize, args.context_size, args.nhid).to(device)\n",
    "\n",
    "# # load it to gpu\n",
    "# model.cuda(gpu)\n",
    "\n",
    "# using ADAM optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = 2e-3)\n",
    "\n",
    "\n",
    "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
    "best_acc = 0\n",
    "best_model_path = None\n",
    "loss_values=[]\n",
    "ppl_values=[]\n",
    "for epoch in range(5):\n",
    "    st = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_ppl =0.0\n",
    "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch+1))\n",
    "    for it, data_tensor in enumerate(range(0, train_data.size(0) - 1, args.bptt)):       \n",
    "        # context_tensor = data_tensor[:,0:2]\n",
    "        # target_tensor = data_tensor[:,2]\n",
    "        context_tensor, target_tensor= get_batch(train_data, data_tensor)\n",
    "\n",
    "        context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
    "\n",
    "        # zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get log probabilities over next words\n",
    "        log_probs = model(context_tensor)\n",
    "\n",
    "        # calculate current accuracy\n",
    "        acc = get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "\n",
    "        # compute loss function\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "\n",
    "        # backward pass and update gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss =+ loss.item()\n",
    "        running_ppl =+ math.exp(loss.item())\n",
    "\n",
    "        if it % 500 == 0: \n",
    "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Acc:{}; Time taken (s): {}| ppl {:8.2f}\".format(it, epoch, loss.item(), acc, (time.time()-st),  math.exp(loss.item())))\n",
    "            st = time.time()\n",
    "    total_runs = (train_data.size(0) - 1)/(args.bptt)\n",
    "    loss_values.append(running_loss / total_runs)\n",
    "    ppl_values.append(running_ppl / total_runs)\n",
    "    print(\"\\n--- Evaluating model on dev data ---\")\n",
    "    dev_acc, dev_loss = evaluate(model, loss_function, val_data)\n",
    "    print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(epoch, dev_acc, dev_loss))\n",
    "    if dev_acc > best_acc:\n",
    "        print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "        best_acc = dev_acc\n",
    "        with open(args.save, 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        # set best model path\n",
    "\n",
    "\n",
    "        \n",
    "#         best_model_path = 'best_model_{}.dat'.format(epoch)\n",
    "#         # saving best model\n",
    "#         torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261077\n",
      "35\n",
      "7459.342857142857\n"
     ]
    }
   ],
   "source": [
    "print(train_data.size(0) - 1)\n",
    "print(args.bptt)\n",
    "testa = (train_data.size(0) - 1)/(args.bptt)\n",
    "print(testa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25f33184070>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_values)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM7fbyTICtB/rCw/ahPljs8",
   "collapsed_sections": [],
   "name": "FNNMain.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
