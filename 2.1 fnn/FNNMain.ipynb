{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bahPjR3cTqaN"
   },
   "source": [
    "# Import and declaring certain arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 832,
     "status": "ok",
     "timestamp": 1606383032335,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "NZqzJsCjTXu_"
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "import data\n",
    "import model\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 907,
     "status": "ok",
     "timestamp": 1606382636549,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "ogqV14bhTkAV"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Args:\n",
    "  data = './data/wikitext-2'\n",
    "  model = 'FNNModel'\n",
    "  emsize = 200\n",
    "  context_size = 8\n",
    "  nhid = 200\n",
    "  nlayers = 2\n",
    "  lr = 20\n",
    "  clip = 0.25\n",
    "  epochs = 40\n",
    "  batch_size = 8\n",
    "  bptt = 35\n",
    "  dropout = 0.2\n",
    "  tied = True\n",
    "  seed = 1111\n",
    "  cuda = True\n",
    "  log_interval = 200\n",
    "  save = 'model.pt'\n",
    "  onnx_export = ''\n",
    "  nhead = 2\n",
    "  dry_run =  True\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 806,
     "status": "ok",
     "timestamp": 1606382638140,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "x1HKajQETmPN"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os7w9It9TzTz"
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3158,
     "status": "ok",
     "timestamp": 1606382642237,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "EVggFpzQToVN"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "corpus = data.Corpus(args.data)\n",
    "\n",
    "# Starting from sequential data, batchify arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,     1,     2,     3,     4,     1,     0,     0,     5,     6,\n",
      "            2,     7,     8,     9,     3,    10,    11,     8,    12,    13,\n",
      "           14,    15,     2,    16,    17,    18,     7,    19,    13,    20,\n",
      "           21,    22,    23,     2,     3,     4,    24,    25,    13,    26,\n",
      "           27,    28,    29,    30,    31,    32,    33,    34,    35,    36,\n",
      "           37,    38,    39,    17,    40,    41,    15,    42,    43,    44,\n",
      "           45,    43,    25,    13,    46,    26,    17,    47,    33,    43,\n",
      "           17,     2,    48,    15,     9,    17,    49,    50,    16,    28,\n",
      "           37,    51,    30,    52,    53,    23,    54,    55,    13,    17,\n",
      "           56,    57,    58,    22,    17,    59,    33,    37,    60,    17,\n",
      "         ...,    93,   622,    22,  5002,    78,  6656,  7628,    43,   293,\n",
      "         1043,    15,  5277,  4726,   284, 23960,    26,   494,   489,   151,\n",
      "        27535,   348,  4737,    43, 17444,    39,    17,  2532,   212,  6014,\n",
      "          212,   348,   581,   721,  7990,    15,     0,  6963, 32212,    78,\n",
      "        15399,    39,  5833,    43,   246,  4854,  2490,    15,    83,  9616,\n",
      "           26, 10660,    37,    16,   159,   915,    13,   494,    46,    26,\n",
      "            9,   310,   665,   154,     9,    15,   491, 23123,   819,    46,\n",
      "         1222,   209,     9,    61,   525,  7096,    13,   564,  1362,   151,\n",
      "         1575,   209,    61,    15,  5824,   221, 11403,  1571,    13,    46,\n",
      "         1575,   808,   209,  3083,    23,   147,  2234,  2112,    15,     0,\n",
      "            0])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(edgeitems=100)\n",
    "print(corpus.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1606382644248,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "tUDx6VSxT4Dq"
   },
   "outputs": [],
   "source": [
    "# # we want to return a tensor with ascending batch \n",
    "\n",
    "# def batchify(data, bsz):\n",
    "#     # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "#     nbatch = data.size(0) // bsz\n",
    "#     # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "#     data = data.narrow(0, 0, nbatch * bsz)\n",
    "#     # Evenly divide the data across the bsz batches.\n",
    "#     data = data.view(-1, bsz).contiguous()\n",
    "#     return data.to(device)\n",
    "\n",
    "# eval_batch_size = 8\n",
    "# train_data = batchify(corpus.train, args.batch_size)\n",
    "# val_data = batchify(corpus.valid, eval_batch_size)\n",
    "# test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    value=[]\n",
    "    data = data.numpy()\n",
    "    for i,word in enumerate(data):\n",
    "        if i+bsz>= len(data):\n",
    "            # sentence boundary reached\n",
    "            # ignoring sentence less than 3 words\n",
    "            break\n",
    "        # convert word to id\n",
    "        value1 = []\n",
    "        for j in range(bsz+1):\n",
    "            value1.append(data[i+j])\n",
    "        value.append(value1)\n",
    "    value = torch.LongTensor(value)\n",
    "    return value.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 8\n",
    "train_data = batchify(corpus.train, args.context_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4,     1,     0,     0,     5],\n",
      "        [    1,     2,     3,     4,     1,     0,     0,     5,     6],\n",
      "        [    2,     3,     4,     1,     0,     0,     5,     6,     2],\n",
      "        [    3,     4,     1,     0,     0,     5,     6,     2,     7],\n",
      "        [    4,     1,     0,     0,     5,     6,     2,     7,     8],\n",
      "        [    1,     0,     0,     5,     6,     2,     7,     8,     9],\n",
      "        [    0,     0,     5,     6,     2,     7,     8,     9,     3],\n",
      "        [    0,     5,     6,     2,     7,     8,     9,     3,    10],\n",
      "        [    5,     6,     2,     7,     8,     9,     3,    10,    11],\n",
      "        [    6,     2,     7,     8,     9,     3,    10,    11,     8],\n",
      "        [    2,     7,     8,     9,     3,    10,    11,     8,    12],\n",
      "        [    7,     8,     9,     3,    10,    11,     8,    12,    13],\n",
      "        [    8,     9,     3,    10,    11,     8,    12,    13,    14],\n",
      "        [    9,     3,    10,    11,     8,    12,    13,    14,    15],\n",
      "        [    3,    10,    11,     8,    12,    13,    14,    15,     2],\n",
      "        [   10,    11,     8,    12,    13,    14,    15,     2,    16],\n",
      "        [   11,     8,    12,    13,    14,    15,     2,    16,    17],\n",
      "        [    8,    12,    13,    14,    15,     2,    16,    17,    18],\n",
      "        [   12,    13,    14,    15,     2,    16,    17,    18,     7],\n",
      "        [   13,    14,    15,     2,    16,    17,    18,     7,    19],\n",
      "        [   14,    15,     2,    16,    17,    18,     7,    19,    13],\n",
      "        [   15,     2,    16,    17,    18,     7,    19,    13,    20],\n",
      "        [    2,    16,    17,    18,     7,    19,    13,    20,    21],\n",
      "        [   16,    17,    18,     7,    19,    13,    20,    21,    22],\n",
      "        [   17,    18,     7,    19,    13,    20,    21,    22,    23],\n",
      "        [   18,     7,    19,    13,    20,    21,    22,    23,     2],\n",
      "        [    7,    19,    13,    20,    21,    22,    23,     2,     3],\n",
      "        [   19,    13,    20,    21,    22,    23,     2,     3,     4],\n",
      "        [   13,    20,    21,    22,    23,     2,     3,     4,    24],\n",
      "        [   20,    21,    22,    23,     2,     3,     4,    24,    25],\n",
      "        [   21,    22,    23,     2,     3,     4,    24,    25,    13],\n",
      "        [   22,    23,     2,     3,     4,    24,    25,    13,    26],\n",
      "        [   23,     2,     3,     4,    24,    25,    13,    26,    27],\n",
      "        [    2,     3,     4,    24,    25,    13,    26,    27,    28],\n",
      "        [    3,     4,    24,    25,    13,    26,    27,    28,    29],\n",
      "        [    4,    24,    25,    13,    26,    27,    28,    29,    30],\n",
      "        [   24,    25,    13,    26,    27,    28,    29,    30,    31],\n",
      "        [   25,    13,    26,    27,    28,    29,    30,    31,    32],\n",
      "        [   13,    26,    27,    28,    29,    30,    31,    32,    33],\n",
      "        [   26,    27,    28,    29,    30,    31,    32,    33,    34],\n",
      "        [   27,    28,    29,    30,    31,    32,    33,    34,    35],\n",
      "        [   28,    29,    30,    31,    32,    33,    34,    35,    36],\n",
      "        [   29,    30,    31,    32,    33,    34,    35,    36,    37],\n",
      "        [   30,    31,    32,    33,    34,    35,    36,    37,    38],\n",
      "        [   31,    32,    33,    34,    35,    36,    37,    38,    39],\n",
      "        [   32,    33,    34,    35,    36,    37,    38,    39,    17],\n",
      "        [   33,    34,    35,    36,    37,    38,    39,    17,    40],\n",
      "        [   34,    35,    36,    37,    38,    39,    17,    40,    41],\n",
      "        [   35,    36,    37,    38,    39,    17,    40,    41,    15],\n",
      "        [   36,    37,    38,    39,    17,    40,    41,    15,    42],\n",
      "        [   37,    38,    39,    17,    40,    41,    15,    42,    43],\n",
      "        [   38,    39,    17,    40,    41,    15,    42,    43,    44],\n",
      "        [   39,    17,    40,    41,    15,    42,    43,    44,    45],\n",
      "        [   17,    40,    41,    15,    42,    43,    44,    45,    43],\n",
      "        [   40,    41,    15,    42,    43,    44,    45,    43,    25],\n",
      "        [   41,    15,    42,    43,    44,    45,    43,    25,    13],\n",
      "        [   15,    42,    43,    44,    45,    43,    25,    13,    46],\n",
      "        [   42,    43,    44,    45,    43,    25,    13,    46,    26],\n",
      "        [   43,    44,    45,    43,    25,    13,    46,    26,    17],\n",
      "        [   44,    45,    43,    25,    13,    46,    26,    17,    47],\n",
      "        [   45,    43,    25,    13,    46,    26,    17,    47,    33],\n",
      "        [   43,    25,    13,    46,    26,    17,    47,    33,    43],\n",
      "        [   25,    13,    46,    26,    17,    47,    33,    43,    17],\n",
      "        [   13,    46,    26,    17,    47,    33,    43,    17,     2],\n",
      "        [   46,    26,    17,    47,    33,    43,    17,     2,    48],\n",
      "        [   26,    17,    47,    33,    43,    17,     2,    48,    15],\n",
      "        [   17,    47,    33,    43,    17,     2,    48,    15,     9],\n",
      "        [   47,    33,    43,    17,     2,    48,    15,     9,    17],\n",
      "        [   33,    43,    17,     2,    48,    15,     9,    17,    49],\n",
      "        [   43,    17,     2,    48,    15,     9,    17,    49,    50],\n",
      "        [   17,     2,    48,    15,     9,    17,    49,    50,    16],\n",
      "        [    2,    48,    15,     9,    17,    49,    50,    16,    28],\n",
      "        [   48,    15,     9,    17,    49,    50,    16,    28,    37],\n",
      "        [   15,     9,    17,    49,    50,    16,    28,    37,    51],\n",
      "        [    9,    17,    49,    50,    16,    28,    37,    51,    30],\n",
      "        [   17,    49,    50,    16,    28,    37,    51,    30,    52],\n",
      "        [   49,    50,    16,    28,    37,    51,    30,    52,    53],\n",
      "        [   50,    16,    28,    37,    51,    30,    52,    53,    23],\n",
      "        [   16,    28,    37,    51,    30,    52,    53,    23,    54],\n",
      "        [   28,    37,    51,    30,    52,    53,    23,    54,    55],\n",
      "        [   37,    51,    30,    52,    53,    23,    54,    55,    13],\n",
      "        [   51,    30,    52,    53,    23,    54,    55,    13,    17],\n",
      "        [   30,    52,    53,    23,    54,    55,    13,    17,    56],\n",
      "        [   52,    53,    23,    54,    55,    13,    17,    56,    57],\n",
      "        [   53,    23,    54,    55,    13,    17,    56,    57,    58],\n",
      "        [   23,    54,    55,    13,    17,    56,    57,    58,    22],\n",
      "        [   54,    55,    13,    17,    56,    57,    58,    22,    17],\n",
      "        [   55,    13,    17,    56,    57,    58,    22,    17,    59],\n",
      "        [   13,    17,    56,    57,    58,    22,    17,    59,    33],\n",
      "        [   17,    56,    57,    58,    22,    17,    59,    33,    37],\n",
      "        [   56,    57,    58,    22,    17,    59,    33,    37,    60],\n",
      "        [   57,    58,    22,    17,    59,    33,    37,    60,    17],\n",
      "        [   58,    22,    17,    59,    33,    37,    60,    17,    61],\n",
      "        [   22,    17,    59,    33,    37,    60,    17,    61,    62],\n",
      "        [   17,    59,    33,    37,    60,    17,    61,    62,    61],\n",
      "        [   59,    33,    37,    60,    17,    61,    62,    61,    13],\n",
      "        [   33,    37,    60,    17,    61,    62,    61,    13,    27],\n",
      "        [   37,    60,    17,    61,    62,    61,    13,    27,    63],\n",
      "        [   60,    17,    61,    62,    61,    13,    27,    63,    64],\n",
      "        [   17,    61,    62,    61,    13,    27,    63,    64,    65],\n",
      "        ...,\n",
      "        [  212,  2163,  5907,   247,   423,   310,  1399,   247,    93],\n",
      "        [ 2163,  5907,   247,   423,   310,  1399,   247,    93,   622],\n",
      "        [ 5907,   247,   423,   310,  1399,   247,    93,   622,    22],\n",
      "        [  247,   423,   310,  1399,   247,    93,   622,    22,  5002],\n",
      "        [  423,   310,  1399,   247,    93,   622,    22,  5002,    78],\n",
      "        [  310,  1399,   247,    93,   622,    22,  5002,    78,  6656],\n",
      "        [ 1399,   247,    93,   622,    22,  5002,    78,  6656,  7628],\n",
      "        [  247,    93,   622,    22,  5002,    78,  6656,  7628,    43],\n",
      "        [   93,   622,    22,  5002,    78,  6656,  7628,    43,   293],\n",
      "        [  622,    22,  5002,    78,  6656,  7628,    43,   293,  1043],\n",
      "        [   22,  5002,    78,  6656,  7628,    43,   293,  1043,    15],\n",
      "        [ 5002,    78,  6656,  7628,    43,   293,  1043,    15,  5277],\n",
      "        [   78,  6656,  7628,    43,   293,  1043,    15,  5277,  4726],\n",
      "        [ 6656,  7628,    43,   293,  1043,    15,  5277,  4726,   284],\n",
      "        [ 7628,    43,   293,  1043,    15,  5277,  4726,   284, 23960],\n",
      "        [   43,   293,  1043,    15,  5277,  4726,   284, 23960,    26],\n",
      "        [  293,  1043,    15,  5277,  4726,   284, 23960,    26,   494],\n",
      "        [ 1043,    15,  5277,  4726,   284, 23960,    26,   494,   489],\n",
      "        [   15,  5277,  4726,   284, 23960,    26,   494,   489,   151],\n",
      "        [ 5277,  4726,   284, 23960,    26,   494,   489,   151, 27535],\n",
      "        [ 4726,   284, 23960,    26,   494,   489,   151, 27535,   348],\n",
      "        [  284, 23960,    26,   494,   489,   151, 27535,   348,  4737],\n",
      "        [23960,    26,   494,   489,   151, 27535,   348,  4737,    43],\n",
      "        [   26,   494,   489,   151, 27535,   348,  4737,    43, 17444],\n",
      "        [  494,   489,   151, 27535,   348,  4737,    43, 17444,    39],\n",
      "        [  489,   151, 27535,   348,  4737,    43, 17444,    39,    17],\n",
      "        [  151, 27535,   348,  4737,    43, 17444,    39,    17,  2532],\n",
      "        [27535,   348,  4737,    43, 17444,    39,    17,  2532,   212],\n",
      "        [  348,  4737,    43, 17444,    39,    17,  2532,   212,  6014],\n",
      "        [ 4737,    43, 17444,    39,    17,  2532,   212,  6014,   212],\n",
      "        [   43, 17444,    39,    17,  2532,   212,  6014,   212,   348],\n",
      "        [17444,    39,    17,  2532,   212,  6014,   212,   348,   581],\n",
      "        [   39,    17,  2532,   212,  6014,   212,   348,   581,   721],\n",
      "        [   17,  2532,   212,  6014,   212,   348,   581,   721,  7990],\n",
      "        [ 2532,   212,  6014,   212,   348,   581,   721,  7990,    15],\n",
      "        [  212,  6014,   212,   348,   581,   721,  7990,    15,     0],\n",
      "        [ 6014,   212,   348,   581,   721,  7990,    15,     0,  6963],\n",
      "        [  212,   348,   581,   721,  7990,    15,     0,  6963, 32212],\n",
      "        [  348,   581,   721,  7990,    15,     0,  6963, 32212,    78],\n",
      "        [  581,   721,  7990,    15,     0,  6963, 32212,    78, 15399],\n",
      "        [  721,  7990,    15,     0,  6963, 32212,    78, 15399,    39],\n",
      "        [ 7990,    15,     0,  6963, 32212,    78, 15399,    39,  5833],\n",
      "        [   15,     0,  6963, 32212,    78, 15399,    39,  5833,    43],\n",
      "        [    0,  6963, 32212,    78, 15399,    39,  5833,    43,   246],\n",
      "        [ 6963, 32212,    78, 15399,    39,  5833,    43,   246,  4854],\n",
      "        [32212,    78, 15399,    39,  5833,    43,   246,  4854,  2490],\n",
      "        [   78, 15399,    39,  5833,    43,   246,  4854,  2490,    15],\n",
      "        [15399,    39,  5833,    43,   246,  4854,  2490,    15,    83],\n",
      "        [   39,  5833,    43,   246,  4854,  2490,    15,    83,  9616],\n",
      "        [ 5833,    43,   246,  4854,  2490,    15,    83,  9616,    26],\n",
      "        [   43,   246,  4854,  2490,    15,    83,  9616,    26, 10660],\n",
      "        [  246,  4854,  2490,    15,    83,  9616,    26, 10660,    37],\n",
      "        [ 4854,  2490,    15,    83,  9616,    26, 10660,    37,    16],\n",
      "        [ 2490,    15,    83,  9616,    26, 10660,    37,    16,   159],\n",
      "        [   15,    83,  9616,    26, 10660,    37,    16,   159,   915],\n",
      "        [   83,  9616,    26, 10660,    37,    16,   159,   915,    13],\n",
      "        [ 9616,    26, 10660,    37,    16,   159,   915,    13,   494],\n",
      "        [   26, 10660,    37,    16,   159,   915,    13,   494,    46],\n",
      "        [10660,    37,    16,   159,   915,    13,   494,    46,    26],\n",
      "        [   37,    16,   159,   915,    13,   494,    46,    26,     9],\n",
      "        [   16,   159,   915,    13,   494,    46,    26,     9,   310],\n",
      "        [  159,   915,    13,   494,    46,    26,     9,   310,   665],\n",
      "        [  915,    13,   494,    46,    26,     9,   310,   665,   154],\n",
      "        [   13,   494,    46,    26,     9,   310,   665,   154,     9],\n",
      "        [  494,    46,    26,     9,   310,   665,   154,     9,    15],\n",
      "        [   46,    26,     9,   310,   665,   154,     9,    15,   491],\n",
      "        [   26,     9,   310,   665,   154,     9,    15,   491, 23123],\n",
      "        [    9,   310,   665,   154,     9,    15,   491, 23123,   819],\n",
      "        [  310,   665,   154,     9,    15,   491, 23123,   819,    46],\n",
      "        [  665,   154,     9,    15,   491, 23123,   819,    46,  1222],\n",
      "        [  154,     9,    15,   491, 23123,   819,    46,  1222,   209],\n",
      "        [    9,    15,   491, 23123,   819,    46,  1222,   209,     9],\n",
      "        [   15,   491, 23123,   819,    46,  1222,   209,     9,    61],\n",
      "        [  491, 23123,   819,    46,  1222,   209,     9,    61,   525],\n",
      "        [23123,   819,    46,  1222,   209,     9,    61,   525,  7096],\n",
      "        [  819,    46,  1222,   209,     9,    61,   525,  7096,    13],\n",
      "        [   46,  1222,   209,     9,    61,   525,  7096,    13,   564],\n",
      "        [ 1222,   209,     9,    61,   525,  7096,    13,   564,  1362],\n",
      "        [  209,     9,    61,   525,  7096,    13,   564,  1362,   151],\n",
      "        [    9,    61,   525,  7096,    13,   564,  1362,   151,  1575],\n",
      "        [   61,   525,  7096,    13,   564,  1362,   151,  1575,   209],\n",
      "        [  525,  7096,    13,   564,  1362,   151,  1575,   209,    61],\n",
      "        [ 7096,    13,   564,  1362,   151,  1575,   209,    61,    15],\n",
      "        [   13,   564,  1362,   151,  1575,   209,    61,    15,  5824],\n",
      "        [  564,  1362,   151,  1575,   209,    61,    15,  5824,   221],\n",
      "        [ 1362,   151,  1575,   209,    61,    15,  5824,   221, 11403],\n",
      "        [  151,  1575,   209,    61,    15,  5824,   221, 11403,  1571],\n",
      "        [ 1575,   209,    61,    15,  5824,   221, 11403,  1571,    13],\n",
      "        [  209,    61,    15,  5824,   221, 11403,  1571,    13,    46],\n",
      "        [   61,    15,  5824,   221, 11403,  1571,    13,    46,  1575],\n",
      "        [   15,  5824,   221, 11403,  1571,    13,    46,  1575,   808],\n",
      "        [ 5824,   221, 11403,  1571,    13,    46,  1575,   808,   209],\n",
      "        [  221, 11403,  1571,    13,    46,  1575,   808,   209,  3083],\n",
      "        [11403,  1571,    13,    46,  1575,   808,   209,  3083,    23],\n",
      "        [ 1571,    13,    46,  1575,   808,   209,  3083,    23,   147],\n",
      "        [   13,    46,  1575,   808,   209,  3083,    23,   147,  2234],\n",
      "        [   46,  1575,   808,   209,  3083,    23,   147,  2234,  2112],\n",
      "        [ 1575,   808,   209,  3083,    23,   147,  2234,  2112,    15],\n",
      "        [  808,   209,  3083,    23,   147,  2234,  2112,    15,     0],\n",
      "        [  209,  3083,    23,   147,  2234,  2112,    15,     0,     0]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(edgeitems=100)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DkpDwi-w2Wf"
   },
   "source": [
    "# Declaring data loader functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1606383300302,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "6_R5kUkpw0x_"
   },
   "outputs": [],
   "source": [
    "# get the train and target for the train values\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len, 0:args.context_size]\n",
    "    target = source[i+1:i+1+seq_len, args.context_size-1:args.context_size]\n",
    "    target = target.narrow(1,0,1).contiguous().view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 783,
     "status": "ok",
     "timestamp": 1606382653610,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "P1VORrg9xHkO",
    "outputId": "08d4d2b6-fac8-4fe1-b4fd-252d9f91da29"
   },
   "outputs": [],
   "source": [
    "data, target = get_batch(train_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 782,
     "status": "ok",
     "timestamp": 1606382656678,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "dgp2OOLGyQz1",
    "outputId": "2535b7bb-a261-428a-e8cf-d56d489cdccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  1,  0,  0],\n",
      "        [ 1,  2,  3,  4,  1,  0,  0,  5],\n",
      "        [ 2,  3,  4,  1,  0,  0,  5,  6],\n",
      "        [ 3,  4,  1,  0,  0,  5,  6,  2],\n",
      "        [ 4,  1,  0,  0,  5,  6,  2,  7],\n",
      "        [ 1,  0,  0,  5,  6,  2,  7,  8],\n",
      "        [ 0,  0,  5,  6,  2,  7,  8,  9],\n",
      "        [ 0,  5,  6,  2,  7,  8,  9,  3],\n",
      "        [ 5,  6,  2,  7,  8,  9,  3, 10],\n",
      "        [ 6,  2,  7,  8,  9,  3, 10, 11],\n",
      "        [ 2,  7,  8,  9,  3, 10, 11,  8],\n",
      "        [ 7,  8,  9,  3, 10, 11,  8, 12],\n",
      "        [ 8,  9,  3, 10, 11,  8, 12, 13],\n",
      "        [ 9,  3, 10, 11,  8, 12, 13, 14],\n",
      "        [ 3, 10, 11,  8, 12, 13, 14, 15],\n",
      "        [10, 11,  8, 12, 13, 14, 15,  2],\n",
      "        [11,  8, 12, 13, 14, 15,  2, 16],\n",
      "        [ 8, 12, 13, 14, 15,  2, 16, 17],\n",
      "        [12, 13, 14, 15,  2, 16, 17, 18],\n",
      "        [13, 14, 15,  2, 16, 17, 18,  7],\n",
      "        [14, 15,  2, 16, 17, 18,  7, 19],\n",
      "        [15,  2, 16, 17, 18,  7, 19, 13],\n",
      "        [ 2, 16, 17, 18,  7, 19, 13, 20],\n",
      "        [16, 17, 18,  7, 19, 13, 20, 21],\n",
      "        [17, 18,  7, 19, 13, 20, 21, 22],\n",
      "        [18,  7, 19, 13, 20, 21, 22, 23],\n",
      "        [ 7, 19, 13, 20, 21, 22, 23,  2],\n",
      "        [19, 13, 20, 21, 22, 23,  2,  3],\n",
      "        [13, 20, 21, 22, 23,  2,  3,  4],\n",
      "        [20, 21, 22, 23,  2,  3,  4, 24],\n",
      "        [21, 22, 23,  2,  3,  4, 24, 25],\n",
      "        [22, 23,  2,  3,  4, 24, 25, 13],\n",
      "        [23,  2,  3,  4, 24, 25, 13, 26],\n",
      "        [ 2,  3,  4, 24, 25, 13, 26, 27],\n",
      "        [ 3,  4, 24, 25, 13, 26, 27, 28]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1606382658230,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "9t6gVANMyUhE",
    "outputId": "092d5a18-839e-436f-c166-03b82ad0a3f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  6,  2,  7,  8,  9,  3, 10, 11,  8, 12, 13, 14, 15,  2, 16, 17, 18,\n",
      "         7, 19, 13, 20, 21, 22, 23,  2,  3,  4, 24, 25, 13, 26, 27, 28, 29],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixljmNNiwzci"
   },
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 1060,
     "status": "ok",
     "timestamp": 1606382660333,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "tws-1kW5kT39"
   },
   "outputs": [],
   "source": [
    "# creating our FNN model \n",
    "\n",
    "# Trigram Neural Network Model\n",
    "class FNNModel(nn.Module):\n",
    "    # Here context_size should be 8( because its 8 gram model ), embedding dimension is 200, h is number of hidden layers , can set it to 200\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
    "        super(FNNModel, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
    "        # Linear 2 is the decoder that returns a variable based on vocab size \n",
    "        self.linear2 = nn.Linear(h, vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # compute x': concatenation of all the 8 words in 8 gram model \n",
    "        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))\n",
    "        # compute h: tanh(W_1.x' + b)\n",
    "        out = torch.tanh(self.linear1(embeds))\n",
    "        # compute W_2.h\n",
    "        out = self.linear2(out)\n",
    "        # compute y: log_softmax(W_2.h)\n",
    "        log_probs = torch.nn.functional.log_softmax(out, dim=1)\n",
    "        # return log probabilities\n",
    "        # BATCH_SIZE x len(vocab)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xn3SErKE8VWM"
   },
   "source": [
    "# Declaring the helper functions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 697,
     "status": "ok",
     "timestamp": 1606382660833,
     "user": {
      "displayName": "yong wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYJ6D3p-i8GPWEqh4u2s4VniV_lHZd6l3-eHSaqA=s64",
      "userId": "07110467073693299033"
     },
     "user_tz": -480
    },
    "id": "5b0MbgDyqde5"
   },
   "outputs": [],
   "source": [
    "# helper function to get accuracy from log probabilities\n",
    "def get_accuracy_from_log_probs(log_probs, labels):\n",
    "    probs = torch.exp(log_probs)\n",
    "    predicted_label = torch.argmax(probs, dim=1)\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "# helper function to evaluate model on dev data\n",
    "def evaluate(model, criterion, data_source):\n",
    "    model.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dev_st = time.time()\n",
    "        for it, data_tensor in enumerate(range(0, data_source.size(0) - 1, args.bptt)):\n",
    "            context_tensor, target_tensor = get_batch(data_source, data_tensor)\n",
    "#             context_tensor = data_tensor[:,0:2]\n",
    "#             target_tensor = data_tensor[:,2]\n",
    "            context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
    "            log_probs = model(context_tensor)\n",
    "            mean_loss += criterion(log_probs, target_tensor).item()\n",
    "            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "            count += 1\n",
    "            if it % 500 == 0: \n",
    "                print(\"Dev Iteration {} complete. Mean Loss: {}; Mean Acc:{}; Time taken (s): {}\".format(it, mean_loss / count, mean_acc / count, (time.time()-dev_st)))\n",
    "                dev_st = time.time()\n",
    "\n",
    "    return mean_acc / count, mean_loss / count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPvjarqo8cis"
   },
   "source": [
    "# Training the model and evaluating on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ReILiFEdqt3X",
    "outputId": "794c39a2-9b37-4f14-a567-0db7a7a94b42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training model Epoch: 1 ---\n",
      "Training Iteration 0 of epoch 0 complete. Loss: 10.338781356811523; Acc:0.0; Time taken (s): 0.01899886131286621| ppl 30908.34\n",
      "Training Iteration 500 of epoch 0 complete. Loss: 7.378848552703857; Acc:0.11428571492433548; Time taken (s): 9.862016916275024| ppl  1601.74\n",
      "Training Iteration 1000 of epoch 0 complete. Loss: 6.444242000579834; Acc:0.02857142873108387; Time taken (s): 9.839999675750732| ppl   629.07\n",
      "Training Iteration 1500 of epoch 0 complete. Loss: 8.416387557983398; Acc:0.05714285746216774; Time taken (s): 9.959983110427856| ppl  4520.54\n",
      "Training Iteration 2000 of epoch 0 complete. Loss: 6.931700706481934; Acc:0.05714285746216774; Time taken (s): 10.073001861572266| ppl  1024.23\n",
      "Training Iteration 2500 of epoch 0 complete. Loss: 6.0366387367248535; Acc:0.11428571492433548; Time taken (s): 10.003998041152954| ppl   418.48\n",
      "Training Iteration 3000 of epoch 0 complete. Loss: 6.636714458465576; Acc:0.05714285746216774; Time taken (s): 9.916999816894531| ppl   762.59\n",
      "Training Iteration 3500 of epoch 0 complete. Loss: 6.539443016052246; Acc:0.1428571492433548; Time taken (s): 10.044999599456787| ppl   691.90\n",
      "Training Iteration 4000 of epoch 0 complete. Loss: 7.476129055023193; Acc:0.02857142873108387; Time taken (s): 10.104999780654907| ppl  1765.39\n",
      "Training Iteration 4500 of epoch 0 complete. Loss: 8.471147537231445; Acc:0.02857142873108387; Time taken (s): 9.88699984550476| ppl  4774.99\n",
      "Training Iteration 5000 of epoch 0 complete. Loss: 7.096073627471924; Acc:0.08571428805589676; Time taken (s): 10.017002582550049| ppl  1207.22\n",
      "Training Iteration 5500 of epoch 0 complete. Loss: 6.434074401855469; Acc:0.22857142984867096; Time taken (s): 9.892996072769165| ppl   622.71\n",
      "Training Iteration 6000 of epoch 0 complete. Loss: 6.074251651763916; Acc:0.1428571492433548; Time taken (s): 9.82200002670288| ppl   434.52\n",
      "Training Iteration 6500 of epoch 0 complete. Loss: 5.66823148727417; Acc:0.11428571492433548; Time taken (s): 10.04699993133545| ppl   289.52\n",
      "Training Iteration 7000 of epoch 0 complete. Loss: 6.944512844085693; Acc:0.11428571492433548; Time taken (s): 9.897000074386597| ppl  1037.44\n",
      "Training Iteration 7500 of epoch 0 complete. Loss: 7.806145191192627; Acc:0.11428571492433548; Time taken (s): 9.857999086380005| ppl  2455.65\n",
      "Training Iteration 8000 of epoch 0 complete. Loss: 7.338638782501221; Acc:0.1428571492433548; Time taken (s): 9.856001138687134| ppl  1538.62\n",
      "Training Iteration 8500 of epoch 0 complete. Loss: 7.683502197265625; Acc:0.1428571492433548; Time taken (s): 9.86099910736084| ppl  2172.21\n",
      "Training Iteration 9000 of epoch 0 complete. Loss: 4.711298942565918; Acc:0.37142857909202576; Time taken (s): 9.906001091003418| ppl   111.20\n",
      "Training Iteration 9500 of epoch 0 complete. Loss: 7.777864933013916; Acc:0.08571428805589676; Time taken (s): 9.891002893447876| ppl  2387.17\n",
      "Training Iteration 10000 of epoch 0 complete. Loss: 6.166494369506836; Acc:0.20000000298023224; Time taken (s): 9.830996990203857| ppl   476.51\n",
      "Training Iteration 10500 of epoch 0 complete. Loss: 8.537213325500488; Acc:0.11428571492433548; Time taken (s): 9.871004343032837| ppl  5101.11\n",
      "Training Iteration 11000 of epoch 0 complete. Loss: 5.886425018310547; Acc:0.17142857611179352; Time taken (s): 9.804996252059937| ppl   360.12\n",
      "Training Iteration 11500 of epoch 0 complete. Loss: 7.430185317993164; Acc:0.11428571492433548; Time taken (s): 9.814002513885498| ppl  1686.12\n",
      "Training Iteration 12000 of epoch 0 complete. Loss: 6.889837741851807; Acc:0.11428571492433548; Time taken (s): 9.821983098983765| ppl   982.24\n",
      "Training Iteration 12500 of epoch 0 complete. Loss: 6.1309099197387695; Acc:0.17142857611179352; Time taken (s): 9.831002950668335| ppl   459.85\n",
      "Training Iteration 13000 of epoch 0 complete. Loss: 6.64019775390625; Acc:0.20000000298023224; Time taken (s): 9.820996284484863| ppl   765.25\n",
      "Training Iteration 13500 of epoch 0 complete. Loss: 6.8339643478393555; Acc:0.0; Time taken (s): 9.903000831604004| ppl   928.87\n",
      "Training Iteration 14000 of epoch 0 complete. Loss: 8.39720630645752; Acc:0.1428571492433548; Time taken (s): 10.034999132156372| ppl  4434.66\n",
      "Training Iteration 14500 of epoch 0 complete. Loss: 8.301942825317383; Acc:0.11428571492433548; Time taken (s): 9.999001026153564| ppl  4031.70\n",
      "Training Iteration 15000 of epoch 0 complete. Loss: 5.5641770362854; Acc:0.08571428805589676; Time taken (s): 10.103001356124878| ppl   260.91\n",
      "Training Iteration 15500 of epoch 0 complete. Loss: 7.810245037078857; Acc:0.11428571492433548; Time taken (s): 10.063998222351074| ppl  2465.73\n",
      "Training Iteration 16000 of epoch 0 complete. Loss: 6.469785690307617; Acc:0.08571428805589676; Time taken (s): 9.803000450134277| ppl   645.35\n",
      "Training Iteration 16500 of epoch 0 complete. Loss: 6.124263286590576; Acc:0.17142857611179352; Time taken (s): 9.798999547958374| ppl   456.81\n",
      "Training Iteration 17000 of epoch 0 complete. Loss: 6.123904228210449; Acc:0.17142857611179352; Time taken (s): 9.97599983215332| ppl   456.64\n",
      "Training Iteration 17500 of epoch 0 complete. Loss: 5.901968479156494; Acc:0.17142857611179352; Time taken (s): 9.807002782821655| ppl   365.76\n",
      "Training Iteration 18000 of epoch 0 complete. Loss: 6.786099910736084; Acc:0.11428571492433548; Time taken (s): 9.8040132522583| ppl   885.45\n",
      "Training Iteration 18500 of epoch 0 complete. Loss: 6.54413366317749; Acc:0.1428571492433548; Time taken (s): 9.811985492706299| ppl   695.15\n",
      "Training Iteration 19000 of epoch 0 complete. Loss: 6.087048530578613; Acc:0.17142857611179352; Time taken (s): 9.80399775505066| ppl   440.12\n",
      "Training Iteration 19500 of epoch 0 complete. Loss: 8.097393035888672; Acc:0.05714285746216774; Time taken (s): 9.806000232696533| ppl  3285.89\n",
      "Training Iteration 20000 of epoch 0 complete. Loss: 7.737959384918213; Acc:0.1428571492433548; Time taken (s): 9.81101655960083| ppl  2293.79\n",
      "Training Iteration 20500 of epoch 0 complete. Loss: 6.506949424743652; Acc:0.11428571492433548; Time taken (s): 9.814982891082764| ppl   669.78\n",
      "Training Iteration 21000 of epoch 0 complete. Loss: 6.675583362579346; Acc:0.02857142873108387; Time taken (s): 9.813999891281128| ppl   792.81\n",
      "Training Iteration 21500 of epoch 0 complete. Loss: 7.73675012588501; Acc:0.11428571492433548; Time taken (s): 9.80500054359436| ppl  2291.01\n",
      "Training Iteration 22000 of epoch 0 complete. Loss: 7.340619087219238; Acc:0.20000000298023224; Time taken (s): 9.804052591323853| ppl  1541.67\n",
      "Training Iteration 22500 of epoch 0 complete. Loss: 5.219345569610596; Acc:0.22857142984867096; Time taken (s): 9.808977127075195| ppl   184.81\n",
      "Training Iteration 23000 of epoch 0 complete. Loss: 5.71065616607666; Acc:0.17142857611179352; Time taken (s): 9.807973146438599| ppl   302.07\n",
      "Training Iteration 23500 of epoch 0 complete. Loss: 6.157524585723877; Acc:0.1428571492433548; Time taken (s): 9.8100004196167| ppl   472.26\n",
      "Training Iteration 24000 of epoch 0 complete. Loss: 5.170594692230225; Acc:0.22857142984867096; Time taken (s): 9.804013013839722| ppl   176.02\n",
      "Training Iteration 24500 of epoch 0 complete. Loss: 6.886157989501953; Acc:0.22857142984867096; Time taken (s): 9.803137063980103| ppl   978.63\n",
      "Training Iteration 25000 of epoch 0 complete. Loss: 8.514158248901367; Acc:0.20000000298023224; Time taken (s): 9.799846649169922| ppl  4984.85\n",
      "Training Iteration 25500 of epoch 0 complete. Loss: 6.880362510681152; Acc:0.08571428805589676; Time taken (s): 9.802001714706421| ppl   972.98\n",
      "Training Iteration 26000 of epoch 0 complete. Loss: 6.406256198883057; Acc:0.17142857611179352; Time taken (s): 9.80101466178894| ppl   605.62\n",
      "Training Iteration 26500 of epoch 0 complete. Loss: 4.278172969818115; Acc:0.2571428716182709; Time taken (s): 9.801983118057251| ppl    72.11\n",
      "Training Iteration 27000 of epoch 0 complete. Loss: 5.693880558013916; Acc:0.17142857611179352; Time taken (s): 9.815000057220459| ppl   297.04\n",
      "Training Iteration 27500 of epoch 0 complete. Loss: 4.172713279724121; Acc:0.22857142984867096; Time taken (s): 9.897000789642334| ppl    64.89\n",
      "Training Iteration 28000 of epoch 0 complete. Loss: 7.024652004241943; Acc:0.11428571492433548; Time taken (s): 9.819995880126953| ppl  1124.00\n",
      "Training Iteration 28500 of epoch 0 complete. Loss: 5.399044990539551; Acc:0.2571428716182709; Time taken (s): 9.803999900817871| ppl   221.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Iteration 29000 of epoch 0 complete. Loss: 7.298385143280029; Acc:0.08571428805589676; Time taken (s): 9.870999813079834| ppl  1477.91\n",
      "Training Iteration 29500 of epoch 0 complete. Loss: 5.891428470611572; Acc:0.22857142984867096; Time taken (s): 9.862000465393066| ppl   361.92\n",
      "Training Iteration 30000 of epoch 0 complete. Loss: 4.547460556030273; Acc:0.3142857253551483; Time taken (s): 9.878999471664429| ppl    94.39\n",
      "Training Iteration 30500 of epoch 0 complete. Loss: 4.9258551597595215; Acc:0.4285714328289032; Time taken (s): 9.80300760269165| ppl   137.81\n",
      "Training Iteration 31000 of epoch 0 complete. Loss: 5.226817607879639; Acc:0.3142857253551483; Time taken (s): 9.84099531173706| ppl   186.20\n",
      "Training Iteration 31500 of epoch 0 complete. Loss: 5.9762372970581055; Acc:0.1428571492433548; Time taken (s): 9.905996799468994| ppl   393.96\n",
      "Training Iteration 32000 of epoch 0 complete. Loss: 5.145524024963379; Acc:0.22857142984867096; Time taken (s): 9.887001037597656| ppl   171.66\n",
      "Training Iteration 32500 of epoch 0 complete. Loss: 7.811660289764404; Acc:0.08571428805589676; Time taken (s): 9.916998624801636| ppl  2469.23\n",
      "Training Iteration 33000 of epoch 0 complete. Loss: 6.973212242126465; Acc:0.1428571492433548; Time taken (s): 9.906001329421997| ppl  1067.65\n",
      "Training Iteration 33500 of epoch 0 complete. Loss: 5.8747382164001465; Acc:0.3142857253551483; Time taken (s): 10.023998498916626| ppl   355.93\n",
      "Training Iteration 34000 of epoch 0 complete. Loss: 8.710400581359863; Acc:0.05714285746216774; Time taken (s): 9.853999853134155| ppl  6065.67\n",
      "Training Iteration 34500 of epoch 0 complete. Loss: 6.113369464874268; Acc:0.2571428716182709; Time taken (s): 9.890016794204712| ppl   451.86\n",
      "Training Iteration 35000 of epoch 0 complete. Loss: 5.395711898803711; Acc:0.2571428716182709; Time taken (s): 9.970985889434814| ppl   220.46\n",
      "Training Iteration 35500 of epoch 0 complete. Loss: 6.200825214385986; Acc:0.22857142984867096; Time taken (s): 9.9830002784729| ppl   493.16\n",
      "Training Iteration 36000 of epoch 0 complete. Loss: 6.246692657470703; Acc:0.20000000298023224; Time taken (s): 10.01599931716919| ppl   516.30\n",
      "Training Iteration 36500 of epoch 0 complete. Loss: 6.40347957611084; Acc:0.1428571492433548; Time taken (s): 9.819017171859741| ppl   603.94\n",
      "Training Iteration 37000 of epoch 0 complete. Loss: 6.485559940338135; Acc:0.05714285746216774; Time taken (s): 9.940986156463623| ppl   655.61\n",
      "Training Iteration 37500 of epoch 0 complete. Loss: 5.5357537269592285; Acc:0.22857142984867096; Time taken (s): 9.906997442245483| ppl   253.60\n",
      "Training Iteration 38000 of epoch 0 complete. Loss: 6.830843448638916; Acc:0.22857142984867096; Time taken (s): 9.879001379013062| ppl   925.97\n",
      "Training Iteration 38500 of epoch 0 complete. Loss: 7.812198162078857; Acc:0.11428571492433548; Time taken (s): 9.810999631881714| ppl  2470.56\n",
      "Training Iteration 39000 of epoch 0 complete. Loss: 8.590902328491211; Acc:0.11428571492433548; Time taken (s): 9.831998825073242| ppl  5382.47\n",
      "Training Iteration 39500 of epoch 0 complete. Loss: 6.078357696533203; Acc:0.20000000298023224; Time taken (s): 9.85699987411499| ppl   436.31\n",
      "Training Iteration 40000 of epoch 0 complete. Loss: 4.936874866485596; Acc:0.3142857253551483; Time taken (s): 9.86200213432312| ppl   139.33\n",
      "Training Iteration 40500 of epoch 0 complete. Loss: 5.06996488571167; Acc:0.17142857611179352; Time taken (s): 9.856014251708984| ppl   159.17\n",
      "Training Iteration 41000 of epoch 0 complete. Loss: 6.019630432128906; Acc:0.3142857253551483; Time taken (s): 9.967986106872559| ppl   411.43\n",
      "Training Iteration 41500 of epoch 0 complete. Loss: 6.909238338470459; Acc:0.11428571492433548; Time taken (s): 9.848996877670288| ppl  1001.48\n",
      "Training Iteration 42000 of epoch 0 complete. Loss: 5.91689395904541; Acc:0.1428571492433548; Time taken (s): 9.864002466201782| ppl   371.26\n",
      "Training Iteration 42500 of epoch 0 complete. Loss: 7.4114508628845215; Acc:0.1428571492433548; Time taken (s): 9.829996347427368| ppl  1654.83\n",
      "Training Iteration 43000 of epoch 0 complete. Loss: 8.053123474121094; Acc:0.02857142873108387; Time taken (s): 9.854016542434692| ppl  3143.60\n",
      "Training Iteration 43500 of epoch 0 complete. Loss: 7.044140338897705; Acc:0.11428571492433548; Time taken (s): 9.829999923706055| ppl  1146.12\n",
      "Training Iteration 44000 of epoch 0 complete. Loss: 7.308746814727783; Acc:0.02857142873108387; Time taken (s): 9.912985563278198| ppl  1493.30\n",
      "Training Iteration 44500 of epoch 0 complete. Loss: 6.581788063049316; Acc:0.08571428805589676; Time taken (s): 9.878998517990112| ppl   721.83\n",
      "Training Iteration 45000 of epoch 0 complete. Loss: 5.51456356048584; Acc:0.22857142984867096; Time taken (s): 9.862002849578857| ppl   248.28\n",
      "Training Iteration 45500 of epoch 0 complete. Loss: 5.0422868728637695; Acc:0.20000000298023224; Time taken (s): 9.86299991607666| ppl   154.82\n",
      "Training Iteration 46000 of epoch 0 complete. Loss: 6.143098831176758; Acc:0.17142857611179352; Time taken (s): 9.894999742507935| ppl   465.49\n",
      "Training Iteration 46500 of epoch 0 complete. Loss: 6.0340495109558105; Acc:0.08571428805589676; Time taken (s): 9.850997924804688| ppl   417.40\n",
      "Training Iteration 47000 of epoch 0 complete. Loss: 5.647148609161377; Acc:0.1428571492433548; Time taken (s): 9.824002504348755| ppl   283.48\n",
      "Training Iteration 47500 of epoch 0 complete. Loss: 6.055139064788818; Acc:0.1428571492433548; Time taken (s): 9.842013359069824| ppl   426.30\n",
      "Training Iteration 48000 of epoch 0 complete. Loss: 6.0746612548828125; Acc:0.22857142984867096; Time taken (s): 9.859982967376709| ppl   434.70\n",
      "Training Iteration 48500 of epoch 0 complete. Loss: 5.703827857971191; Acc:0.22857142984867096; Time taken (s): 9.839999675750732| ppl   300.01\n",
      "Training Iteration 49000 of epoch 0 complete. Loss: 3.049900531768799; Acc:0.4000000059604645; Time taken (s): 9.81600022315979| ppl    21.11\n",
      "Training Iteration 49500 of epoch 0 complete. Loss: 7.622190475463867; Acc:0.02857142873108387; Time taken (s): 9.82200026512146| ppl  2043.03\n",
      "Training Iteration 50000 of epoch 0 complete. Loss: 7.684028148651123; Acc:0.08571428805589676; Time taken (s): 9.94599986076355| ppl  2173.36\n",
      "Training Iteration 50500 of epoch 0 complete. Loss: 7.011848449707031; Acc:0.17142857611179352; Time taken (s): 9.850000143051147| ppl  1109.70\n",
      "Training Iteration 51000 of epoch 0 complete. Loss: 8.375758171081543; Acc:0.02857142873108387; Time taken (s): 9.849440097808838| ppl  4340.56\n",
      "Training Iteration 51500 of epoch 0 complete. Loss: 6.822174072265625; Acc:0.1428571492433548; Time taken (s): 9.956043004989624| ppl   917.98\n",
      "Training Iteration 52000 of epoch 0 complete. Loss: 6.795235633850098; Acc:0.11428571492433548; Time taken (s): 9.84399938583374| ppl   893.58\n",
      "Training Iteration 52500 of epoch 0 complete. Loss: 6.442673206329346; Acc:0.20000000298023224; Time taken (s): 9.810993194580078| ppl   628.08\n",
      "Training Iteration 53000 of epoch 0 complete. Loss: 2.843310594558716; Acc:0.4571428596973419; Time taken (s): 9.829999446868896| ppl    17.17\n",
      "Training Iteration 53500 of epoch 0 complete. Loss: 4.708578109741211; Acc:0.2571428716182709; Time taken (s): 9.841015100479126| ppl   110.89\n",
      "Training Iteration 54000 of epoch 0 complete. Loss: 6.5467658042907715; Acc:0.17142857611179352; Time taken (s): 9.930980443954468| ppl   696.99\n",
      "Training Iteration 54500 of epoch 0 complete. Loss: 6.1750969886779785; Acc:0.2857142984867096; Time taken (s): 9.91800045967102| ppl   480.63\n",
      "Training Iteration 55000 of epoch 0 complete. Loss: 6.77503776550293; Acc:0.11428571492433548; Time taken (s): 9.805999994277954| ppl   875.71\n",
      "Training Iteration 55500 of epoch 0 complete. Loss: 7.350553035736084; Acc:0.05714285746216774; Time taken (s): 9.818000078201294| ppl  1557.06\n",
      "Training Iteration 56000 of epoch 0 complete. Loss: 6.085890769958496; Acc:0.2571428716182709; Time taken (s): 9.889017581939697| ppl   439.61\n",
      "Training Iteration 56500 of epoch 0 complete. Loss: 6.149674892425537; Acc:0.05714285746216774; Time taken (s): 9.820998907089233| ppl   468.57\n",
      "Training Iteration 57000 of epoch 0 complete. Loss: 6.2999982833862305; Acc:0.1428571492433548; Time taken (s): 9.799987316131592| ppl   544.57\n",
      "Training Iteration 57500 of epoch 0 complete. Loss: 7.5577192306518555; Acc:0.05714285746216774; Time taken (s): 9.80201506614685| ppl  1915.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Iteration 58000 of epoch 0 complete. Loss: 7.570720672607422; Acc:0.1428571492433548; Time taken (s): 9.798983097076416| ppl  1940.54\n",
      "Training Iteration 58500 of epoch 0 complete. Loss: 6.528282642364502; Acc:0.11428571492433548; Time taken (s): 9.844014406204224| ppl   684.22\n",
      "Training Iteration 59000 of epoch 0 complete. Loss: 8.430855751037598; Acc:0.08571428805589676; Time taken (s): 9.84398365020752| ppl  4586.42\n",
      "Training Iteration 59500 of epoch 0 complete. Loss: 6.518679141998291; Acc:0.1428571492433548; Time taken (s): 9.966999530792236| ppl   677.68\n",
      "\n",
      "--- Evaluating model on dev data ---\n",
      "Dev Iteration 0 complete. Mean Loss: 5.513369560241699; Mean Acc:0.20000000298023224; Time taken (s): 0.002000093460083008\n",
      "Dev Iteration 500 complete. Mean Loss: 6.812665486764051; Mean Acc:0.15118339657783508; Time taken (s): 0.8270001411437988\n",
      "Dev Iteration 1000 complete. Mean Loss: 6.810358872304072; Mean Acc:0.15190523862838745; Time taken (s): 0.8259828090667725\n",
      "Dev Iteration 1500 complete. Mean Loss: 6.81361794217597; Mean Acc:0.15227921307086945; Time taken (s): 0.8240175247192383\n",
      "Dev Iteration 2000 complete. Mean Loss: 6.833544774153184; Mean Acc:0.1512669175863266; Time taken (s): 0.8279867172241211\n",
      "Dev Iteration 2500 complete. Mean Loss: 6.804603053778946; Mean Acc:0.15327556431293488; Time taken (s): 0.8260126113891602\n",
      "Dev Iteration 3000 complete. Mean Loss: 6.848681456563632; Mean Acc:0.15068280696868896; Time taken (s): 0.8239998817443848\n",
      "Dev Iteration 3500 complete. Mean Loss: 6.851326671950104; Mean Acc:0.1500057727098465; Time taken (s): 0.8449833393096924\n",
      "Dev Iteration 4000 complete. Mean Loss: 6.855863011023367; Mean Acc:0.14953359961509705; Time taken (s): 0.8520011901855469\n",
      "Dev Iteration 4500 complete. Mean Loss: 6.846204071674207; Mean Acc:0.1497122198343277; Time taken (s): 0.834998607635498\n",
      "Dev Iteration 5000 complete. Mean Loss: 6.8456423701202604; Mean Acc:0.14992943406105042; Time taken (s): 0.8290164470672607\n",
      "Dev Iteration 5500 complete. Mean Loss: 6.873538228863479; Mean Acc:0.14838775992393494; Time taken (s): 0.8259997367858887\n",
      "Dev Iteration 6000 complete. Mean Loss: 6.883050242338036; Mean Acc:0.14780767261981964; Time taken (s): 0.8239998817443848\n",
      "Epoch 0 complete! Development Accuracy: 0.14704586565494537; Development Loss: 6.888678178926779\n",
      "Best development accuracy improved from 0 to 0.14704586565494537, saving model...\n"
     ]
    }
   ],
   "source": [
    "# Using negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# create model\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = FNNModel(ntokens, args.emsize, args.context_size, args.nhid).to(device)\n",
    "\n",
    "# # load it to gpu\n",
    "# model.cuda(gpu)\n",
    "\n",
    "# using ADAM optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = 2e-3)\n",
    "\n",
    "#define epochs\n",
    "epochs = 1\n",
    "\n",
    "\n",
    "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
    "best_acc = 0\n",
    "best_per= 9999999999999999\n",
    "best_model_path = None\n",
    "loss_values=[]\n",
    "ppl_values=[]\n",
    "for epoch in range(epochs):\n",
    "    st = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_ppl =0.0\n",
    "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch+1))\n",
    "    for it, data_tensor in enumerate(range(0, train_data.size(0) - 1, args.bptt)):       \n",
    "        # context_tensor = data_tensor[:,0:2]\n",
    "        # target_tensor = data_tensor[:,2]\n",
    "        context_tensor, target_tensor= get_batch(train_data, data_tensor)\n",
    "\n",
    "        context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
    "\n",
    "        # zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get log probabilities over next words\n",
    "        log_probs = model(context_tensor)\n",
    "\n",
    "        # calculate current accuracy\n",
    "        acc = get_accuracy_from_log_probs(log_probs, target_tensor)\n",
    "\n",
    "        # compute loss function\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "\n",
    "        # backward pass and update gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss =+ loss.item()\n",
    "        running_ppl =+ math.exp(loss.item())\n",
    "\n",
    "        if it % 10000 == 0: \n",
    "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Acc:{}; Time taken (s): {}| ppl {:8.2f}\".format(it, epoch, loss.item(), acc, (time.time()-st),  math.exp(loss.item())))\n",
    "            st = time.time()\n",
    "    total_runs = math.floor((train_data.size(0) - 1)/(args.bptt))\n",
    "    loss_values.append(running_loss / total_runs)\n",
    "    ppl_values.append(running_ppl / total_runs)\n",
    "    print(\"\\n--- Evaluating model on dev data ---\")\n",
    "    dev_acc, dev_loss = evaluate(model, loss_function, val_data)\n",
    "    dev_per = math.exp(dev_loss)\n",
    "    print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {};  Development ppl: {}\".format(epoch, dev_acc, dev_loss), dev_per)\n",
    "    if dev_per < best_per:\n",
    "        print(\"Best validation perplexity improved from {} to {}, saving model...\".format(best_per, dev_per))\n",
    "        best_per = dev_per\n",
    "        with open(args.save, 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        # set best model path\n",
    "\n",
    "\n",
    "        \n",
    "#         best_model_path = 'best_model_{}.dat'.format(epoch)\n",
    "#         # saving best model\n",
    "#         torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261077\n",
      "35\n",
      "7459.342857142857\n"
     ]
    }
   ],
   "source": [
    "print(train_data.size(0) - 1)\n",
    "print(args.bptt)\n",
    "testa = (train_data.size(0) - 1)/(args.bptt)\n",
    "print(testa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001043031141913544]\n"
     ]
    }
   ],
   "source": [
    "print(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM7fbyTICtB/rCw/ahPljs8",
   "collapsed_sections": [],
   "name": "FNNMain.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
