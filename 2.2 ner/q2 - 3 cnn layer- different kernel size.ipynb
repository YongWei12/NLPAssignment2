{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:30:49.640152Z",
     "start_time": "2020-11-20T15:30:47.618838Z"
    },
    "executionInfo": {
     "elapsed": 28819,
     "status": "ok",
     "timestamp": 1606354221991,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "C8Vw4S7ypgGs"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "import sys\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "\n",
    "import time\n",
    "import _pickle as cPickle\n",
    "\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "plt.style.use('seaborn-pastel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28812,
     "status": "ok",
     "timestamp": 1606354221993,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "Q8dn34d5pgGt",
    "outputId": "7cc58be1-74aa-4579-f7f0-fa9ae3709e97"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ol9R7K36pgGt"
   },
   "source": [
    "##### Define constants and paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:30:50.686689Z",
     "start_time": "2020-11-20T15:30:50.679703Z"
    },
    "executionInfo": {
     "elapsed": 28810,
     "status": "ok",
     "timestamp": 1606354221994,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "GAhHRhXYpgGt"
   },
   "outputs": [],
   "source": [
    "#parameters for the Model\n",
    "parameters = OrderedDict()\n",
    "parameters['train'] =  \"./data/eng.train\" #Path to train file\n",
    "parameters['dev'] =  \"./data/eng.testa\" #Path to test file\n",
    "parameters['test'] =  \"./data/eng.testb\" #Path to dev file\n",
    "parameters['tag_scheme'] = \"BIOES\" #BIO or BIOES\n",
    "parameters['lower'] = True # Boolean variable to control lowercasing of words\n",
    "parameters['zeros'] =  True # Boolean variable to control replacement of  all digits by 0 \n",
    "parameters['char_dim'] = 30 #Char embedding dimension\n",
    "parameters['word_dim'] = 100 #Token embedding dimension\n",
    "parameters['word_lstm_dim'] = 200 #Token LSTM hidden layer size\n",
    "parameters['word_bidirect'] = True #Use a bidirectional LSTM for words\n",
    "parameters['embedding_path'] =  \"./data/glove.6B.100d.txt\" #Location of pretrained embeddings\n",
    "parameters['all_emb'] = 1 #Load all embeddings\n",
    "parameters['crf'] =1 #Use CRF (0 to disable)\n",
    "parameters['dropout'] = 0.5 #Droupout on the input (0 = no dropout)\n",
    "parameters['epoch'] =  50 #Number of epochs to run\"\n",
    "parameters['weights'] = \"\" #path to Pretrained for from a previous run\n",
    "parameters['name'] = \"self-trained-model-3-layer\" # Model name\n",
    "parameters['gradient_clip']=5.0\n",
    "parameters['char_mode']=\"CNN\"\n",
    "models_path = \"./models/\" #path to saved models\n",
    "\n",
    "#GPU\n",
    "parameters['use_gpu'] = torch.cuda.is_available() #GPU Check\n",
    "use_gpu = parameters['use_gpu']\n",
    "\n",
    "parameters['reload'] =  \"./models/pre-trained-model\" \n",
    "\n",
    "#Constants\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<STOP>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:30:55.600573Z",
     "start_time": "2020-11-20T15:30:55.596583Z"
    },
    "executionInfo": {
     "elapsed": 28809,
     "status": "ok",
     "timestamp": 1606354221995,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "s7kfp-JepgGu"
   },
   "outputs": [],
   "source": [
    "#paths to files \n",
    "#To stored mapping file\n",
    "mapping_file =  './data/mapping.pkl'\n",
    "\n",
    "#To stored model\n",
    "name = parameters['name']\n",
    "model_name = models_path + name #get_name(parameters)\n",
    "\n",
    "if not os.path.exists(models_path):\n",
    "    os.makedirs(models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73B7s0W9pgGu"
   },
   "source": [
    "##### Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:30:56.615639Z",
     "start_time": "2020-11-20T15:30:56.608690Z"
    },
    "executionInfo": {
     "elapsed": 28807,
     "status": "ok",
     "timestamp": 1606354221995,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "SijUJrVZpgGu"
   },
   "outputs": [],
   "source": [
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace every digit in a string by a zero.\n",
    "    \"\"\"\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "def load_sentences(path, zeros):\n",
    "    \"\"\"\n",
    "    Load sentences. A line must contain at least a word and its tag.\n",
    "    Sentences are separated by empty lines.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:30:58.438949Z",
     "start_time": "2020-11-20T15:30:57.211827Z"
    },
    "executionInfo": {
     "elapsed": 31213,
     "status": "ok",
     "timestamp": 1606354224402,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "detvY-35pgGu"
   },
   "outputs": [],
   "source": [
    "train_sentences = load_sentences(parameters['train'], parameters['zeros'])\n",
    "test_sentences = load_sentences(parameters['test'], parameters['zeros'])\n",
    "dev_sentences = load_sentences(parameters['dev'], parameters['zeros'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlDGHFSkpgGu"
   },
   "source": [
    "##### Update tagging scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:30:59.008093Z",
     "start_time": "2020-11-20T15:30:58.995099Z"
    },
    "executionInfo": {
     "elapsed": 31214,
     "status": "ok",
     "timestamp": 1606354224405,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "EaGQ34CdpgGu"
   },
   "outputs": [],
   "source": [
    "def iob2(tags):\n",
    "    \"\"\"\n",
    "    Check that tags have a valid BIO format.\n",
    "    Tags in BIO1 format are converted to BIO2.\n",
    "    \"\"\"\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            continue\n",
    "        split = tag.split('-')\n",
    "        if len(split) != 2 or split[0] not in ['I', 'B']:\n",
    "            return False\n",
    "        if split[0] == 'B':\n",
    "            continue\n",
    "        elif i == 0 or tags[i - 1] == 'O':  # conversion IOB1 to IOB2\n",
    "            tags[i] = 'B' + tag[1:]\n",
    "        elif tags[i - 1][1:] == tag[1:]:\n",
    "            continue\n",
    "        else:  # conversion IOB1 to IOB2\n",
    "            tags[i] = 'B' + tag[1:]\n",
    "    return True\n",
    "\n",
    "def iob_iobes(tags):\n",
    "    \"\"\"\n",
    "    the function is used to convert\n",
    "    BIO -> BIOES tagging\n",
    "    \"\"\"\n",
    "    new_tags = []\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            new_tags.append(tag)\n",
    "        elif tag.split('-')[0] == 'B':\n",
    "            if i + 1 != len(tags) and \\\n",
    "               tags[i + 1].split('-')[0] == 'I':\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag.replace('B-', 'S-'))\n",
    "        elif tag.split('-')[0] == 'I':\n",
    "            if i + 1 < len(tags) and \\\n",
    "                    tags[i + 1].split('-')[0] == 'I':\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag.replace('I-', 'E-'))\n",
    "        else:\n",
    "            raise Exception('Invalid IOB format!')\n",
    "    return new_tags\n",
    "\n",
    "def update_tag_scheme(sentences, tag_scheme):\n",
    "    \"\"\"\n",
    "    Check and update sentences tagging scheme to BIO2\n",
    "    Only BIO1 and BIO2 schemes are accepted for input data.\n",
    "    \"\"\"\n",
    "    for i, s in enumerate(sentences):\n",
    "        tags = [w[-1] for w in s]\n",
    "        # Check that tags are given in the BIO format\n",
    "        if not iob2(tags):\n",
    "            s_str = '\\n'.join(' '.join(w) for w in s)\n",
    "            raise Exception('Sentences should be given in BIO format! ' +\n",
    "                            'Please check sentence %i:\\n%s' % (i, s_str))\n",
    "        if tag_scheme == 'BIOES':\n",
    "            new_tags = iob_iobes(tags)\n",
    "            for word, new_tag in zip(s, new_tags):\n",
    "                word[-1] = new_tag\n",
    "        else:\n",
    "            raise Exception('Wrong tagging scheme!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:30:59.918043Z",
     "start_time": "2020-11-20T15:30:59.731074Z"
    },
    "executionInfo": {
     "elapsed": 31212,
     "status": "ok",
     "timestamp": 1606354224405,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "o27r1i1ipgGu"
   },
   "outputs": [],
   "source": [
    "update_tag_scheme(train_sentences, parameters['tag_scheme'])\n",
    "update_tag_scheme(dev_sentences, parameters['tag_scheme'])\n",
    "update_tag_scheme(test_sentences, parameters['tag_scheme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:31:00.329718Z",
     "start_time": "2020-11-20T15:31:00.317769Z"
    },
    "executionInfo": {
     "elapsed": 31212,
     "status": "ok",
     "timestamp": 1606354224406,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "DFj-uA8PpgGu"
   },
   "outputs": [],
   "source": [
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico\n",
    "\n",
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "def word_mapping(sentences, lower):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of words, sorted by frequency.\n",
    "    \"\"\"\n",
    "    words = [[x[0].lower() if lower else x[0] for x in s] for s in sentences]\n",
    "    dico = create_dico(words)\n",
    "    dico['<UNK>'] = 10000000 #UNK tag for unknown words\n",
    "    word_to_id, id_to_word = create_mapping(dico)\n",
    "    print(\"Found %i unique words (%i in total)\" % (\n",
    "        len(dico), sum(len(x) for x in words)\n",
    "    ))\n",
    "    return dico, word_to_id, id_to_word\n",
    "\n",
    "def char_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and mapping of characters, sorted by frequency.\n",
    "    \"\"\"\n",
    "    chars = [\"\".join([w[0] for w in s]) for s in sentences]\n",
    "    dico = create_dico(chars)\n",
    "    char_to_id, id_to_char = create_mapping(dico)\n",
    "    print(\"Found %i unique characters\" % len(dico))\n",
    "    return dico, char_to_id, id_to_char\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
    "    \"\"\"\n",
    "    tags = [[word[-1] for word in s] for s in sentences]\n",
    "    dico = create_dico(tags)\n",
    "    dico[START_TAG] = -1\n",
    "    dico[STOP_TAG] = -2\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print(\"Found %i unique named entity tags\" % len(dico))\n",
    "    return dico, tag_to_id, id_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:31:01.488712Z",
     "start_time": "2020-11-20T15:31:01.038916Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32128,
     "status": "ok",
     "timestamp": 1606354225330,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "FWOWc_KXpgGu",
    "outputId": "aa4f031f-d40b-4532-ec48-84d441c57b46"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 17493 unique words (203621 in total)\nFound 75 unique characters\nFound 19 unique named entity tags\n"
     ]
    }
   ],
   "source": [
    "dico_words,word_to_id,id_to_word = word_mapping(train_sentences, parameters['lower'])\n",
    "dico_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
    "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FjO0YLWpgGu"
   },
   "source": [
    "##### Preparing final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:31:02.359816Z",
     "start_time": "2020-11-20T15:31:02.354833Z"
    },
    "executionInfo": {
     "elapsed": 32127,
     "status": "ok",
     "timestamp": 1606354225330,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "0qHh_OavpgGu"
   },
   "outputs": [],
   "source": [
    "def lower_case(x,lower=False):\n",
    "    if lower:\n",
    "        return x.lower()  \n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:31:03.631650Z",
     "start_time": "2020-11-20T15:31:02.825312Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32121,
     "status": "ok",
     "timestamp": 1606354225331,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "lgEaYnqfpgGu",
    "outputId": "6a9909d3-e6d2-4dfe-be83-7518bbb89d22"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14041 / 3250 / 3453 sentences in train / dev / test.\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(sentences, word_to_id, char_to_id, tag_to_id, lower=False):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return a list of lists of dictionaries containing:\n",
    "        - word indexes\n",
    "        - word char indexes\n",
    "        - tag indexes\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        str_words = [w[0] for w in s]\n",
    "        words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>']\n",
    "                 for w in str_words]\n",
    "        # Skip characters that are not in the training set\n",
    "        chars = [[char_to_id[c] for c in w if c in char_to_id]\n",
    "                 for w in str_words]\n",
    "        tags = [tag_to_id[w[-1]] for w in s]\n",
    "        data.append({\n",
    "            'str_words': str_words,\n",
    "            'words': words,\n",
    "            'chars': chars,\n",
    "            'tags': tags,\n",
    "        })\n",
    "    return data\n",
    "\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "dev_data = prepare_dataset(\n",
    "    dev_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "test_data = prepare_dataset(\n",
    "    test_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "print(\"{} / {} / {} sentences in train / dev / test.\".format(len(train_data), len(dev_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YfVW0ECpgGu"
   },
   "source": [
    "##### Load Word Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:31:24.961146Z",
     "start_time": "2020-11-20T15:31:04.597833Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49827,
     "status": "ok",
     "timestamp": 1606354243046,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "CVDBmvZmpgGu",
    "outputId": "c9ac02e5-e8f2-419f-ab26-10611b19cf03"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 400000 pretrained embeddings.\n"
     ]
    }
   ],
   "source": [
    "all_word_embeds = {}\n",
    "for i, line in enumerate(codecs.open(parameters['embedding_path'], 'r', 'utf-8')):\n",
    "    s = line.strip().split()\n",
    "    if len(s) == parameters['word_dim'] + 1:\n",
    "        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n",
    "\n",
    "#Intializing Word Embedding Matrix\n",
    "word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), parameters['word_dim']))\n",
    "\n",
    "for w in word_to_id:\n",
    "    if w in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w]\n",
    "    elif w.lower() in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w.lower()]\n",
    "\n",
    "print('Loaded %i pretrained embeddings.' % len(all_word_embeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv6oGZtwpgGu"
   },
   "source": [
    "##### Storing Processed Data for Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:40:56.752801Z",
     "start_time": "2020-11-20T15:40:56.702186Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50960,
     "status": "ok",
     "timestamp": 1606354244187,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "Wef9h3rHpgGu",
    "outputId": "4bab49c5-20ab-4f8c-b6e5-23e5bb48be2b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word_to_id:  17493\n"
     ]
    }
   ],
   "source": [
    "with open(mapping_file, 'wb') as f:\n",
    "    mappings = {\n",
    "        'word_to_id': word_to_id,\n",
    "        'tag_to_id': tag_to_id,\n",
    "        'char_to_id': char_to_id,\n",
    "        'parameters': parameters,\n",
    "        'word_embeds': word_embeds\n",
    "    }\n",
    "    try:\n",
    "        cPickle.dump(mappings, f)\n",
    "    except:\n",
    "        None\n",
    "\n",
    "print('word_to_id: ', len(word_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8wKaprDpgGu"
   },
   "source": [
    "Your job is to replace the LSTM-based word-level\n",
    "encoder with a CNN layer (convolutional layer followed by an optional max pooling layer).\n",
    "The CNN layer should have the same output dimensions (out_channels) as the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu6kXlgTpgGu"
   },
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzoydwT_pgGu"
   },
   "source": [
    "##### Initialization of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:51:12.364759Z",
     "start_time": "2020-11-20T15:51:12.356765Z"
    },
    "executionInfo": {
     "elapsed": 50958,
     "status": "ok",
     "timestamp": 1606354244188,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "WC2RTafDpgGu"
   },
   "outputs": [],
   "source": [
    "def init_embedding(input_embedding):\n",
    "    \"\"\"\n",
    "    Initialize embedding\n",
    "    \"\"\"\n",
    "    bias = np.sqrt(3.0 / input_embedding.size(1))\n",
    "    nn.init.uniform(input_embedding, -bias, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:54:29.109085Z",
     "start_time": "2020-11-20T15:54:29.101106Z"
    },
    "executionInfo": {
     "elapsed": 50956,
     "status": "ok",
     "timestamp": 1606354244188,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "wtk5DgyfpgGu"
   },
   "outputs": [],
   "source": [
    "def init_linear(input_linear):\n",
    "    \"\"\"\n",
    "    Initialize linear transformation\n",
    "    \"\"\"\n",
    "    bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\n",
    "    nn.init.uniform(input_linear.weight, -bias, bias)\n",
    "    if input_linear.bias is not None:\n",
    "        input_linear.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:59:43.585836Z",
     "start_time": "2020-11-20T15:59:43.571873Z"
    },
    "executionInfo": {
     "elapsed": 50954,
     "status": "ok",
     "timestamp": 1606354244189,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "RDGA_IFTpgGu"
   },
   "outputs": [],
   "source": [
    "def init_lstm(input_lstm):\n",
    "    \"\"\"\n",
    "    Initialize lstm\n",
    "    \n",
    "    PyTorch weights parameters:\n",
    "    \n",
    "        weight_ih_l[k]: the learnable input-hidden weights of the k-th layer,\n",
    "            of shape `(hidden_size * input_size)` for `k = 0`. Otherwise, the shape is\n",
    "            `(hidden_size * hidden_size)`\n",
    "            \n",
    "        weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer,\n",
    "            of shape `(hidden_size * hidden_size)`            \n",
    "    \"\"\"\n",
    "    \n",
    "    # Weights init for forward layer\n",
    "    for ind in range(0, input_lstm.num_layers):\n",
    "        \n",
    "        ## Gets the weights Tensor from our model, for the input-hidden weights in our current layer\n",
    "        weight = eval('input_lstm.weight_ih_l' + str(ind))\n",
    "        \n",
    "        # Initialize the sampling range\n",
    "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        \n",
    "        # Randomly sample from our samping range using uniform distribution and apply it to our current layer\n",
    "        nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "        \n",
    "        # Similar to above but for the hidden-hidden weights of the current layer\n",
    "        weight = eval('input_lstm.weight_hh_l' + str(ind))\n",
    "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "        \n",
    "        \n",
    "    # We do the above again, for the backward layer if we are using a bi-directional LSTM (our final model uses this)\n",
    "    if input_lstm.bidirectional:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            weight = eval('input_lstm.weight_ih_l' + str(ind) + '_reverse')\n",
    "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "            weight = eval('input_lstm.weight_hh_l' + str(ind) + '_reverse')\n",
    "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "\n",
    "    # Bias initialization steps\n",
    "    \n",
    "    # We initialize them to zero except for the forget gate bias, which is initialized to 1\n",
    "    if input_lstm.bias:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            bias = eval('input_lstm.bias_ih_l' + str(ind))\n",
    "            \n",
    "            # Initializing to zero\n",
    "            bias.data.zero_()\n",
    "            \n",
    "            # This is the range of indices for our forget gates for each LSTM cell\n",
    "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            \n",
    "            #Similar for the hidden-hidden layer\n",
    "            bias = eval('input_lstm.bias_hh_l' + str(ind))\n",
    "            bias.data.zero_()\n",
    "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            \n",
    "        # Similar to above, we do for backward layer if we are using a bi-directional LSTM \n",
    "        if input_lstm.bidirectional:\n",
    "            for ind in range(0, input_lstm.num_layers):\n",
    "                bias = eval('input_lstm.bias_ih_l' + str(ind) + '_reverse')\n",
    "                bias.data.zero_()\n",
    "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "                bias = eval('input_lstm.bias_hh_l' + str(ind) + '_reverse')\n",
    "                bias.data.zero_()\n",
    "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Es6Y4TmxpgGu"
   },
   "source": [
    "##### Evaluation schemes: Forward pass and Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5WABOpcpgGu"
   },
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T15:59:51.439259Z",
     "start_time": "2020-11-20T15:59:51.431280Z"
    },
    "executionInfo": {
     "elapsed": 50951,
     "status": "ok",
     "timestamp": 1606354244189,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "TI_4EpREpgGu"
   },
   "outputs": [],
   "source": [
    "def log_sum_exp(vec):\n",
    "    '''\n",
    "    This function calculates the score explained above for the forward algorithm\n",
    "    vec 2D: 1 * tagset_size\n",
    "    '''\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "    \n",
    "def argmax(vec):\n",
    "    '''\n",
    "    This function returns the max index in a vector\n",
    "    '''\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return to_scalar(idx)\n",
    "\n",
    "def to_scalar(var):\n",
    "    '''\n",
    "    Function to convert pytorch tensor to a scalar\n",
    "    '''\n",
    "    return var.view(-1).data.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:00:01.897093Z",
     "start_time": "2020-11-20T16:00:01.889142Z"
    },
    "executionInfo": {
     "elapsed": 50949,
     "status": "ok",
     "timestamp": 1606354244189,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "LEjOTa1dpgGu"
   },
   "outputs": [],
   "source": [
    "def score_sentences(self, feats, tags):\n",
    "    # tags is ground_truth, a list of ints, length is len(sentence)\n",
    "    # feats is a 2D tensor, len(sentence) * tagset_size\n",
    "    r = torch.LongTensor(range(feats.size()[0]))\n",
    "    if self.use_gpu:\n",
    "        r = r.cuda()\n",
    "        pad_start_tags = torch.cat([torch.cuda.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
    "        pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
    "    else:\n",
    "        pad_start_tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
    "        pad_stop_tags = torch.cat([tags, torch.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
    "\n",
    "    score = torch.sum(self.transitions[pad_stop_tags, pad_start_tags]) + torch.sum(feats[r, tags])\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pkh3HFF_pgGu"
   },
   "source": [
    "##### Implementation of Forward Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:00:12.389264Z",
     "start_time": "2020-11-20T16:00:12.380281Z"
    },
    "executionInfo": {
     "elapsed": 50949,
     "status": "ok",
     "timestamp": 1606354244190,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "9Eyxi4xqpgGu"
   },
   "outputs": [],
   "source": [
    "def forward_alg(self, feats):\n",
    "    '''\n",
    "    This function performs the forward algorithm explained above\n",
    "    '''\n",
    "    # calculate in log domain\n",
    "    # feats is len(sentence) * tagset_size\n",
    "    # initialize alpha with a Tensor with values all equal to -10000.\n",
    "    \n",
    "    # Do the forward algorithm to compute the partition function\n",
    "    init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "    \n",
    "    # START_TAG has all of the score.\n",
    "    init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "    \n",
    "    # Wrap in a variable so that we will get automatic backprop\n",
    "    forward_var = autograd.Variable(init_alphas)\n",
    "    if self.use_gpu:\n",
    "        forward_var = forward_var.cuda()\n",
    "        \n",
    "    # Iterate through the sentence\n",
    "    for feat in feats:\n",
    "        # broadcast the emission score: it is the same regardless of\n",
    "        # the previous tag\n",
    "        emit_score = feat.view(-1, 1)\n",
    "        \n",
    "        # the ith entry of trans_score is the score of transitioning to\n",
    "        # next_tag from i\n",
    "        tag_var = forward_var + self.transitions + emit_score\n",
    "        \n",
    "        # The ith entry of next_tag_var is the value for the\n",
    "        # edge (i -> next_tag) before we do log-sum-exp\n",
    "        max_tag_var, _ = torch.max(tag_var, dim=1)\n",
    "        \n",
    "        # The forward variable for this tag is log-sum-exp of all the\n",
    "        # scores.\n",
    "        tag_var = tag_var - max_tag_var.view(-1, 1)\n",
    "        \n",
    "        # Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "        forward_var = max_tag_var + torch.log(torch.sum(torch.exp(tag_var), dim=1)).view(1, -1) # ).view(1, -1)\n",
    "    terminal_var = (forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]).view(1, -1)\n",
    "    alpha = log_sum_exp(terminal_var)\n",
    "    # Z(x)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvB7k5zApgGu"
   },
   "source": [
    "##### Implementation of Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:00:16.904048Z",
     "start_time": "2020-11-20T16:00:16.891084Z"
    },
    "executionInfo": {
     "elapsed": 50947,
     "status": "ok",
     "timestamp": 1606354244190,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "KqDk5YcBpgGu"
   },
   "outputs": [],
   "source": [
    "def viterbi_algo(self, feats):\n",
    "    '''\n",
    "    In this function, we implement the viterbi algorithm explained above.\n",
    "    A Dynamic programming based approach to find the best tag sequence\n",
    "    '''\n",
    "    backpointers = []\n",
    "    # analogous to forward\n",
    "    \n",
    "    # Initialize the viterbi variables in log space\n",
    "    init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "    init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "    \n",
    "    # forward_var at step i holds the viterbi variables for step i-1\n",
    "    forward_var = Variable(init_vvars)\n",
    "    if self.use_gpu:\n",
    "        forward_var = forward_var.cuda()\n",
    "    for feat in feats:\n",
    "        next_tag_var = forward_var.view(1, -1).expand(self.tagset_size, self.tagset_size) + self.transitions\n",
    "        _, bptrs_t = torch.max(next_tag_var, dim=1)\n",
    "        bptrs_t = bptrs_t.squeeze().data.cpu().numpy() # holds the backpointers for this step\n",
    "        next_tag_var = next_tag_var.data.cpu().numpy() \n",
    "        viterbivars_t = next_tag_var[range(len(bptrs_t)), bptrs_t] # holds the viterbi variables for this step\n",
    "        viterbivars_t = Variable(torch.FloatTensor(viterbivars_t))\n",
    "        if self.use_gpu:\n",
    "            viterbivars_t = viterbivars_t.cuda()\n",
    "            \n",
    "        # Now add in the emission scores, and assign forward_var to the set\n",
    "        # of viterbi variables we just computed\n",
    "        forward_var = viterbivars_t + feat\n",
    "        backpointers.append(bptrs_t)\n",
    "\n",
    "    # Transition to STOP_TAG\n",
    "    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "    terminal_var.data[self.tag_to_ix[STOP_TAG]] = -10000.\n",
    "    terminal_var.data[self.tag_to_ix[START_TAG]] = -10000.\n",
    "    best_tag_id = argmax(terminal_var.unsqueeze(0))\n",
    "    path_score = terminal_var[best_tag_id]\n",
    "    \n",
    "    # Follow the back pointers to decode the best path.\n",
    "    best_path = [best_tag_id]\n",
    "    for bptrs_t in reversed(backpointers):\n",
    "        best_tag_id = bptrs_t[best_tag_id]\n",
    "        best_path.append(best_tag_id)\n",
    "        \n",
    "    # Pop off the start tag (we dont want to return that to the caller)\n",
    "    start = best_path.pop()\n",
    "    assert start == self.tag_to_ix[START_TAG] # Sanity check\n",
    "    best_path.reverse()\n",
    "    return path_score, best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:00:19.161218Z",
     "start_time": "2020-11-20T16:00:19.154243Z"
    },
    "executionInfo": {
     "elapsed": 50946,
     "status": "ok",
     "timestamp": 1606354244191,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "mBoJqHGcpgGu"
   },
   "outputs": [],
   "source": [
    "def forward_calc(self, sentence, chars, chars2_length, d):\n",
    "    \n",
    "    '''\n",
    "    The function calls viterbi decode and generates the \n",
    "    most probable sequence of tags for the sentence\n",
    "    '''\n",
    "    \n",
    "    # Get the emission scores from the BiLSTM\n",
    "    feats = self._get_lstm_features(sentence, chars, chars2_length, d)\n",
    "    # viterbi to get tag_seq\n",
    "    \n",
    "    # Find the best path, given the features.\n",
    "    if self.use_crf:\n",
    "        score, tag_seq = self.viterbi_decode(feats)\n",
    "    else:\n",
    "        score, tag_seq = torch.max(feats, 1)\n",
    "        tag_seq = list(tag_seq.cpu().data)\n",
    "\n",
    "    return score, tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0tkeE4ZpgGu"
   },
   "source": [
    "### Details fo the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZfTNBPzpgGu"
   },
   "source": [
    "##### Main Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXUyb7HMpgGu"
   },
   "source": [
    "The get_lstm_features function returns the LSTM's tag vectors. The function performs all the steps mentioned above for the model.\n",
    "\n",
    "Steps:\n",
    "1. It takes in characters, converts them to embeddings using our character CNN.\n",
    "2. We concat Character Embeeding with glove vectors, use this as features that we feed to Bidirectional-LSTM. \n",
    "3. The Bidirectional-LSTM generates outputs based on these set of features.\n",
    "4. The output are passed through a linear layer to convert to tag space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:00:22.182066Z",
     "start_time": "2020-11-20T16:00:22.166139Z"
    },
    "executionInfo": {
     "elapsed": 50945,
     "status": "ok",
     "timestamp": 1606354244191,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "CkjxwS8IpgGu"
   },
   "outputs": [],
   "source": [
    "def get_lstm_features(self, sentence, chars2, chars2_length, d):\n",
    "    \n",
    "    if self.char_mode == 'LSTM':\n",
    "        \n",
    "            chars_embeds = self.char_embeds(chars2).transpose(0, 1)\n",
    "            \n",
    "            packed = torch.nn.utils.rnn.pack_padded_sequence(chars_embeds, chars2_length)\n",
    "            \n",
    "            lstm_out, _ = self.char_lstm(packed)\n",
    "            \n",
    "            outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out)\n",
    "            \n",
    "            outputs = outputs.transpose(0, 1)\n",
    "            \n",
    "            chars_embeds_temp = Variable(torch.FloatTensor(torch.zeros((outputs.size(0), outputs.size(2)))))\n",
    "            \n",
    "            if self.use_gpu:\n",
    "                chars_embeds_temp = chars_embeds_temp.cuda()\n",
    "            \n",
    "            for i, index in enumerate(output_lengths):\n",
    "                chars_embeds_temp[i] = torch.cat((outputs[i, index-1, :self.char_lstm_dim], outputs[i, 0, self.char_lstm_dim:]))\n",
    "            \n",
    "            chars_embeds = chars_embeds_temp.clone()\n",
    "            \n",
    "            for i in range(chars_embeds.size(0)):\n",
    "                chars_embeds[d[i]] = chars_embeds_temp[i]\n",
    "    \n",
    "    \n",
    "    if self.char_mode == 'CNN':\n",
    "        chars_embeds = self.char_embeds(chars2).unsqueeze(1)\n",
    "\n",
    "        ## Creating Character level representation using Convolutional Neural Netowrk\n",
    "        ## followed by a Maxpooling Layer\n",
    "        chars_cnn_out3 = self.char_cnn3(chars_embeds)\n",
    "        chars_embeds = nn.functional.max_pool2d(chars_cnn_out3,\n",
    "                                             kernel_size=(chars_cnn_out3.size(2), 1)).view(chars_cnn_out3.size(0), self.out_channels)\n",
    "\n",
    "        ## Loading word embeddings\n",
    "    embeds = self.word_embeds(sentence)\n",
    "\n",
    "    ## We concatenate the word embeddings and the character level representation\n",
    "    ## to create unified representation for each word\n",
    "    embeds = torch.cat((embeds, chars_embeds), 1)\n",
    "\n",
    "    embeds = embeds.unsqueeze(1)\n",
    "\n",
    "    ## Dropout on the unified embeddings\n",
    "    embeds = self.dropout(embeds)\n",
    "\n",
    "#     ## Word lstm\n",
    "#     ## Takes words as input and generates a output at each step\n",
    "#     lstm_out, _ = self.lstm(embeds)\n",
    "\n",
    "#     ## Reshaping the outputs from the lstm layer\n",
    "#     lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)\n",
    "\n",
    "#     ## Dropout on the lstm output\n",
    "#     lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "#     ## Linear layer converts the ouput vectors to tag space\n",
    "#     lstm_feats = self.hidden2tag(lstm_out)\n",
    "    \n",
    "    #word CNN\n",
    "\n",
    "    cnn_out = self.conv(embeds)\n",
    "    \n",
    "    cnn_out = self.conv1(cnn_out) \n",
    "\n",
    "    cnn_out = self.conv2(cnn_out) \n",
    "    \n",
    "    cnn_out = cnn_out.view(len(sentence),  self.hidden_dim*2)\n",
    "    \n",
    "    cnn_feats = self.hidden2tag(cnn_out)\n",
    "    \n",
    "    return cnn_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_PRheqGpgGu"
   },
   "source": [
    "##### Funtion for Negative log likelihood calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:00:24.720041Z",
     "start_time": "2020-11-20T16:00:24.710068Z"
    },
    "executionInfo": {
     "elapsed": 50943,
     "status": "ok",
     "timestamp": 1606354244191,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "wwwLDi3cpgGu"
   },
   "outputs": [],
   "source": [
    "def get_neg_log_likelihood(self, sentence, tags, chars2, chars2_length, d):\n",
    "    # sentence, tags is a list of ints\n",
    "    # features is a 2D tensor, len(sentence) * self.tagset_size\n",
    "    feats = self._get_lstm_features(sentence, chars2, chars2_length, d)\n",
    "\n",
    "    if self.use_crf:\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "    else:\n",
    "        tags = Variable(tags)\n",
    "        scores = nn.functional.cross_entropy(feats, tags)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3vxPW8JpgGv"
   },
   "source": [
    "##### Main Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:09:58.149221Z",
     "start_time": "2020-11-20T16:09:58.132266Z"
    },
    "executionInfo": {
     "elapsed": 50944,
     "status": "ok",
     "timestamp": 1606354244193,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "_TAXtdAxpgGv"
   },
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim,\n",
    "                 char_to_ix=None, pre_word_embeds=None, char_out_dimension=25,char_embedding_dim=25, use_gpu=False\n",
    "                 , use_crf=True, char_mode='CNN'): \n",
    "        '''\n",
    "        Input parameters:\n",
    "                \n",
    "                vocab_size= Size of vocabulary (int)\n",
    "                tag_to_ix = Dictionary that maps NER tags to indices\n",
    "                embedding_dim = Dimension of word embeddings (int)\n",
    "                hidden_dim = The hidden dimension of the LSTM layer (int)\n",
    "                char_to_ix = Dictionary that maps characters to indices\n",
    "                pre_word_embeds = Numpy array which provides mapping from word embeddings to word indices\n",
    "                char_out_dimension = Output dimension from the CNN encoder for character\n",
    "                char_embedding_dim = Dimension of the character embeddings\n",
    "                use_gpu = defines availability of GPU, \n",
    "                    when True: CUDA function calls are made\n",
    "                    else: Normal CPU function calls are made\n",
    "                use_crf = parameter which decides if you want to use the CRF layer for output decoding\n",
    "        '''\n",
    "        \n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        \n",
    "        #parameter initialization for the model\n",
    "        self.use_gpu = use_gpu\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.use_crf = use_crf\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.out_channels = char_out_dimension\n",
    "        self.char_mode = char_mode\n",
    "\n",
    "        if char_embedding_dim is not None:\n",
    "            self.char_embedding_dim = char_embedding_dim\n",
    "            \n",
    "            #Initializing the character embedding layer\n",
    "            self.char_embeds = nn.Embedding(len(char_to_ix), char_embedding_dim)\n",
    "            init_embedding(self.char_embeds.weight)\n",
    "            \n",
    "            #Performing LSTM encoding on the character embeddings\n",
    "            if self.char_mode == 'LSTM':\n",
    "                self.char_lstm = nn.LSTM(char_embedding_dim, char_lstm_dim, num_layers=1, bidirectional=True)\n",
    "                init_lstm(self.char_lstm)\n",
    "                \n",
    "            #Performing CNN encoding on the character embeddings\n",
    "            if self.char_mode == 'CNN':\n",
    "                self.char_cnn3 = nn.Conv2d(in_channels=1, out_channels=self.out_channels, kernel_size=(3, char_embedding_dim), padding=(2,0))\n",
    "\n",
    "        #Creating Embedding layer with dimension of ( number of words * dimension of each word)\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pre_word_embeds is not None:\n",
    "            #Initializes the word embeddings with pretrained word embeddings\n",
    "            self.pre_word_embeds = True\n",
    "            self.word_embeds.weight = nn.Parameter(torch.FloatTensor(pre_word_embeds))\n",
    "        else:\n",
    "            self.pre_word_embeds = False\n",
    "    \n",
    "        #Initializing the dropout layer, with dropout specificed in parameters\n",
    "        self.dropout = nn.Dropout(parameters['dropout'])\n",
    "        \n",
    "        #Lstm Layer:\n",
    "        #input dimension: word embedding dimension + character level representation\n",
    "        #bidirectional=True, specifies that we are using the bidirectional LSTM\n",
    "        if self.char_mode == 'LSTM':\n",
    "            self.lstm = nn.LSTM(embedding_dim+char_lstm_dim*2, hidden_dim, bidirectional=True)\n",
    "        if self.char_mode == 'CNN':\n",
    "#todo:            #replacing lstm layer with CNN layer\n",
    "\n",
    "            self.conv = nn.Conv1d(in_channels=1, out_channels=hidden_dim*2, kernel_size=(self.out_channels))\n",
    "            self.conv1 = nn.Conv1d(in_channels=hidden_dim*2, out_channels=hidden_dim*2, kernel_size=(embedding_dim))\n",
    "            self.conv2 = nn.Conv1d(in_channels=hidden_dim*2, out_channels=hidden_dim*2, kernel_size=(2))\n",
    "#             self.lstm = nn.LSTM(embedding_dim+self.out_channels, hidden_dim, bidirectional=True)\n",
    "\n",
    "        \"\"\"\n",
    "        #Initializing the lstm layer using predefined function for initialization\n",
    "        init_lstm(self.lstm)\"\"\"\n",
    "        \n",
    "        # Linear layer which maps the output of the bidirectional LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
    "        \n",
    "        #Initializing the linear layer using predefined function for initialization\n",
    "        init_linear(self.hidden2tag)\n",
    "        \n",
    "\n",
    "        if self.use_crf:\n",
    "            # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
    "            # Matrix has a dimension of (total number of tags * total number of tags)\n",
    "            self.transitions = nn.Parameter(\n",
    "                torch.zeros(self.tagset_size, self.tagset_size))\n",
    "            \n",
    "            # These two statements enforce the constraint that we never transfer\n",
    "            # to the start tag and we never transfer from the stop tag\n",
    "            self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "            self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "    #assigning the functions, which we have defined earlier\n",
    "    _score_sentence = score_sentences\n",
    "    _get_lstm_features = get_lstm_features\n",
    "    _forward_alg = forward_alg\n",
    "    viterbi_decode = viterbi_algo\n",
    "    neg_log_likelihood = get_neg_log_likelihood\n",
    "    forward = forward_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:10:00.955406Z",
     "start_time": "2020-11-20T16:10:00.927482Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62242,
     "status": "ok",
     "timestamp": 1606354255498,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "iglKPSkqpgGv",
    "outputId": "db527974-cf65-4bd8-98dd-dbc33ce4b0ed"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Initialized with gpu!!!\n"
     ]
    }
   ],
   "source": [
    "#creating the model using the Class defined above\n",
    "model = BiLSTM_CRF(vocab_size=len(word_to_id),\n",
    "                   tag_to_ix=tag_to_id,\n",
    "                   embedding_dim=parameters['word_dim'],\n",
    "                   hidden_dim=parameters['word_lstm_dim'],\n",
    "                   use_gpu=use_gpu,\n",
    "                   char_to_ix=char_to_id,\n",
    "                   pre_word_embeds=word_embeds,\n",
    "                   use_crf=parameters['crf'],\n",
    "                   char_mode=parameters['char_mode'])\n",
    "if use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "    print(\"Model Initialized with gpu!!!\")\n",
    "\n",
    "else:\n",
    "    \n",
    "    print(\"Model Initialized!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62237,
     "status": "ok",
     "timestamp": 1606354255499,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "riL7y46GpgGv",
    "outputId": "92bd5736-0d0d-41dd-c110-e2027a8a04f4"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BiLSTM_CRF(\n",
       "  (char_embeds): Embedding(75, 25)\n",
       "  (char_cnn3): Conv2d(1, 25, kernel_size=(3, 25), stride=(1, 1), padding=(2, 0))\n",
       "  (word_embeds): Embedding(17493, 100)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (conv): Conv1d(1, 400, kernel_size=(25,), stride=(1,))\n",
       "  (conv1): Conv1d(400, 400, kernel_size=(100,), stride=(1,))\n",
       "  (conv2): Conv1d(400, 400, kernel_size=(2,), stride=(1,))\n",
       "  (hidden2tag): Linear(in_features=400, out_features=19, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:00:32.961202Z",
     "start_time": "2020-11-20T16:00:32.923311Z"
    },
    "executionInfo": {
     "elapsed": 62236,
     "status": "ok",
     "timestamp": 1606354255500,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "8O6wIc6dpgGv"
   },
   "outputs": [],
   "source": [
    "# #Reload a saved model, if parameter[\"reload\"] is set to a path\n",
    "# if parameters['reload']:\n",
    "#     if not os.path.exists(parameters['reload']):\n",
    "#         print(\"downloading pre-trained model\")\n",
    "#         model_url=\"https://github.com/TheAnig/NER-LSTM-CNN-Pytorch/raw/master/trained-model-cpu\"\n",
    "#         urllib.request.urlretrieve(model_url, parameters['reload'])\n",
    "#     model.load_state_dict(torch.load(parameters['reload']))\n",
    "#     print(\"model reloaded :\", parameters['reload'])\n",
    "\n",
    "# if use_gpu:\n",
    "#     model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PRK8hwjpgGv"
   },
   "source": [
    "##### Training Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:07:51.704112Z",
     "start_time": "2020-11-20T16:07:51.698129Z"
    },
    "executionInfo": {
     "elapsed": 62235,
     "status": "ok",
     "timestamp": 1606354255501,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "JrsuBQnEpgGv"
   },
   "outputs": [],
   "source": [
    "#Initializing the optimizer\n",
    "#The best results in the paper where achived using stochastic gradient descent (SGD) \n",
    "#learning rate=0.015 and momentum=0.9 \n",
    "#decay_rate=0.05 \n",
    "\n",
    "learning_rate = 0.015\n",
    "momentum = 0.9\n",
    "number_of_epochs = parameters['epoch'] \n",
    "decay_rate = 0.05\n",
    "gradient_clip = parameters['gradient_clip']\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "#variables which will used in training process\n",
    "losses = [] #list to store all losses\n",
    "loss = 0.0 #Loss Initializatoin\n",
    "best_dev_F = -1.0 # Current best F-1 Score on Dev Set\n",
    "best_test_F = -1.0 # Current best F-1 Score on Test Set\n",
    "best_train_F = -1.0 # Current best F-1 Score on Train Set\n",
    "all_F = [[0, 0, 0]] # List storing all the F-1 Scores\n",
    "eval_every = len(train_data) # Calculate F-1 Score after this many iterations\n",
    "plot_every = 2000 # Store loss after this many iterations\n",
    "count = 0 #Counts the number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxi3iaiEpgGv"
   },
   "source": [
    "\n",
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiltbxJYpgGv"
   },
   "source": [
    "##### Helper functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:00:38.031580Z",
     "start_time": "2020-11-20T16:00:38.026593Z"
    },
    "executionInfo": {
     "elapsed": 62230,
     "status": "ok",
     "timestamp": 1606354255501,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "qvxOzzEZpgGv"
   },
   "outputs": [],
   "source": [
    "def get_chunk_type(tok, idx_to_tag):\n",
    "    \"\"\"\n",
    "    The function takes in a chunk (\"B-PER\") and then splits it into the tag (PER) and its class (B)\n",
    "    as defined in BIOES\n",
    "    \n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:00:39.947994Z",
     "start_time": "2020-11-20T16:00:39.940015Z"
    },
    "executionInfo": {
     "elapsed": 62229,
     "status": "ok",
     "timestamp": 1606354255502,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "t0JmT5i6pgGv"
   },
   "outputs": [],
   "source": [
    "def get_chunks(seq, tags):\n",
    "    \"\"\"Given a sequence of tags, group entities and their position\n",
    "\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict[\"O\"] = 4\n",
    "\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "\n",
    "    Example:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # We assume by default the tags lie outside a named entity\n",
    "    default = tags[\"O\"]\n",
    "    \n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                # Initialize chunk for each entity\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                # If chunk class is B, i.e., its a beginning of a new named entity\n",
    "                # or, if the chunk type is different from the previous one, then we\n",
    "                # start labelling it as a new entity\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:00:42.208299Z",
     "start_time": "2020-11-20T16:00:42.192345Z"
    },
    "executionInfo": {
     "elapsed": 62227,
     "status": "ok",
     "timestamp": 1606354255502,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "gdb53oBcpgGv"
   },
   "outputs": [],
   "source": [
    "def evaluating(model, datas, best_F,dataset=\"Train\"):\n",
    "    '''\n",
    "    The function takes as input the model, data and calcuates F-1 Score\n",
    "    It performs conditional updates \n",
    "     1) Flag to save the model \n",
    "     2) Best F-1 score\n",
    "    ,if the F-1 score calculated improves on the previous F-1 score\n",
    "    '''\n",
    "    # Initializations\n",
    "    prediction = [] # A list that stores predicted tags\n",
    "    save = False # Flag that tells us if the model needs to be saved\n",
    "    new_F = 0.0 # Variable to store the current F1-Score (may not be the best)\n",
    "    correct_preds, total_correct, total_preds = 0., 0., 0. # Count variables\n",
    "    \n",
    "    for data in datas:\n",
    "        ground_truth_id = data['tags']\n",
    "        words = data['str_words']\n",
    "        chars2 = data['chars']\n",
    "        \n",
    "        if parameters['char_mode'] == 'LSTM':\n",
    "            chars2_sorted = sorted(chars2, key=lambda p: len(p), reverse=True)\n",
    "            d = {}\n",
    "            for i, ci in enumerate(chars2):\n",
    "                for j, cj in enumerate(chars2_sorted):\n",
    "                    if ci == cj and not j in d and not i in d.values():\n",
    "                        d[j] = i\n",
    "                        continue\n",
    "            chars2_length = [len(c) for c in chars2_sorted]\n",
    "            char_maxl = max(chars2_length)\n",
    "            chars2_mask = np.zeros((len(chars2_sorted), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2_sorted):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "        \n",
    "        \n",
    "        if parameters['char_mode'] == 'CNN':\n",
    "            d = {} \n",
    "\n",
    "            # Padding the each word to max word size of that sentence\n",
    "            chars2_length = [len(c) for c in chars2]\n",
    "            char_maxl = max(chars2_length)\n",
    "            chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "        dwords = Variable(torch.LongTensor(data['words']))\n",
    "        \n",
    "        # We are getting the predicted output from our model\n",
    "        if use_gpu:\n",
    "            val,out = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "        else:\n",
    "            val,out = model(dwords, chars2_mask, chars2_length, d)\n",
    "        predicted_id = out\n",
    "    \n",
    "        \n",
    "        # We use the get chunks function defined above to get the true chunks\n",
    "        # and the predicted chunks from true labels and predicted labels respectively\n",
    "        lab_chunks      = set(get_chunks(ground_truth_id,tag_to_id))\n",
    "        lab_pred_chunks = set(get_chunks(predicted_id,\n",
    "                                         tag_to_id))\n",
    "\n",
    "        # Updating the count variables\n",
    "        correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "        total_preds   += len(lab_pred_chunks)\n",
    "        total_correct += len(lab_chunks)\n",
    "    \n",
    "    # Calculating the F1-Score\n",
    "    p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "    r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "    new_F  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "\n",
    "    print(\"{}: new_F: {} best_F: {} \".format(dataset,new_F,best_F))\n",
    "    \n",
    "    # If our current F1-Score is better than the previous best, we update the best\n",
    "    # to current F1 and we set the flag to indicate that we need to checkpoint this model\n",
    "    \n",
    "    if new_F>best_F:\n",
    "        best_F=new_F\n",
    "        save=True\n",
    "\n",
    "    return best_F, new_F, save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7lPyluhpgGv"
   },
   "source": [
    "##### Helper function for performing Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:00:44.282066Z",
     "start_time": "2020-11-20T16:00:44.278079Z"
    },
    "executionInfo": {
     "elapsed": 62226,
     "status": "ok",
     "timestamp": 1606354255503,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "Q4PtRzXBpgGv"
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, lr):\n",
    "    \"\"\"\n",
    "    shrink learning rate\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM0bFpV_pgGv"
   },
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:33:24.351813Z",
     "start_time": "2020-11-20T16:33:24.284026Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 190537,
     "status": "ok",
     "timestamp": 1606363743408,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "3fJLjSu_pgGv",
    "outputId": "2d5e51bd-9665-4939-be18-912bcb3e3513",
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ls/self-trained-model-3-layer\n",
      "Test: new_F: 0.6977115486961151 best_F: -1.0 \n",
      "epoch: 5\n",
      "58000 :  tensor(0.3507, device='cuda:0')\n",
      "60000 :  tensor(0.3473, device='cuda:0')\n",
      "62000 :  tensor(0.3204, device='cuda:0')\n",
      "64000 :  tensor(0.3023, device='cuda:0')\n",
      "66000 :  tensor(0.3315, device='cuda:0')\n",
      "68000 :  tensor(0.3320, device='cuda:0')\n",
      "70000 :  tensor(0.3275, device='cuda:0')\n",
      "epoch: 6\n",
      "72000 :  tensor(0.3363, device='cuda:0')\n",
      "74000 :  tensor(0.2944, device='cuda:0')\n",
      "76000 :  tensor(0.2927, device='cuda:0')\n",
      "78000 :  tensor(0.2864, device='cuda:0')\n",
      "80000 :  tensor(0.3056, device='cuda:0')\n",
      "82000 :  tensor(0.2782, device='cuda:0')\n",
      "84000 :  tensor(0.2776, device='cuda:0')\n",
      "epoch: 7\n",
      "86000 :  tensor(0.2756, device='cuda:0')\n",
      "88000 :  tensor(0.3107, device='cuda:0')\n",
      "90000 :  tensor(0.2649, device='cuda:0')\n",
      "92000 :  tensor(0.2623, device='cuda:0')\n",
      "94000 :  tensor(0.2512, device='cuda:0')\n",
      "96000 :  tensor(0.2779, device='cuda:0')\n",
      "98000 :  tensor(0.2625, device='cuda:0')\n",
      "epoch: 8\n",
      "100000 :  tensor(0.2152, device='cuda:0')\n",
      "102000 :  tensor(0.2651, device='cuda:0')\n",
      "104000 :  tensor(0.2663, device='cuda:0')\n",
      "106000 :  tensor(0.2748, device='cuda:0')\n",
      "108000 :  tensor(0.2215, device='cuda:0')\n",
      "110000 :  tensor(0.2549, device='cuda:0')\n",
      "112000 :  tensor(0.2301, device='cuda:0')\n",
      "Train: new_F: 0.8469669312730823 best_F: 0.7869910521573312 \n",
      "Dev: new_F: 0.799218285325856 best_F: 0.7623040160979291 \n",
      "Saving Model to  ./models/self-trained-model-3-layer\n",
      "Test: new_F: 0.7380093761269383 best_F: 0.6977115486961151 \n",
      "epoch: 9\n",
      "114000 :  tensor(0.2030, device='cuda:0')\n",
      "116000 :  tensor(0.2236, device='cuda:0')\n",
      "118000 :  tensor(0.2535, device='cuda:0')\n",
      "120000 :  tensor(0.2373, device='cuda:0')\n",
      "122000 :  tensor(0.2304, device='cuda:0')\n",
      "124000 :  tensor(0.2424, device='cuda:0')\n",
      "126000 :  tensor(0.2240, device='cuda:0')\n",
      "epoch: 10\n",
      "128000 :  tensor(0.2001, device='cuda:0')\n",
      "130000 :  tensor(0.2076, device='cuda:0')\n",
      "132000 :  tensor(0.2214, device='cuda:0')\n",
      "134000 :  tensor(0.2240, device='cuda:0')\n",
      "136000 :  tensor(0.2238, device='cuda:0')\n",
      "138000 :  tensor(0.2058, device='cuda:0')\n",
      "140000 :  tensor(0.2355, device='cuda:0')\n",
      "epoch: 11\n",
      "142000 :  tensor(0.2279, device='cuda:0')\n",
      "144000 :  tensor(0.2116, device='cuda:0')\n",
      "146000 :  tensor(0.2207, device='cuda:0')\n",
      "148000 :  tensor(0.2086, device='cuda:0')\n",
      "150000 :  tensor(0.1989, device='cuda:0')\n",
      "152000 :  tensor(0.2135, device='cuda:0')\n",
      "154000 :  tensor(0.1726, device='cuda:0')\n",
      "epoch: 12\n",
      "156000 :  tensor(0.1850, device='cuda:0')\n",
      "158000 :  tensor(0.1803, device='cuda:0')\n",
      "160000 :  tensor(0.1896, device='cuda:0')\n",
      "162000 :  tensor(0.1840, device='cuda:0')\n",
      "164000 :  tensor(0.1804, device='cuda:0')\n",
      "166000 :  tensor(0.2082, device='cuda:0')\n",
      "168000 :  tensor(0.1963, device='cuda:0')\n",
      "Train: new_F: 0.8913763270886292 best_F: 0.8469669312730823 \n",
      "Dev: new_F: 0.8231879108682659 best_F: 0.799218285325856 \n",
      "Saving Model to  ./models/self-trained-model-3-layer\n",
      "Test: new_F: 0.7602392171076477 best_F: 0.7380093761269383 \n",
      "epoch: 13\n",
      "170000 :  tensor(0.1888, device='cuda:0')\n",
      "172000 :  tensor(0.1914, device='cuda:0')\n",
      "174000 :  tensor(0.1756, device='cuda:0')\n",
      "176000 :  tensor(0.1579, device='cuda:0')\n",
      "178000 :  tensor(0.1595, device='cuda:0')\n",
      "180000 :  tensor(0.1720, device='cuda:0')\n",
      "182000 :  tensor(0.1561, device='cuda:0')\n",
      "epoch: 14\n",
      "184000 :  tensor(0.1613, device='cuda:0')\n",
      "186000 :  tensor(0.1534, device='cuda:0')\n",
      "188000 :  tensor(0.1511, device='cuda:0')\n",
      "190000 :  tensor(0.1618, device='cuda:0')\n",
      "192000 :  tensor(0.1697, device='cuda:0')\n",
      "194000 :  tensor(0.1922, device='cuda:0')\n",
      "196000 :  tensor(0.1801, device='cuda:0')\n",
      "epoch: 15\n",
      "198000 :  tensor(0.1676, device='cuda:0')\n",
      "200000 :  tensor(0.1535, device='cuda:0')\n",
      "202000 :  tensor(0.1664, device='cuda:0')\n",
      "204000 :  tensor(0.1625, device='cuda:0')\n",
      "206000 :  tensor(0.1422, device='cuda:0')\n",
      "208000 :  tensor(0.1589, device='cuda:0')\n",
      "210000 :  tensor(0.1729, device='cuda:0')\n",
      "epoch: 16\n",
      "212000 :  tensor(0.1355, device='cuda:0')\n",
      "214000 :  tensor(0.1385, device='cuda:0')\n",
      "216000 :  tensor(0.1518, device='cuda:0')\n",
      "218000 :  tensor(0.1363, device='cuda:0')\n",
      "220000 :  tensor(0.1708, device='cuda:0')\n",
      "222000 :  tensor(0.1483, device='cuda:0')\n",
      "224000 :  tensor(0.1401, device='cuda:0')\n",
      "Train: new_F: 0.9067236771779796 best_F: 0.8913763270886292 \n",
      "Dev: new_F: 0.8299389877116096 best_F: 0.8231879108682659 \n",
      "Saving Model to  ./models/self-trained-model-3-layer\n",
      "Test: new_F: 0.7635369188696445 best_F: 0.7602392171076477 \n",
      "epoch: 17\n",
      "226000 :  tensor(0.1499, device='cuda:0')\n",
      "228000 :  tensor(0.1488, device='cuda:0')\n",
      "230000 :  tensor(0.1447, device='cuda:0')\n",
      "232000 :  tensor(0.1408, device='cuda:0')\n",
      "234000 :  tensor(0.1578, device='cuda:0')\n",
      "236000 :  tensor(0.1454, device='cuda:0')\n",
      "238000 :  tensor(0.1434, device='cuda:0')\n",
      "epoch: 18\n",
      "240000 :  tensor(0.1479, device='cuda:0')\n",
      "242000 :  tensor(0.1431, device='cuda:0')\n",
      "244000 :  tensor(0.1297, device='cuda:0')\n",
      "246000 :  tensor(0.1498, device='cuda:0')\n",
      "248000 :  tensor(0.1391, device='cuda:0')\n",
      "250000 :  tensor(0.1282, device='cuda:0')\n",
      "252000 :  tensor(0.1596, device='cuda:0')\n",
      "epoch: 19\n",
      "254000 :  tensor(0.1295, device='cuda:0')\n",
      "256000 :  tensor(0.1376, device='cuda:0')\n",
      "258000 :  tensor(0.1348, device='cuda:0')\n",
      "260000 :  tensor(0.1423, device='cuda:0')\n",
      "262000 :  tensor(0.1174, device='cuda:0')\n",
      "264000 :  tensor(0.1364, device='cuda:0')\n",
      "266000 :  tensor(0.1283, device='cuda:0')\n",
      "epoch: 20\n",
      "268000 :  tensor(0.1172, device='cuda:0')\n",
      "270000 :  tensor(0.1442, device='cuda:0')\n",
      "272000 :  tensor(0.1395, device='cuda:0')\n",
      "274000 :  tensor(0.1476, device='cuda:0')\n",
      "276000 :  tensor(0.1219, device='cuda:0')\n",
      "278000 :  tensor(0.1205, device='cuda:0')\n",
      "280000 :  tensor(0.1218, device='cuda:0')\n",
      "epoch: 21\n",
      "282000 :  tensor(0.1206, device='cuda:0')\n",
      "284000 :  tensor(0.1397, device='cuda:0')\n",
      "286000 :  tensor(0.1225, device='cuda:0')\n",
      "288000 :  tensor(0.1340, device='cuda:0')\n",
      "290000 :  tensor(0.1062, device='cuda:0')\n",
      "292000 :  tensor(0.1228, device='cuda:0')\n",
      "294000 :  tensor(0.1384, device='cuda:0')\n",
      "Train: new_F: 0.9157640886782874 best_F: 0.9067236771779796 \n",
      "Dev: new_F: 0.8345832255840013 best_F: 0.8299389877116096 \n",
      "Saving Model to  ./models/self-trained-model-3-layer\n",
      "Test: new_F: 0.7594382637242385 best_F: 0.7635369188696445 \n",
      "epoch: 22\n",
      "296000 :  tensor(0.1178, device='cuda:0')\n",
      "298000 :  tensor(0.1218, device='cuda:0')\n",
      "300000 :  tensor(0.1038, device='cuda:0')\n",
      "302000 :  tensor(0.1097, device='cuda:0')\n",
      "304000 :  tensor(0.1178, device='cuda:0')\n",
      "306000 :  tensor(0.1144, device='cuda:0')\n",
      "308000 :  tensor(0.1244, device='cuda:0')\n",
      "Train: new_F: 0.9124710186544146 best_F: 0.9157640886782874 \n",
      "Dev: new_F: 0.8304389562789074 best_F: 0.8345832255840013 \n",
      "Test: new_F: 0.7464052287581701 best_F: 0.7635369188696445 \n",
      "epoch: 23\n",
      "310000 :  tensor(0.1335, device='cuda:0')\n",
      "312000 :  tensor(0.1043, device='cuda:0')\n",
      "314000 :  tensor(0.1082, device='cuda:0')\n",
      "316000 :  tensor(0.1193, device='cuda:0')\n",
      "318000 :  tensor(0.0971, device='cuda:0')\n",
      "320000 :  tensor(0.1151, device='cuda:0')\n",
      "322000 :  tensor(0.1121, device='cuda:0')\n",
      "Train: new_F: 0.9247829048186617 best_F: 0.9157640886782874 \n",
      "Dev: new_F: 0.8320584735679075 best_F: 0.8345832255840013 \n",
      "Test: new_F: 0.7600323188796122 best_F: 0.7635369188696445 \n",
      "epoch: 24\n",
      "324000 :  tensor(0.1213, device='cuda:0')\n",
      "326000 :  tensor(0.1112, device='cuda:0')\n",
      "328000 :  tensor(0.1099, device='cuda:0')\n",
      "330000 :  tensor(0.1104, device='cuda:0')\n",
      "332000 :  tensor(0.1173, device='cuda:0')\n",
      "334000 :  tensor(0.1070, device='cuda:0')\n",
      "336000 :  tensor(0.1100, device='cuda:0')\n",
      "Train: new_F: 0.9200510855683268 best_F: 0.9247829048186617 \n",
      "Dev: new_F: 0.8163195920101998 best_F: 0.8345832255840013 \n",
      "Test: new_F: 0.7532865117954259 best_F: 0.7635369188696445 \n",
      "epoch: 25\n",
      "338000 :  tensor(0.1063, device='cuda:0')\n",
      "340000 :  tensor(0.0994, device='cuda:0')\n",
      "342000 :  tensor(0.1038, device='cuda:0')\n",
      "344000 :  tensor(0.0976, device='cuda:0')\n",
      "346000 :  tensor(0.1010, device='cuda:0')\n",
      "348000 :  tensor(0.1018, device='cuda:0')\n",
      "350000 :  tensor(0.1047, device='cuda:0')\n",
      "Train: new_F: 0.931774210694164 best_F: 0.9247829048186617 \n",
      "Dev: new_F: 0.846107733150638 best_F: 0.8345832255840013 \n",
      "Saving Model to  ./models/self-trained-model-3-layer\n",
      "Test: new_F: 0.77735368956743 best_F: 0.7635369188696445 \n",
      "epoch: 26\n",
      "352000 :  tensor(0.1129, device='cuda:0')\n",
      "354000 :  tensor(0.1040, device='cuda:0')\n",
      "356000 :  tensor(0.1097, device='cuda:0')\n",
      "358000 :  tensor(0.0947, device='cuda:0')\n",
      "360000 :  tensor(0.1102, device='cuda:0')\n",
      "362000 :  tensor(0.1150, device='cuda:0')\n",
      "364000 :  tensor(0.0922, device='cuda:0')\n",
      "Train: new_F: 0.9283148062048269 best_F: 0.931774210694164 \n",
      "Dev: new_F: 0.838116137744767 best_F: 0.846107733150638 \n",
      "Test: new_F: 0.7693407182419153 best_F: 0.77735368956743 \n",
      "epoch: 27\n",
      "366000 :  tensor(0.1040, device='cuda:0')\n",
      "368000 :  tensor(0.1036, device='cuda:0')\n",
      "370000 :  tensor(0.0935, device='cuda:0')\n",
      "372000 :  tensor(0.0954, device='cuda:0')\n",
      "374000 :  tensor(0.1051, device='cuda:0')\n",
      "376000 :  tensor(0.1095, device='cuda:0')\n",
      "378000 :  tensor(0.1059, device='cuda:0')\n",
      "Train: new_F: 0.934307820905759 best_F: 0.931774210694164 \n",
      "Dev: new_F: 0.8489716656368611 best_F: 0.846107733150638 \n",
      "Saving Model to  ./models/self-trained-model-3-layer\n",
      "Test: new_F: 0.7660585328772329 best_F: 0.77735368956743 \n",
      "epoch: 28\n",
      "380000 :  tensor(0.1027, device='cuda:0')\n",
      "382000 :  tensor(0.1127, device='cuda:0')\n",
      "384000 :  tensor(0.0773, device='cuda:0')\n",
      "386000 :  tensor(0.1045, device='cuda:0')\n",
      "388000 :  tensor(0.1086, device='cuda:0')\n",
      "390000 :  tensor(0.0997, device='cuda:0')\n",
      "392000 :  tensor(0.1002, device='cuda:0')\n",
      "Train: new_F: 0.9404612604952705 best_F: 0.934307820905759 \n",
      "Dev: new_F: 0.857343024783256 best_F: 0.8489716656368611 \n",
      "Saving Model to  ./models/self-trained-model-3-layer\n",
      "Test: new_F: 0.7721709659197891 best_F: 0.77735368956743 \n",
      "epoch: 29\n",
      "394000 :  tensor(0.1041, device='cuda:0')\n",
      "396000 :  tensor(0.0843, device='cuda:0')\n",
      "398000 :  tensor(0.0866, device='cuda:0')\n",
      "400000 :  tensor(0.0984, device='cuda:0')\n",
      "402000 :  tensor(0.1032, device='cuda:0')\n",
      "404000 :  tensor(0.0967, device='cuda:0')\n",
      "406000 :  tensor(0.0920, device='cuda:0')\n",
      "Train: new_F: 0.9390301217688528 best_F: 0.9404612604952705 \n",
      "Dev: new_F: 0.8537467700258399 best_F: 0.857343024783256 \n",
      "Test: new_F: 0.7781744941865787 best_F: 0.77735368956743 \n",
      "epoch: 30\n",
      "408000 :  tensor(0.0852, device='cuda:0')\n",
      "410000 :  tensor(0.0913, device='cuda:0')\n",
      "412000 :  tensor(0.0851, device='cuda:0')\n",
      "414000 :  tensor(0.0934, device='cuda:0')\n",
      "416000 :  tensor(0.0864, device='cuda:0')\n",
      "418000 :  tensor(0.0943, device='cuda:0')\n",
      "420000 :  tensor(0.0896, device='cuda:0')\n",
      "Train: new_F: 0.9384953205278529 best_F: 0.9404612604952705 \n",
      "Dev: new_F: 0.8499743106696351 best_F: 0.857343024783256 \n",
      "Test: new_F: 0.7735280693747149 best_F: 0.7781744941865787 \n",
      "epoch: 31\n",
      "422000 :  tensor(0.0942, device='cuda:0')\n",
      "424000 :  tensor(0.0858, device='cuda:0')\n",
      "426000 :  tensor(0.0916, device='cuda:0')\n",
      "428000 :  tensor(0.0814, device='cuda:0')\n",
      "430000 :  tensor(0.0865, device='cuda:0')\n",
      "432000 :  tensor(0.0944, device='cuda:0')\n",
      "434000 :  tensor(0.0903, device='cuda:0')\n",
      "Train: new_F: 0.9374149081007488 best_F: 0.9404612604952705 \n",
      "Dev: new_F: 0.8496587030716723 best_F: 0.857343024783256 \n",
      "Test: new_F: 0.7879609544468547 best_F: 0.7781744941865787 \n",
      "epoch: 32\n",
      "436000 :  tensor(0.1005, device='cuda:0')\n",
      "438000 :  tensor(0.0883, device='cuda:0')\n",
      "440000 :  tensor(0.0971, device='cuda:0')\n",
      "442000 :  tensor(0.0823, device='cuda:0')\n",
      "444000 :  tensor(0.0862, device='cuda:0')\n",
      "446000 :  tensor(0.0919, device='cuda:0')\n",
      "448000 :  tensor(0.0756, device='cuda:0')\n",
      "Train: new_F: 0.9413272389414554 best_F: 0.9404612604952705 \n",
      "Dev: new_F: 0.8548849326964829 best_F: 0.857343024783256 \n",
      "Test: new_F: 0.7794500185804534 best_F: 0.7879609544468547 \n",
      "epoch: 33\n",
      "450000 :  tensor(0.0933, device='cuda:0')\n",
      "452000 :  tensor(0.0920, device='cuda:0')\n",
      "454000 :  tensor(0.0869, device='cuda:0')\n",
      "456000 :  tensor(0.0857, device='cuda:0')\n",
      "458000 :  tensor(0.0768, device='cuda:0')\n",
      "460000 :  tensor(0.0886, device='cuda:0')\n",
      "462000 :  tensor(0.0813, device='cuda:0')\n",
      "Train: new_F: 0.9428919629685062 best_F: 0.9413272389414554 \n",
      "Dev: new_F: 0.8515514809590973 best_F: 0.857343024783256 \n",
      "Test: new_F: 0.7751378065006652 best_F: 0.7879609544468547 \n",
      "epoch: 34\n",
      "464000 :  tensor(0.0737, device='cuda:0')\n",
      "466000 :  tensor(0.0774, device='cuda:0')\n",
      "468000 :  tensor(0.0787, device='cuda:0')\n",
      "470000 :  tensor(0.0871, device='cuda:0')\n",
      "472000 :  tensor(0.0745, device='cuda:0')\n",
      "474000 :  tensor(0.0790, device='cuda:0')\n",
      "476000 :  tensor(0.0787, device='cuda:0')\n",
      "Train: new_F: 0.9461603733559609 best_F: 0.9428919629685062 \n",
      "Dev: new_F: 0.8558419243986255 best_F: 0.857343024783256 \n",
      "Test: new_F: 0.7794683776351972 best_F: 0.7879609544468547 \n",
      "epoch: 35\n",
      "478000 :  tensor(0.1117, device='cuda:0')\n",
      "480000 :  tensor(0.0846, device='cuda:0')\n",
      "482000 :  tensor(0.0741, device='cuda:0')\n",
      "484000 :  tensor(0.0788, device='cuda:0')\n",
      "486000 :  tensor(0.0767, device='cuda:0')\n",
      "488000 :  tensor(0.0752, device='cuda:0')\n",
      "490000 :  tensor(0.0808, device='cuda:0')\n",
      "Train: new_F: 0.9486403546159747 best_F: 0.9461603733559609 \n",
      "Dev: new_F: 0.8619737750172533 best_F: 0.857343024783256 \n",
      "Saving Model to  ./models/self-trained-model-3-layer\n",
      "Test: new_F: 0.7888827535431622 best_F: 0.7879609544468547 \n",
      "epoch: 36\n",
      "492000 :  tensor(0.0687, device='cuda:0')\n",
      "494000 :  tensor(0.0820, device='cuda:0')\n",
      "496000 :  tensor(0.0746, device='cuda:0')\n",
      "498000 :  tensor(0.0852, device='cuda:0')\n",
      "500000 :  tensor(0.0949, device='cuda:0')\n",
      "502000 :  tensor(0.0786, device='cuda:0')\n",
      "504000 :  tensor(0.0769, device='cuda:0')\n",
      "Train: new_F: 0.9479343760625638 best_F: 0.9486403546159747 \n",
      "Dev: new_F: 0.8587906030022898 best_F: 0.8619737750172533 \n",
      "Test: new_F: 0.7834349229114378 best_F: 0.7888827535431622 \n",
      "epoch: 37\n",
      "506000 :  tensor(0.0630, device='cuda:0')\n",
      "508000 :  tensor(0.0766, device='cuda:0')\n",
      "510000 :  tensor(0.0797, device='cuda:0')\n",
      "512000 :  tensor(0.0719, device='cuda:0')\n",
      "514000 :  tensor(0.0763, device='cuda:0')\n",
      "516000 :  tensor(0.0654, device='cuda:0')\n",
      "518000 :  tensor(0.0813, device='cuda:0')\n",
      "Train: new_F: 0.9497543259987182 best_F: 0.9486403546159747 \n",
      "Dev: new_F: 0.8638118638118638 best_F: 0.8619737750172533 \n",
      "Saving Model to  ./models/self-trained-model-3-layer\n",
      "Test: new_F: 0.791150609813747 best_F: 0.7888827535431622 \n",
      "epoch: 38\n",
      "520000 :  tensor(0.0824, device='cuda:0')\n",
      "522000 :  tensor(0.0757, device='cuda:0')\n",
      "524000 :  tensor(0.0748, device='cuda:0')\n",
      "526000 :  tensor(0.0702, device='cuda:0')\n",
      "528000 :  tensor(0.0786, device='cuda:0')\n",
      "530000 :  tensor(0.0761, device='cuda:0')\n",
      "532000 :  tensor(0.0648, device='cuda:0')\n",
      "Train: new_F: 0.9519617224880383 best_F: 0.9497543259987182 \n",
      "Dev: new_F: 0.8565314158534497 best_F: 0.8638118638118638 \n",
      "Test: new_F: 0.7860787172011662 best_F: 0.791150609813747 \n",
      "epoch: 39\n",
      "534000 :  tensor(0.0734, device='cuda:0')\n",
      "536000 :  tensor(0.0707, device='cuda:0')\n",
      "538000 :  tensor(0.0655, device='cuda:0')\n",
      "540000 :  tensor(0.0756, device='cuda:0')\n",
      "542000 :  tensor(0.0846, device='cuda:0')\n",
      "544000 :  tensor(0.0720, device='cuda:0')\n",
      "546000 :  tensor(0.0686, device='cuda:0')\n",
      "Train: new_F: 0.9502741216374767 best_F: 0.9519617224880383 \n",
      "Dev: new_F: 0.8633512940552954 best_F: 0.8638118638118638 \n",
      "Test: new_F: 0.7801987767584098 best_F: 0.791150609813747 \n",
      "epoch: 40\n",
      "548000 :  tensor(0.0799, device='cuda:0')\n",
      "550000 :  tensor(0.0729, device='cuda:0')\n",
      "552000 :  tensor(0.0770, device='cuda:0')\n",
      "554000 :  tensor(0.0769, device='cuda:0')\n",
      "556000 :  tensor(0.0664, device='cuda:0')\n",
      "558000 :  tensor(0.0800, device='cuda:0')\n",
      "560000 :  tensor(0.0680, device='cuda:0')\n",
      "Train: new_F: 0.9463935961849613 best_F: 0.9519617224880383 \n",
      "Dev: new_F: 0.8446276732800826 best_F: 0.8638118638118638 \n",
      "Test: new_F: 0.7737932293092435 best_F: 0.791150609813747 \n",
      "epoch: 41\n",
      "562000 :  tensor(0.0674, device='cuda:0')\n",
      "564000 :  tensor(0.0579, device='cuda:0')\n",
      "566000 :  tensor(0.0769, device='cuda:0')\n",
      "568000 :  tensor(0.0661, device='cuda:0')\n",
      "570000 :  tensor(0.0721, device='cuda:0')\n",
      "572000 :  tensor(0.0610, device='cuda:0')\n",
      "574000 :  tensor(0.0890, device='cuda:0')\n",
      "Train: new_F: 0.9463400097911923 best_F: 0.9519617224880383 \n",
      "Dev: new_F: 0.8519543410584572 best_F: 0.8638118638118638 \n",
      "Test: new_F: 0.7772135536884868 best_F: 0.791150609813747 \n",
      "epoch: 42\n",
      "576000 :  tensor(0.0853, device='cuda:0')\n",
      "578000 :  tensor(0.0734, device='cuda:0')\n",
      "580000 :  tensor(0.0705, device='cuda:0')\n",
      "582000 :  tensor(0.0605, device='cuda:0')\n",
      "584000 :  tensor(0.0753, device='cuda:0')\n",
      "586000 :  tensor(0.0698, device='cuda:0')\n",
      "588000 :  tensor(0.0791, device='cuda:0')\n",
      "Train: new_F: 0.9433094015637318 best_F: 0.9519617224880383 \n",
      "Dev: new_F: 0.8549258936355711 best_F: 0.8638118638118638 \n",
      "Test: new_F: 0.7756476199382774 best_F: 0.791150609813747 \n",
      "epoch: 43\n",
      "590000 :  tensor(0.0618, device='cuda:0')\n",
      "592000 :  tensor(0.0708, device='cuda:0')\n",
      "594000 :  tensor(0.0708, device='cuda:0')\n",
      "596000 :  tensor(0.0652, device='cuda:0')\n",
      "598000 :  tensor(0.0589, device='cuda:0')\n",
      "600000 :  tensor(0.0656, device='cuda:0')\n",
      "602000 :  tensor(0.0615, device='cuda:0')\n",
      "Train: new_F: 0.9500818852755385 best_F: 0.9519617224880383 \n",
      "Dev: new_F: 0.8566800238724529 best_F: 0.8638118638118638 \n",
      "Test: new_F: 0.7874514058403399 best_F: 0.791150609813747 \n",
      "epoch: 44\n",
      "604000 :  tensor(0.0784, device='cuda:0')\n",
      "606000 :  tensor(0.0739, device='cuda:0')\n",
      "608000 :  tensor(0.0665, device='cuda:0')\n",
      "610000 :  tensor(0.0690, device='cuda:0')\n",
      "612000 :  tensor(0.0684, device='cuda:0')\n",
      "614000 :  tensor(0.0748, device='cuda:0')\n",
      "616000 :  tensor(0.0515, device='cuda:0')\n",
      "Train: new_F: 0.9556721703144332 best_F: 0.9519617224880383 \n",
      "Dev: new_F: 0.8630885285701899 best_F: 0.8638118638118638 \n",
      "Test: new_F: 0.7939754555596876 best_F: 0.791150609813747 \n",
      "epoch: 45\n",
      "618000 :  tensor(0.0679, device='cuda:0')\n",
      "620000 :  tensor(0.0627, device='cuda:0')\n",
      "622000 :  tensor(0.0715, device='cuda:0')\n",
      "624000 :  tensor(0.0581, device='cuda:0')\n",
      "626000 :  tensor(0.0630, device='cuda:0')\n",
      "628000 :  tensor(0.0723, device='cuda:0')\n",
      "630000 :  tensor(0.0607, device='cuda:0')\n",
      "Train: new_F: 0.9507622667292992 best_F: 0.9556721703144332 \n",
      "Dev: new_F: 0.854810698159083 best_F: 0.8638118638118638 \n",
      "Test: new_F: 0.7836560733808949 best_F: 0.7939754555596876 \n",
      "epoch: 46\n",
      "632000 :  tensor(0.0715, device='cuda:0')\n",
      "634000 :  tensor(0.0641, device='cuda:0')\n",
      "636000 :  tensor(0.0636, device='cuda:0')\n",
      "638000 :  tensor(0.0638, device='cuda:0')\n",
      "640000 :  tensor(0.0663, device='cuda:0')\n",
      "642000 :  tensor(0.0724, device='cuda:0')\n",
      "644000 :  tensor(0.0687, device='cuda:0')\n",
      "Train: new_F: 0.9573498082658713 best_F: 0.9556721703144332 \n",
      "Dev: new_F: 0.860715825786977 best_F: 0.8638118638118638 \n",
      "Test: new_F: 0.7944700460829492 best_F: 0.7939754555596876 \n",
      "epoch: 47\n",
      "646000 :  tensor(0.0631, device='cuda:0')\n",
      "648000 :  tensor(0.0708, device='cuda:0')\n",
      "650000 :  tensor(0.0535, device='cuda:0')\n",
      "652000 :  tensor(0.0659, device='cuda:0')\n",
      "654000 :  tensor(0.0692, device='cuda:0')\n",
      "656000 :  tensor(0.0569, device='cuda:0')\n",
      "658000 :  tensor(0.0657, device='cuda:0')\n",
      "Train: new_F: 0.9553090467775247 best_F: 0.9573498082658713 \n",
      "Dev: new_F: 0.8625196027182436 best_F: 0.8638118638118638 \n",
      "Test: new_F: 0.7830885791787484 best_F: 0.7944700460829492 \n",
      "epoch: 48\n",
      "660000 :  tensor(0.0733, device='cuda:0')\n",
      "662000 :  tensor(0.0603, device='cuda:0')\n",
      "664000 :  tensor(0.0622, device='cuda:0')\n",
      "666000 :  tensor(0.0566, device='cuda:0')\n",
      "668000 :  tensor(0.0620, device='cuda:0')\n",
      "670000 :  tensor(0.0556, device='cuda:0')\n",
      "672000 :  tensor(0.0742, device='cuda:0')\n",
      "Train: new_F: 0.9600204804369159 best_F: 0.9573498082658713 \n",
      "Dev: new_F: 0.8629272567922874 best_F: 0.8638118638118638 \n",
      "Test: new_F: 0.7848530519969856 best_F: 0.7944700460829492 \n",
      "epoch: 49\n",
      "674000 :  tensor(0.0673, device='cuda:0')\n",
      "676000 :  tensor(0.0703, device='cuda:0')\n",
      "678000 :  tensor(0.0544, device='cuda:0')\n",
      "680000 :  tensor(0.0610, device='cuda:0')\n",
      "682000 :  tensor(0.0702, device='cuda:0')\n",
      "684000 :  tensor(0.0512, device='cuda:0')\n",
      "686000 :  tensor(0.0530, device='cuda:0')\n",
      "688000 :  tensor(0.0658, device='cuda:0')\n",
      "Train: new_F: 0.9536129789732095 best_F: 0.9600204804369159 \n",
      "Dev: new_F: 0.862436636951582 best_F: 0.8638118638118638 \n",
      "Test: new_F: 0.7854129558451298 best_F: 0.7944700460829492 \n",
      "time taken: 34626.684611558914\n"
     ]
    }
   ],
   "source": [
    "parameters['reload']=False\n",
    "\n",
    "if not parameters['reload']:\n",
    "    tr = time.time()\n",
    "    model.train(True)\n",
    "\n",
    "    for epoch in range(1,number_of_epochs):\n",
    "        print('epoch:', epoch)\n",
    "\n",
    "        for i, index in enumerate(np.random.permutation(len(train_data))):\n",
    "            count += 1\n",
    "            data = train_data[index]\n",
    "\n",
    "            ##gradient updates for each data entry\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = data['words']\n",
    "            sentence_in = Variable(torch.LongTensor(sentence_in))\n",
    "            tags = data['tags']\n",
    "            chars2 = data['chars']\n",
    "            \n",
    "            if parameters['char_mode'] == 'LSTM':\n",
    "                chars2_sorted = sorted(chars2, key=lambda p: len(p), reverse=True)\n",
    "                d = {}\n",
    "                for i, ci in enumerate(chars2):\n",
    "                    for j, cj in enumerate(chars2_sorted):\n",
    "                        if ci == cj and not j in d and not i in d.values():\n",
    "                            d[j] = i\n",
    "                            continue\n",
    "                chars2_length = [len(c) for c in chars2_sorted]\n",
    "                char_maxl = max(chars2_length)\n",
    "                chars2_mask = np.zeros((len(chars2_sorted), char_maxl), dtype='int')\n",
    "                for i, c in enumerate(chars2_sorted):\n",
    "                    chars2_mask[i, :chars2_length[i]] = c\n",
    "                chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "            \n",
    "            if parameters['char_mode'] == 'CNN':\n",
    "\n",
    "                d = {}\n",
    "\n",
    "                ## Padding the each word to max word size of that sentence\n",
    "                chars2_length = [len(c) for c in chars2]\n",
    "                char_maxl = max(chars2_length)\n",
    "                chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "                for i, c in enumerate(chars2):\n",
    "                    chars2_mask[i, :chars2_length[i]] = c\n",
    "                chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "\n",
    "            targets = torch.LongTensor(tags)\n",
    "\n",
    "            #we calculate the negative log-likelihood for the predicted tags using the predefined function\n",
    "            if use_gpu:\n",
    "                neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "            else:\n",
    "                neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets, chars2_mask, chars2_length, d)\n",
    "            loss += neg_log_likelihood.data / len(data['words'])\n",
    "            neg_log_likelihood.backward()\n",
    "\n",
    "            #we use gradient clipping to avoid exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            #Storing loss\n",
    "            if count % plot_every == 0:\n",
    "                loss /= plot_every\n",
    "                print(count, ': ', loss)\n",
    "                if losses == []:\n",
    "                    losses.append(loss)\n",
    "                losses.append(loss)\n",
    "                loss = 0.0\n",
    "\n",
    "            #Evaluating on Train, Test, Dev Sets\n",
    "            if count % (eval_every) == 0 and count > (eval_every * 20) or \\\n",
    "                    count % (eval_every*4) == 0 and count < (eval_every * 20):\n",
    "                model.train(False)\n",
    "                best_train_F, new_train_F, _ = evaluating(model, train_data, best_train_F,\"Train\")\n",
    "                best_dev_F, new_dev_F, save = evaluating(model, dev_data, best_dev_F,\"Dev\")\n",
    "                if save:\n",
    "                    print(\"Saving Model to \", model_name)\n",
    "                    torch.save(model.state_dict(), model_name)\n",
    "                best_test_F, new_test_F, _ = evaluating(model, test_data, best_test_F,\"Test\")\n",
    "\n",
    "                all_F.append([new_train_F, new_dev_F, new_test_F])\n",
    "                model.train(True)\n",
    "\n",
    "            #Performing decay on the learning rate\n",
    "            if count % len(train_data) == 0:\n",
    "                adjust_learning_rate(optimizer, lr=learning_rate/(1+decay_rate*count/len(train_data)))\n",
    "\n",
    "    print('time taken:', time.time() - tr)\n",
    "\n",
    "if not parameters['reload']:\n",
    "    #reload the best model saved from training\n",
    "    model.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 480x320 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 385.78125 262.19625\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-11-29T02:44:54.013223</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 385.78125 262.19625 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 224.64 \nL 378.58125 224.64 \nL 378.58125 7.2 \nL 43.78125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"md28cea4441\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#md28cea4441\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(55.818182 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"103.238332\" xlink:href=\"#md28cea4441\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <g transform=\"translate(96.875832 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.477233\" xlink:href=\"#md28cea4441\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <g transform=\"translate(137.933483 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"191.716134\" xlink:href=\"#md28cea4441\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 150 -->\n      <g transform=\"translate(182.172384 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.955034\" xlink:href=\"#md28cea4441\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 200 -->\n      <g transform=\"translate(226.411284 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"280.193935\" xlink:href=\"#md28cea4441\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 250 -->\n      <g transform=\"translate(270.650185 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.432836\" xlink:href=\"#md28cea4441\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 300 -->\n      <g transform=\"translate(314.889086 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.671736\" xlink:href=\"#md28cea4441\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 350 -->\n      <g transform=\"translate(359.127986 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- step (1 step = 2000 iterations) -->\n     <g transform=\"translate(133.879687 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path id=\"DejaVuSans-32\"/>\n       <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n       <path d=\"M 10.59375 45.40625 \nL 73.1875 45.40625 \nL 73.1875 37.203125 \nL 10.59375 37.203125 \nz\nM 10.59375 25.484375 \nL 73.1875 25.484375 \nL 73.1875 17.1875 \nL 10.59375 17.1875 \nz\n\" id=\"DejaVuSans-61\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n       <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"52.099609\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"91.308594\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"216.308594\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"248.095703\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"287.109375\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"350.732422\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"382.519531\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"434.619141\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"473.828125\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"535.351562\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"598.828125\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"630.615234\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"714.404297\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"746.191406\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"809.814453\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"873.4375\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"937.060547\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1000.683594\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1032.470703\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"1060.253906\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"1099.462891\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"1160.986328\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"1202.099609\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"1263.378906\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"1302.587891\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"1330.371094\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"1391.552734\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"1454.931641\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"1507.03125\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mc01d5249e5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc01d5249e5\" y=\"223.80377\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 227.602988)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc01d5249e5\" y=\"188.443936\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 192.243155)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc01d5249e5\" y=\"153.084102\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 156.883321)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc01d5249e5\" y=\"117.724268\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 121.523487)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc01d5249e5\" y=\"82.364434\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.8 -->\n      <g transform=\"translate(20.878125 86.163653)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc01d5249e5\" y=\"47.0046\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 50.803819)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mc01d5249e5\" y=\"11.644767\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 1.2 -->\n      <g transform=\"translate(20.878125 15.443985)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- loss (negative log likelihood) -->\n     <g transform=\"translate(14.798438 187.546562)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n       <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n       <path d=\"M 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 31.109375 \nL 44.921875 54.6875 \nL 56.390625 54.6875 \nL 27.390625 29.109375 \nL 57.625 0 \nL 45.90625 0 \nL 18.109375 26.703125 \nL 18.109375 0 \nL 9.078125 0 \nz\n\" id=\"DejaVuSans-107\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n       <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"224.951172\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"263.964844\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"327.34375\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"388.867188\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"452.34375\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"513.623047\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"552.832031\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"580.615234\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"639.794922\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"701.318359\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"733.105469\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"760.888672\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"822.070312\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"885.546875\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"917.333984\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"945.117188\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"972.900391\" xlink:href=\"#DejaVuSans-107\"/>\n      <use x=\"1027.185547\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"1088.708984\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"1116.492188\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"1144.275391\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"1207.654297\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"1268.835938\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"1330.017578\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"1393.494141\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p264f88f968)\" d=\"M 58.999432 17.083636 \nL 59.88421 17.083636 \nL 60.768988 125.490271 \nL 61.653766 134.269047 \nL 62.538544 139.9689 \nL 63.423322 133.209119 \nL 64.3081 136.372095 \nL 65.192878 138.074749 \nL 66.077656 147.58906 \nL 66.962434 144.991863 \nL 67.847212 145.515245 \nL 68.73199 144.021408 \nL 69.616768 147.789642 \nL 70.501546 143.216538 \nL 71.386324 152.694472 \nL 72.271102 151.514348 \nL 73.15588 149.410457 \nL 74.040658 151.498156 \nL 74.925436 154.93511 \nL 75.810214 148.756039 \nL 76.694992 158.62142 \nL 77.57977 156.695939 \nL 78.464548 160.135096 \nL 79.349326 158.74069 \nL 80.234104 161.344737 \nL 82.00366 156.768204 \nL 82.888438 161.947772 \nL 83.773216 156.676734 \nL 84.657994 161.791582 \nL 85.542772 162.39408 \nL 86.42755 167.150003 \nL 87.312328 170.355989 \nL 88.197106 165.198619 \nL 89.081884 165.101874 \nL 89.966662 165.90177 \nL 90.85144 164.343877 \nL 91.736218 171.74894 \nL 92.620996 172.061551 \nL 93.505774 173.173621 \nL 94.390552 169.766738 \nL 95.27533 174.618968 \nL 96.160108 174.723379 \nL 97.044886 175.077147 \nL 97.929664 168.875277 \nL 98.814442 176.976667 \nL 99.69922 177.437296 \nL 100.583998 179.394829 \nL 101.468776 174.674308 \nL 102.353554 177.392773 \nL 103.238332 185.749013 \nL 104.12311 176.932555 \nL 105.007888 176.725061 \nL 105.892666 175.213667 \nL 106.777445 184.639493 \nL 107.662223 178.737512 \nL 109.431779 187.905999 \nL 110.316557 184.270316 \nL 111.201335 178.985821 \nL 112.086113 181.846043 \nL 112.970891 183.068141 \nL 113.855669 180.944887 \nL 114.740447 184.208587 \nL 115.625225 188.429098 \nL 116.510003 187.096319 \nL 117.394781 184.65245 \nL 118.279559 184.209417 \nL 119.164337 184.238689 \nL 120.049115 187.415917 \nL 120.933893 182.162814 \nL 121.818671 183.518937 \nL 122.703449 186.397343 \nL 123.588227 184.789531 \nL 124.473005 186.9302 \nL 125.357783 188.647154 \nL 126.242561 186.054883 \nL 127.127339 193.290198 \nL 128.012117 191.088149 \nL 128.896895 191.927141 \nL 129.781673 190.291411 \nL 130.666451 191.276453 \nL 131.551229 191.904326 \nL 132.436007 186.999366 \nL 133.320785 189.100511 \nL 134.205563 190.425787 \nL 135.090341 189.965754 \nL 136.859897 195.895766 \nL 137.744675 195.60211 \nL 138.629453 193.40201 \nL 139.514231 196.204984 \nL 140.399009 195.280423 \nL 141.283787 196.675202 \nL 142.168565 197.083301 \nL 143.053343 195.192432 \nL 143.938121 193.800216 \nL 144.822899 189.827221 \nL 146.592455 194.171348 \nL 147.477233 196.671883 \nL 148.362011 194.383455 \nL 149.246789 195.068276 \nL 150.131567 198.654815 \nL 151.016345 195.714548 \nL 151.901123 193.232323 \nL 152.785901 199.853573 \nL 153.670679 199.325288 \nL 154.555457 196.971606 \nL 155.440235 199.699254 \nL 156.325013 193.610273 \nL 157.209791 197.577602 \nL 158.094569 199.042328 \nL 158.979347 197.295785 \nL 159.864125 197.493769 \nL 161.633681 198.915552 \nL 162.518459 195.903698 \nL 163.403237 198.089303 \nL 164.288015 198.44741 \nL 165.172793 197.649411 \nL 166.057571 198.505572 \nL 166.942349 200.865353 \nL 167.827127 197.314683 \nL 169.596683 201.13181 \nL 170.481461 195.584005 \nL 171.366239 200.914703 \nL 172.251017 199.471235 \nL 173.135795 199.970032 \nL 174.020573 198.64537 \nL 174.905351 203.041927 \nL 175.790129 199.681361 \nL 176.674908 201.113919 \nL 177.559686 203.086246 \nL 178.444464 198.318124 \nL 179.329242 199.137508 \nL 180.21402 197.69998 \nL 181.098798 202.252018 \nL 181.983576 202.496888 \nL 182.868354 202.262683 \nL 183.753132 202.485253 \nL 184.63791 199.11063 \nL 185.522688 202.142068 \nL 186.407466 200.116843 \nL 187.292244 205.025712 \nL 189.0618 199.342157 \nL 189.946578 202.972452 \nL 190.831356 202.264221 \nL 191.716134 205.44511 \nL 192.600912 204.416315 \nL 193.48569 202.975264 \nL 194.370468 203.572692 \nL 196.140024 200.207473 \nL 197.024802 205.363776 \nL 197.90958 204.669858 \nL 198.794358 202.716604 \nL 199.679136 206.632257 \nL 200.563914 203.454813 \nL 201.448692 203.979508 \nL 202.33347 202.355337 \nL 203.218248 204.138225 \nL 204.103026 204.364979 \nL 204.987804 204.278127 \nL 205.872582 203.058213 \nL 206.75736 204.883665 \nL 207.642138 204.357019 \nL 208.526916 205.016054 \nL 209.411694 206.233776 \nL 210.296472 205.454448 \nL 211.18125 206.549509 \nL 212.066028 205.951125 \nL 212.950806 205.800714 \nL 213.835584 205.284776 \nL 214.720362 203.846713 \nL 215.60514 205.415586 \nL 216.489918 204.405082 \nL 217.374696 207.066095 \nL 218.259474 204.325596 \nL 219.144252 203.475622 \nL 220.02903 207.509619 \nL 220.913808 205.422017 \nL 221.798586 205.487112 \nL 222.683364 207.274517 \nL 223.568142 206.944555 \nL 224.45292 205.214961 \nL 225.337698 204.437061 \nL 227.107254 205.640433 \nL 227.992032 203.870905 \nL 228.87681 210.141622 \nL 229.761588 205.335742 \nL 230.646366 204.604748 \nL 231.531144 206.178354 \nL 232.415922 206.097137 \nL 233.3007 205.392076 \nL 234.185478 208.898399 \nL 235.070256 208.492909 \nL 235.955034 206.399635 \nL 236.839812 205.552807 \nL 237.72459 206.710414 \nL 238.609368 207.534678 \nL 239.494146 208.736998 \nL 240.378924 207.666929 \nL 241.263702 208.762822 \nL 242.14848 207.283944 \nL 243.033258 208.527763 \nL 243.918036 207.124948 \nL 244.802814 207.955453 \nL 245.687592 207.148253 \nL 246.572371 208.634359 \nL 247.457149 207.602244 \nL 248.341927 209.420154 \nL 249.226705 208.511957 \nL 250.111483 207.112762 \nL 250.996261 207.833638 \nL 251.881039 206.031687 \nL 252.765817 208.192978 \nL 253.650595 206.642202 \nL 254.535373 209.247705 \nL 255.420151 208.555717 \nL 256.304929 207.548782 \nL 257.189707 210.430644 \nL 258.074485 207.302227 \nL 258.959263 207.539968 \nL 259.844041 208.433411 \nL 260.728819 208.646393 \nL 261.613597 210.220428 \nL 262.498375 208.147234 \nL 264.267931 210.772786 \nL 265.152709 210.126813 \nL 266.037487 209.8856 \nL 266.922265 208.397428 \nL 267.807043 210.632201 \nL 268.691821 209.835873 \nL 269.576599 209.889604 \nL 270.461377 204.061636 \nL 271.346155 208.840549 \nL 272.230933 210.698584 \nL 273.115711 209.863901 \nL 274.885267 210.5083 \nL 275.770045 209.514493 \nL 276.654823 211.655422 \nL 277.539601 209.31316 \nL 278.424379 210.617848 \nL 280.193935 207.028606 \nL 281.078713 209.906565 \nL 281.963491 210.212498 \nL 282.848269 212.668768 \nL 283.733047 210.260762 \nL 284.617825 209.721063 \nL 285.502603 211.094288 \nL 286.387381 210.311324 \nL 287.272159 212.235792 \nL 288.156937 209.434753 \nL 289.041715 209.236635 \nL 289.926493 210.423171 \nL 290.811271 210.574472 \nL 291.696049 211.398574 \nL 292.580827 209.909649 \nL 293.465605 210.341889 \nL 294.350383 212.343037 \nL 295.235161 210.827363 \nL 296.119939 211.296144 \nL 297.004717 212.222671 \nL 298.774273 208.855368 \nL 299.659051 211.081874 \nL 300.543829 211.675299 \nL 301.428607 209.685827 \nL 302.313385 210.916828 \nL 303.198163 210.187697 \nL 304.082941 210.215517 \nL 304.967719 212.07081 \nL 305.852497 209.66312 \nL 306.737275 211.779465 \nL 307.622053 211.895828 \nL 308.506831 213.570918 \nL 309.391609 210.215724 \nL 310.276387 212.11111 \nL 311.161165 211.062096 \nL 312.045943 213.017127 \nL 312.930721 208.062833 \nL 313.815499 208.725206 \nL 314.700277 210.831661 \nL 315.585055 211.338688 \nL 316.469834 213.112596 \nL 317.354612 210.496176 \nL 318.23939 211.455145 \nL 319.124168 209.817974 \nL 320.008946 212.869376 \nL 320.893724 211.285835 \nL 321.778502 211.292007 \nL 323.548058 213.39603 \nL 324.432836 212.20123 \nL 325.317614 212.930304 \nL 326.202392 209.938594 \nL 327.08717 210.741185 \nL 327.971948 212.054646 \nL 328.856726 211.597932 \nL 329.741504 211.710666 \nL 330.626282 210.580118 \nL 331.51106 214.702721 \nL 332.395838 211.807507 \nL 333.280616 212.714193 \nL 334.165394 211.167193 \nL 335.050172 213.534019 \nL 335.93495 212.659211 \nL 336.819728 211.024215 \nL 337.704506 213.070005 \nL 338.589284 211.160956 \nL 339.474062 212.479557 \nL 340.35884 212.563917 \nL 341.243618 212.520734 \nL 342.128396 212.078115 \nL 343.013174 211.011346 \nL 343.897952 211.655703 \nL 344.78273 212.652186 \nL 345.667508 211.290526 \nL 346.552286 214.352169 \nL 347.437064 212.153254 \nL 348.321842 211.562957 \nL 349.20662 213.741454 \nL 350.976176 210.85129 \nL 351.860954 213.139351 \nL 352.745732 212.79902 \nL 353.63051 213.800597 \nL 354.515288 212.836971 \nL 355.400066 213.970193 \nL 356.284844 210.683184 \nL 357.169622 211.913341 \nL 358.0544 211.382137 \nL 358.939178 214.192943 \nL 359.823956 213.014439 \nL 360.708734 211.394857 \nL 361.593512 214.756364 \nL 362.47829 214.436433 \nL 363.363068 212.1673 \nL 363.363068 212.1673 \n\" style=\"fill:none;stroke:#92c6ff;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 224.64 \nL 43.78125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 224.64 \nL 378.58125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 224.64 \nL 378.58125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 7.2 \nL 378.58125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p264f88f968\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAasAAAEhCAYAAADBOo/TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAxOAAAMTgF/d4wjAAA2cklEQVR4nO3deZxcdZnv8c/T3ekk3Z19J52QhBAgJBA2AUUExHGZEUVgFHdHFMf1Do6Oo3NH56ozd3AuM9cZHEVwZ3QURBG5gjo6KoSdkA2SQEg6C9nTSbo7Sy/P/eM5RVV3qrtPkq6q053v+/WqV1edc6rqd7qSevr3+z3n+Zm7IyIikmVVlW6AiIhIfxSsREQk8xSsREQk8xSsREQk8xSsREQk8xSsREQk82oq3YBjNXz4cJ80aVKlmyEiIsdo06ZNh9x9eLF9gz5YTZo0iY0bN1a6GSIicozMbHtv+0o+DGhmXzazdWbmZragl2MuM7OHzWylmS03sy+amZW6bSIiMjiUY87qDuAiYH0fx+wGrnX3+cC5wCuAa8vQNhERGQRKPgzo7r8D6Kuj5O5PFtw/YGZLgDmlbpuIiAwOmcsGNLOpwNXAvb3sv8HMNuZuLS0t5W2giIiUXaaClZmNBn4G3OjuTxQ7xt1vcvfG3K2hoaG8jRQRkbLLTLAys1HAL4C73f2mSrdHRESyIxPByswaiEB1n7t/vtLtERGRbClH6vrNZrYRaAR+ZWbPJttvNbMrksM+BrwEuNLMliS3z5S6bSIiMjjYYF98sbGx0Y/louD710BHF5w+BU4cO3DtEhGRI2Nmm9y9sdi+QV/B4lit3QXtXdDpClYiIlmViTmrSvrA+VBTBYO8gykiMqQd98EKwADFKhGR7FKwAjD1rEREskzBivglKFaJiGSXghVgGgcUEck0BauEYpWISHYpWBE9K81ZiYhkl4IVygYUEck6BSuSYKVoJSKSWQpWJMOAlW6EiIj0SsEK9axERLJOwYokdV1ERDJLwSqhjpWISHYpWKHUdRGRrFOwQnNWIiJZp2CFsgFFRLJOwQpdFCwiknUKVmjOSkQk6xSsUM9KRCTrFKxQz0pEJOsUrIielYiIZJeCVUI9KxGR7FKwQqnrIiJZp2CFLgoWEck6BSvUsxIRyToFK5S6LiKSdQpWKHVdRCTrFKzQnJWISNYpWKE5KxGRrCt5sDKzL5vZOjNzM1vQx3HvNbM1Zvacmd1iZjWlbtuL742ClYhIlpWjZ3UHcBGwvrcDzGw28PnkuLnAVOC9ZWhb0gAUrUREMixV78XM6oGXAY3AfuApd1+Z5rnu/rvkNfo67GrgLnffmhz7VeCTwNfSvMexUqwSEcm2PntWZnaimX0TWAv8FfBKIrDcYWYrzOw9A9SOmXTvea1LthVr0w1mtjF3a2lpOeY3VzagiEi29TcM+F3gp8B0d3+lu7/N3a9y9/nAHwPzzOzDA9SWwnDRazfM3W9y98bcraGh4ZjfWD0rEZFs63MY0N0v7mPfOuCvB6gdTcCsgscnJtvKQqnrIiLZ1t8wYF1ftwFsx53AlWY2xWJy6wPADwbw9fuk1HURkWzrbxiwBdjXx61fZnazmW0kkjN+ZWbPJttvNbMrANx9LfBZ4AHgOWAbcNsRn81RyuV+qHclIpJN/Q0DVgGY2d8AB4FbiFGz64CONG/g7h8CPlRk+3U9Hn8d+HqqVg8wLb4oIpJtaS+8fa27v6zg8T+Z2R+Afxn4JpVfLlh1OVQrcomIZE7ai4LHm9nc3IPk/sTSNKn8+r4ETEREKi1tz+ozwENm9jjREVkEvL9UjaoUzVmJiGRTqmDl7j82s98DFxDBarG7by9py8roxQSLyjZDRER6cSTFYmuBOuI7vbY0zamM3CiggpWISDalmrMyszcATwFvAd4KPGlmry9lw8pJqesiItmWtmf1WeACd89dI3US8CPgZ6VqWDmpZyUikm1pswGrc4EKwN2fO4LnZp56ViIi2ZY24GxLFkc0ADN7F7CjdM0qL2Wui4hkW9pg9QHgfcB+M9ufPB4yqevqWYmIZFva1PXngAvMrAEwd09VF3Cw0JyViEi2pU5dN7OrgMsBN7NfuvtdpWtWealnJSKSbWlT1/+WqGKxClgNfCYpbjukKFaJiGRT2p7V1UTqehuAmX0dWAx8oVQNK6cXhwEVrUREMiltgoXlAhWAu7cyhJLoVG5JRCTb0vasHjGz7wBfJb7T3wc8WrJWlZl6ViIi2Za2Z/VRYDPwZeDfiJV8P1KqRpWbelYiItmWNnW9FfhUidtSMUpdFxHJtlTBysxqgKuAkwqf4+7/q0TtKi9FKxGRTEs7Z/UDYCrwCNBZuuZUhmKViEi2pQ1WC4FT3YdmCkKVLgoWEcm0tAkWTcCwUjYkCxSrRESyqc+elZl9MLm7GvgvM/sxcCC3392/UsK2lY3KLYmIZFt/w4DnFdxfQwwH5gyZr3bNWYmIZFufwcrd31OuhlSSelYiItnW3zDgy9z9ATN7XbH97n5vaZpVXupZiYhkW3/DgO8GHgA+UWSfA0MjWKlnJSKSaf0NA74v+XlpeZpTGepZiYhkW3/DgPP72u/uKwe2ORWmaCUikkn9DQP+vI99DswZwLZUjArZiohkW3/DgLPL1ZBKqlKwEhHJtLQVLDCzN5jZXyX3TzCzhf09Jzn2ZDN70MxWm9kjxYYWLXzJzFaY2VIz+42ZzU1/GgNDCRYiItmUKliZ2eeADwDvTTY5sRBjGl8DbnH3ecCNwG1FjrkCuBhY5O5nAL8G/j7l6x8zJViIiGRb2p7VG4E/AVoB3P0FYFR/TzKzycDZwPeSTXcCs81sVpHDhwMjzMyA0cDGlG07ZkpdFxHJtrRV1w+4e6flvtXTmwFsdvcOAHd3M2sCZgLrCo77GXAJsAXYB2wCXlHsBc3sBuCG3OMxY8YcaZsOf83kp2KViEg2pe1ZrTeziwA3syoz+xtgWcrn9owBxSLe2cCpwHTgBGIY8N+Kvpj7Te7emLs1NDSkbEbv1LMSEcm2tD2rjwLfBhYAbcDvgbeleN4GoNHMaty9Ixnim0EsOVLo3cBv3L0ZwMy+TRmrY6hnJSKSbWl7Vh3u/hpgLDDR3V9FzCv1yd23AU8Cb082XQWsc/d1PQ5dC7zSzHJrZr0eWJ6ybcdMPSsRkWxLG6zuMrNad29z9xYzm0HMM6VxPXC9ma0GPkWSUWhmt5rZFckxNxO9rWVmthS4FPhQ6rMQEZEhLe0w4B3A7cA1ZjYF+AXw8TRPdPdVwIVFtl9XcP8g8L6UbRlw6lmJiGRbqmDl7l82s1lmdjPwUuBzQ2V5ENCclYhI1h1JIdtvAt8AfgmsMLP5Q6WQrXpWIiLZdjSFbK9JbkOnkG3yU7FKRCSbVMgWVV0XEcm6/oYBh7v7QTOrK7bf3dtK06zyevEqZUUrEZFM6m8YcDFRXaKF+CovrD7hQHWJ2lVWGgYUEcm2/oYBz05+pl5KZDBSgoWISLYN6SB0pBSrRESyqb85q+0U/w43ooj65JK0qszUsxIRybb+5qzOLUsrKkxzViIi2dbfnNX6cjWkktSzEhHJNs1ZoZ6ViEjWKVihnpWISNYpWKGelYhI1qWqum5mHyyyeQ/wiLuvGdgmlZ8pWomIZFra9axeC1wM/Dp5fBnwB+AfzOxz7v6NUjSuXBSrRESy7UiGARe4+5vc/U3AQqAVeAlwQ0laVkYqZCsikm1pg9Usd9+Qe5Dcn+fuW4COkrSsApRgISKSTWmD1VYz+7SZTTOzqWb218AuM6tmCHRINAwoIpJtaYPVO4nq68uBFcA5wLuAYcm+QU2p6yIi2ZYqwcLdNwNX97J72cA1pzJe7FkpWImIZFLa1PUa4GPA5cRo2S+Bf3X3ITFfpQQLEZFsS5u6fhNwEvC15PF7gdnAR0vRqHLTnJWISLalDVaXAIvcvQvAzO4BnihVo8pNc1YiItmWNsHCehxrdF/iflAbMiciIjJEpe1Z3QfcZ2a3EaNl7wb+X6kaVW7qWYmIZFvaYPVJ4HrgTURH5C7gllI1qtw0ZyUikm1pU9e7gH9PbkOPelYiIpnWZ7Aysxv72u/unxzY5lSGelYiItnWX8+qtSytqDDNWYmIZFufwcrd/+5Y38DMTga+DUwEmoF3u/vKIsctBP4VmEJkHv61u//4WN8/VRvL8SYiInLU0iZYHIuvAbe4+7fM7GrgNuDCwgPMrA74CfAud/9DUjFjXBnaFu+f/OxSz0pEJJNKuqy9mU0mCuB+L9l0JzDbzGb1OPStwGJ3/wOAu3e4+/ZStq17O+OnYpWISDaVNFgBM4DNuRqC7u5AEzCzx3HzgQNmdo+ZLTGz75jZpGIvaGY3mNnG3K2lpeWYG/niMKCilYhIJqUOVma2yMzemtwfZ2bTUj61ZwgoNkU0DHg1cS3XWcAG4OaiL+Z+k7s35m4NDQ0pm9E79axERLItVbAysw8QSRKfTzaNB25P8dQNQGMyB4WZGdHbaupx3HrgN+6+Kel93Q68JE3bBoJS10VEsi1tz+p64AJgL4C7PwdM7u9J7r4NeBJ4e7LpKmCdu6/rcegPgfPMbHTy+DXAUynbdsyUui4ikm1pswEPuft+s24jeGnXsroe+JaZfZoIdu8CMLNbgbvd/W53bzKzfwAWm1kHsAl4f8rXHzCKVSIi2ZQ2WG03s3kk3+dm9g5iiK9f7r6KHqnqyfbrejz+DvCdlO0ZUOpZiYhkW9pg9T+A/wBOMbN1QBvw+hK1qew0ZyUikm1pC9k+a2YXAKcQ3+2r3L2zpC0rI1O0EhHJtLTZgP8EnOruT7v7yqEUqHIM6FSwEhHJpLTZgC3APWb2iJl9wMzGlLJRlTByGOxvr3QrRESkmFTByt0/5+5zgE8BLwWeM7M011kNGvW10KZgJSKSSUdUyNbd/8vM9iQP3wy8beCbVBn1tbCzLTICTWXYRUQyJe2c1WQz+7iZLSPSy5cCJ5a0ZWVWPyyqrh9Ie/WYiIiUTdqe1UrgDuB97v5QCdtTMfW18bP1UMxfiYhIdqQNVjPcfX9JW1JhLwar9lglUkREsqPPYGVm17j7j4D3WJGJHHf/SqkaVm65YNV2qLLtEBGRw/XXs1oA/Ag4r8i+IXVVUn0y9NdyCJ7cDNNGwdRRlW2TiIiEPoOVu382ufsJd99RuM/MhtRoWa5ntaMNnt0Z9z9yWEVDERGphLQXBd+fctugNXIYVBls2pvfdmjI1ekQERmc+gxWZlZjZnVAlZmNNLO65DYNqCtPE8ujymDMiO5VLDbu6f14EREpn/56Vp8hSi0tBFqT+y3A06RbKXhQGTei++PccKCIiFRWn8HK3f/O3auAW9y9quA21t0/39dzB6OxI/P3J9TB6h2we0gn7IuIDA5pawP+OeSHBXO30jat/MYnwaq2Gi6ZHemOy7ZUtEkiIkL6ckvnJaWWDgD7Cm5DyrgkWI0eHqnrBuw7WNEmiYgI6StY/CtwHfBV4GLgo8CQGyDLDQOOGRHFbOuGqRK7iEgWpE1dH+buDwM17r7P3b8IXFHCdlXEiBp47Ty4YGY8rtOyISIimZC2Z5WrRb7TzBYBGxliVddz5k7I368bFgkWWjZERKSy0vasfmBmE4C/B34HNAE3l6xVGVE3DDq6oL3g4mB3WLlNNQRFRMopVc/K3f85uXt/ErRGuPuQS7DoqS6pF9jaDtVV8NgmqKmCB5ugcTRceXpl2ycicrxIFazM7HVFtu0Blrv7kK3zUJfUC3xsU1S2WN+c39d8oCJNEhE5LqWds/qfwLnAsuTxQmAJMMPMrnP3e0rQtorL9aye2d77PhERKb20c1ZrgfPd/Wx3Pxt4CfAkcCnwhVI1rtLqCwLSlIb8dVgiIlJeaYPVQnd/IvfA3Z8EznL3p4lrZ4ekkbX5+9csgLOm5R8rpV1EpHzSBqs2M7s29yC535U8HFKLMBYaPTxKMF10YqSuN46J6uwQwcoLztw9qrR3DdnfhohI5aQNVu8BPm5mB8ysDfg48GdJfcBPlKx1FVZTBW9bBGedEI/HjIB3nQ3zJ0VQajkED6yHPQdiXuuulfDIhoo2WURkSEqbuv40cK6ZjQLM3QuWKOSXJWlZRjXUQv3wuP/YJli+NYJWddLjemHIJ/SLiJRf2kK21Wb2MeDv3X2vmZ1kZpelfO7JZvagma02s0fMbH4fx44ws5Vm9ljK9ldELvFi+db4+ezOfCr7sOrKtElEZChLOwz4r8AC4PLk8U7gH1M+92vEeljzgBuB2/o49ovA4pSvWzGFaesT6mJIMNejqhqy6SYiIpWTNli91N3fRywRgrs3A7V9PgMws8nA2cD3kk13ArPNbFaRY18OnAx8N2WbKqau4MyvOLV7gFKWoIjIwEsbrLrVazCz6pTPnQFsdvcOAHd3oq7gzB6vVw/8C/Dn/b2gmd1gZhtzt5aWlnRnMIAm18OiaXDtGdAwHCbV5/ftV7ASERlwaYPVUjN7G2BJr+grREHbNHomcxcbKPsScLO7b+r3xdxvcvfG3K2hoSFlMwZOdRW8fBZMTILUmOH5fepZiYgMvLTB6gZi0cVpwMPJ8/4qxfM2AI1mVgMR6YjeVlOP4y4C/tbM1gE/ABaa2YqUbau4BVPz9w91RqV2EREZOKmClbu3uPv17j4lub3P3fsdf3P3bURZprcnm64C1rn7uh7HneHus9x9FvAWYJm7D5qa5tNHw/vPi6FBiGuvDhUsK9LeGddh7W+HJzbrwmERkSOVtpAtZnY+cFLhc9z9Oymeej3wLTP7NLAXeFfyercCd7v73UfU4owaXgMjkyzBpVugthouTGbmHt8Ej26CaaMia3DcSJg9rnJtFREZbNIuEfLvwKuJSuu5PoMD/QYrd18FXFhk+3W9HP9bosL7oFNbcI3V9ta4TaiDp5Oq7bn09p1tClYiIkcibc/qcmC+u2sVpz5MKKjKvr45bvMnRYWLQrvaytkqEZHBL22CxQsKVP2bPgbevqh7dfaVSa9qeEGva6eClYjIEUnbs3rQzH5IZOq9GLTc/d6StGoQGzcSTpsMz++OHlVHFzSOhppqWLc7jtm1P5IsVO1CRCSdtMHq/OTnRwq2OaBgVcSEOnjHWTFX9atnI7V9W0sEq/ph0NoOa3bAKZMq3VIRkcEhbdX1S0vdkKHo1IkwsS5uo2qjt/WyE+GXa+D+Z6Pyxfi6SrdSRCT7+pyzMrMZ/b2AmZ0wcM0ZWswiIJnB1FExnzV7HFx2UuzfUuRKNdc1WCIih+kvweL7ZnarmV1qZiNyG81slpl90MweBi4obROHnslJmabtrfFz70FoOQgPNcE3H+9+QbGIiPQzDOjuF5nZNcBngJea2UFgBPACUUH9GnfvWTpJ+jFqeFyTtaMVlm2B3z7ffX/zfphc/pKHIiKZ1e+clbv/CPhRUt9vItDWY6VgOUJmMY+1ow3qkt/kyBrY3xH39x1UsBIRKZT2OivcvcPdtyhQDYyJ9THct243jB4e2YMXnRj79h3q+7kiIseb1MFKBta0UfGzoytS3YfXwEnjY9u+g5Vrl4hIFilYVciMMfn7E5L09fraWOyrt2D10Aa4c3nJmyYikjkKVhWSq9AOUfUCYlHH+tp8sHpsE9y+BDqT9bHW7YbN+5QtKCLHn1TBysyuN7Mxyf2bzewxM7u4tE0b+k5NKlhMqs9vGzU8H6ye2R6lmXbvj+uvmvfH9p6FcUVEhrq0PasPufseM3sZsIBIZf+n0jXr+HDZHHjrmflhQIhgtb8DHtkQQQoia3B/O7QnPawWzWmJyHEmbbBKkqq5DPiOu9/HESzcKMVVV3UPVBBJFsOq4OGN+W07WqG5oOZ9q3pWInKcSRususzsLcCbgV8n22pL06Tj29wJ8O5z8o+rLJYU2VMQrDQMKCLHm7S9ow8DnwK+7u7rzGwe8JvSNev4NqIG3nN2zFctboKmPflVhiEfrDbthQPtcNKEyrRTRKRc0lZdfwh4I4CZGbEY40f6fJIck4bhcXthH2xrzc9XVVl+zurHK+Ln9edBbU0kYfx8VWQUXjqnMu0WESmFtNmAt5nZWDOrBZYAW83sgyVtmQBw/gy47lx49cnw2nnQUJss6liQvr6uORZzXL41liFZvrVizRURKYm0c1bnuHsz8GrgSWAqcH2pGiXdjRwG8ybGfFZDbWQJPrQhv//ZnXDvqu4FcbXUiIgMJWmDVW4B9ouBe5L6gF2laZL05dxGGFYNT76Q37Z5b6xEDPkyTgc7Dn+uiMhglTZYbTGzrwLXAL8ys2FAdemaJb05cSy8am7+8ZSGuC6rrR3mjIfG0bH9+0vhl8/mj3uwCZ7eXtamiogMmLTB6m3AM8BbkuHA6cBNpWqU9O3Esfn7uZ6UA/XDoC65oKDlEKzdFcOBhzrh8U3wq2c1PCgig1PabMAdZvY14AwzewmwzN2/VdKWSa/M4D3nQHtnJFTk1NdCXUHNwUOdsQrxgYIhwW2tMWQ4qR6mjipfm0VEjkWqYGVmLwXuALYS81eTzOxqd19cysZJ7xqSHtT21vy2nsEqt7+9IHPwkQ2RPQjwkQuLv3bzfhg9ItLkRUSyIO0w4E3EEvZnufsiYu7qn0vWKkmtoaCOSH1t3Aptb40KGDm5QAXFkzD2HYTvLoGfrBzIVoqIHJu0wWqEuz+Qe+DuDwIjStMkORKFwal+WPelR4wIVrv2Ry/pDad1f+7GIms+53pqm/bC+t2H7xcRqYS05ZbazOxyd/8VgJldArT1+Qwpi549q9qCHM1J9RF8qi3WzJo5Ft40Hzodfvo0bGiOwrnLtsCjG+FQVwS8nOXbojLGpHqo0cpnIlJBaYPVR4E7zewgkXg2HLiqZK2S1Kqropbgoc74CXEdVm11BJltSU/ptGRl4uljIiOwbhhsaYmK7r99PoJUeyc0J/NbE+sim3DtLjhrGlw0q+ynJiLyolR/L7v7Y8Bc4E3A1cDJ7v54muea2clm9qCZrTazR8xsfpFjLjOzh81spZktN7MvJjUIJYWxI2D08MgShCjP9M6zui/qOH10/r5ZXJ+1sw1WbIttr5kHs8fF/dpqOGVi/viV26Jqxvef6l5QV0SkXPoMVmZWl7sBw4C1wHPAsGRbGl8DbnH3ecCNwG1FjtkNXOvu84FzgVcA16Z8/ePeq+bCH5+Sf1xTFbfCYHXC6O7PmdIQ9QSXbolAN20UzEh6X4c6o7RTtcV6Wwc74XtLYhHIRzZymF374UfL8otFiogMtP56Vi3AvuRn7v6+gvt9MrPJwNnA95JNdwKzzWxW4XHu/qS7r03uHyCK5apueEpjR8L4In86TKiLJIuG2ghIhaY05O8vmBK9rdzFxgumROr6+86Dq06H4QXzYCOKDBwv2xJDivetOdYzEREprs85K3c/1mn1GcBmd+9IXs/NrAmYCawr9gQzm0oMNb7uGN/7uDesGs6dHkuN9BxUnZwEKwMWTYv7Y0fCu87OJ1kMS4LUWSfkC+cWW6X4QHv83N4K+9u7ZySKiAyEcixN37PAT69zUWY2GvgZcKO7P9HLMTcAN+QejxkzZiDaOGRdMLP49hE1cM2C6EFVF/xJ0rMHBhHMOrtg2daoiJHTeigyEHcU5IX+ZCWcPDGe018G4cY98NwuePksXYAsIn0rdbDaADSaWY27dyRJEzOApp4Hmtko4BfA3e7ea93BZN+L+xsbG1Xt7iilLbc0rDqC3o42WLcbtuyD3Qei1uAZU2Ouava4WCBy4x7Y0RTZhvMn9/26T2yG9c0wcwzMHn/MpyMiQ1hJr55x923E+ldvTzZdBaxz93WFx5lZAxGo7nP3z5eyTXL0Rg+PbvKPlkeggkjQcGBiPfzRXFg4Jbav3dX3a7nD1mRZk2VaLFJE+lGOSz2vB643s9XAp4D3ApjZrWZ2RXLMx4CXAFea2ZLk9pkytE2OwKgeQ4TnTYepydzXCaNiSPCSObFMSVNzZBX2pvlAvsDu+mbYc6AULRaRoaLkc1buvgo4rGSqu19XcP+LwBdL3RY5NoUXHb95YVzfdf4M2Heo+1zXnPFRymlDcwS4JzfDpSd1r66xJcklnT85ruNasQ1e2mN+bXFTDDG+riAtf/PeSLlv1FSlyHFFRXQktTnjYe54+NMFUb7JLG49kzJmJRcXr2+GxRtg9c5IpMhxh1U74v5LGmHM8AhYTc3wUFNU0jjUCY9tiud1FPTQ7lwBd63Uulwix5tyZAPKEDG8Bl57Sv/HjRkRva5cdQyAxethVxuc3xgBaMMeOG1S9LxOmxyp8T99Oo5t2hOJGzl7D8Z1ZJ1d+W17DkSq/ZHa3hrBsVb/8kUGFfWspCRmjs3fr6mC1vbI/nt4Y1TBqK2Gi06M/YWlnSASLx4tqJSxJ0mX31eQNv9CS/fnuEfa/O+e771Nbe3ww2X5a8ZEZPDQ35dSEoumxdzSzLExjPf79VEJ44nNsf/c6TAiuXh49Igo9/TCPrhgRgST5gNxQZ4Dew/Akhfgme3519+yD8aNiF7XzDERiDbsiaVNzp0eFybnLoTetAfGjIz5ry7PF/cVkcFDwUpKYswIuLSgYNa8iRFY/vv5CERnTu1+/KVzogL8zLH5ns+Z0yJIbW+FNTuho2AYcH0zrNoe13aNHJava9jl8If1sHlfVI6/eHbMcU0bFRcrQwxHuh9e1SNna0u8zi9Wwx+d3L0IsIhUhoKVlIVZBLArTiu+f0Jd3HL3d7bFNVtLt8DT27sfe/rk/HzY9NGwrQVW74jhxvph+eSNfQejp+VE8MoFu4Od0RPrcrjnmUjyOGlC7NuyL64jy3lmez5YdXbB87sj0SRXccM9VlweoRJTIiWlYCWZ85LG6GWNHRlDh/s7ogJ8Z5IBePGsCEI1VfDG+TG8d9+auNbrwpnRC5tQB79fl7/wGLoP/32jYIGbhzdGgFs07fAhwsLyUk9vh9+shctPioSPKQ0RzH79HFx7Zj7YFuryqJdYuKKziBw5BSvJnLkT4gbR41m7Cy6ZDVtbI629phrefEbMaVUly5i89cz88xckVTTmT84Hq9njoleUmwcrtLMtbs/tgpOT933T6fDg+giauSHDpubY96vn4ucVp0WwcmJYsliw+u/n45j3nK3el8ixUDagZNqlc+C950bQeulMeMXs2F5bna8K35tc4DllIpwzPe7nshSnNkQAyR2Ts2ZnDCVOHx2V6Q90xEXP7tGbK9TUnN+2ucc+iHW+VmyN4cfCYr8icuTUs5Iha3gNvO/cCGrVVfDaeTC5PnpCDbWxbUpDBKhRtTFk19oO45IeUm7xyntXRZIHRJDMlZFasS3fS3t+dyxA+SenRqWPji54uCm/f2dbXEjdvB+mJ8kgy7bEfNtlJ+WTPfpK/BA5nilYyZBWOPQ2d8Lh+3Pres0cG4HoyRfyZaVmjY06h1taItDsb4dXnxy9qeVb87UNT5sU81lbWmLIcu/BqL4B0YPb0hLBanFTZC++/tRIs1/yQhwze3xU/bhvdSR+XHl630umHOqM/bklWDxZ8XnaqPz59Ka9M56ngCiDjfkgr1vT2NjoGzcWWWtdJIUuh8c3RWp93TB4eAMsnBqZi32555noTU0bFb2ppVviuWNHRGo+REB588JInW/vzCeIFDIi+WJCXcx7QaThz50QWY89g8rBDrj9qbjGbP7kSEJZsTV6eZPrYy6v5/mt3BbJJ4c6I9PxlSdFgN3aEsFtMKwl5g4/XxW/mzOnVbo1UipmtsndG4vtU89KjmtVBucV/Ne4aFa6502sy6exj6iJDMZV2/OB6poFUTOxrjZ+FrsQ+WUzI1lkcVMEqlwvbMOe/O38xghY40ZG6vzv1sWil62HolhwYcLIzrZ4nf3tcOLYGAb9+TOwrjnOM1dIePnWmG9bsxPOmhbFiNc3RxJKdVUEht0HIiBmpQfW1h6/79ZDClbHKwUrkaMwb2IEoFMLSkWdOikuaL54VveFLRvHxLEvnxX72zsju3FhcmH0gikxx1VbHQGkeX/MZT27M2611VGlY8OeuEZs2qi4sPlgZz5QjUsqdNyd1FecNQ5OGh+Bas64SPbIBdKdbfksySdfiPXEOrqi3Qunxlplq3ZENZHziv6N27/nd0UwzZXUOtagt3t/vu1dPjh6gzKwFKxEjsL4usMvcD77hOhp9Uxhv3Bm9GDqamOl5Q17utdOLOz15LITD3VGHcODHdFTenZXJIPMGR/Xgx3qiPqI966KBJLzpsP9yYKYtdXxPut2x3Vql8+N91i9I+oythyK4y6YEUOXuYulN+yJntWqHXF8blgzV/mjyyOVv742ejmNoyMInzwhH4x2749e26Mb4/q4Lftibu+tZ+YDTFt7zOtNro/3SyMXaDs9gvn4IpcJ5HR2waObYqizv+FcGTwUrEQGSHVV8WutqiwCFUT6/bbW/r9Ea6vhLQsjCGxK1vDKLb0CUFMbCSBjhkfgKywJddXp8IOl0et66YkxHAhw+pQISGt2xuOFU2NIrdpi6ZXNe6OafW11XGf2s6fjYuutLdGGkcNiuLBuWASc3BDk09sii/L1p8LPnum+kOaWpAe3vjnm1loORYLHlpYIhFfOh4bh0VOE7kGosyvOv8ryPSuIzMy+gtXyrREsWw/CK+f2/Xsul+2t8QdGLhNUjpyClUgZTW7oP2MvpybpbRX2wgpVV8E7zor7ZpE4MakeJtbDW86IubSGHmuN5YLkqNp81iNEsNvaAgf3R89tUn0ErB8ui6HCQm3tMQ+3vz0CSdOe2P6tJ+Ln1CRpIxdoIZIjemo+AN9dEokka3ZGcHrrorisoL0Tvr802vHaedGbytnRBsVWqsktIfN4Uiz5+d19Dxl2ObQdOvx3VMyu/TG0edYJRzcEee+q6E2eNx0umNn/8XI4BSuRQaxwLmj+5Pz9ifXFjx87ovj+k8ZHRfxpo2I4EyLT8FUnRwLIrLHx5X9eYwztXTAjgmlTc/Rk1u2OXlbj6Bgera6KYNZ6CO5fAzv3x8XWre3x2q88KXpYj26Ep7bk25ErZ7W4KXpoew7EStPrmqPtBzpiHm9iXQSZsSOi3NXscfBgUzxuPRQBr+VQtGvO+Ag2K7dGT3LU8JgTvGtlDLdeOifmDds74xxzySk57jGPt7UlenTDq+O4C2ZEe4bXwMY9sf8lSUKMe/w+qixeN1e26/HN8V6FAXLtrhhandLLHzFbW+KPi1zv/Hil1HWR48gL++CO5RF0LpjRfV9HZ743d6S27Ive0fkz8vNvObvaIljMGQ+3PRZf8O85JwLK3gPwvSXxBT9zDKzdna8DObIm5r1yTpkYwejhgv/uhTUjc0bUxOUEdy6PgHHe9AgSXR6JKF1d8cX/wr44tr0zrnXr8ggcw5Nh0D0HI3A1Ned7hrPGRYHknW354DtyWATefQcjC3TqKPhlEtyuPSN+3rki5ui2tcYSNhfOjGSbgx1x2QPARy48/Pe672C+x/rWpP5ke2c899RJ+QvXC7UeiqHankkt7kkg76MnebAjhiwbewxXbm+N8yh2OcVA6it1XcFK5DjiDs/siJ7IiAqMqyzdEr2ll8/Kb3tuV/ycPhq+/1QM5104M0ps/XRltPP8GfFF3dkF//FUVBmZmFybVjifNbw6htnOmBoB9K6V+QSSXG8rZ/TwGGL89XOHl8OqqYrn1dfGUGF1Vbxfbg4uN1+XW4ctZ/a4yPT85hOHn/tr50W9yT0HI9AUrs8GMH5k9PTmT47zhahy8ttkQdGRNXFeTXviPWeNi0BsFlmgl86Jdv/smQgqq3dGLzV3MfziprhY/ZSJkRTzyMYI6i2HYrj2zKnwn0ujF/ynC7v39L77ZAzbnjQ+ficXz4rr95oPwMtOjADaX/mzNBSsRGRQONgRX7653lmx8lOdXfk5MYjg9x9PxVzbhT3mg+5fE9mNY0fEBdMrkiHLjXsjQzN3Xd3qHfDUC5E5ed+afHmtnHOmR3bhz1dFb+eq06NHN7k+vsj3HYrgV1ilv6d3JvOLv1jd/bq7wuHRYdXxxf+ymdHG3IXi50yPi9cL5Y7NmdIQj3cVBO8RSYBr3h/Bq+dzZo7JzzmeODb/fqdMjN9lQ23MURauUgAR5O55Ju7/2TmxQveeg3D1gnxllaOhYCUiQ9r+9pg76pn8sGEP/GRlZGHmihnvORBzY5fMjnm5np7fDX9YF9mNO/dHILt0Tnzxd3bF9W11BWW8Nu7JX3P31JZ8qa1F06JHlgsyH74gAmxuzbSaKrju3Bh+vOXROOZdZ8FPnu6eUTmlIYLAY5si6I4ZEXN9z+/OH1Nl8TqQz9Ycm6ykndveOAZeNTfOLZcR2lNdMqS592D0HifV5wPr/MnxPsu3dn/Oy2fBA+ujZ/zG+cVfNy0FKxE5bm1tiS/dclxI7A5ffSSGED94fgSrp7dHkMstXQORDDKiJp/MsnZXBIqpo2JY884VUSLrhNGRtNIzIeaJzREgRtbE8GKnw0+fjmMvnxurZV8wI96j9RDU1kTPDyJ4Ne+Paigb9kSPscujB3rZnJjPW7I5hkYPFMwZvuWMGIq9+aF4PLm+ew/x1SfHxfLHQsFKRKRM9rfH3NOxXJDcc6izp817I6CdNimCE0RvbNTw9EF5+Vb47Vq4ZmEM961vjrm03PN3tMZcVt2wGJK8ZHa058GmGEp903z44fJ437ph8O6z01/k3RsFKxGRIcQ9hhxPGh8B6mhf40BHZDMeraZm2Lxv4KqFqJCtiMgQYhZzYsf6GscSqCAuWO/tovWBppWCRUQk8xSsREQk8xSsREQk8xSsREQk8xSsREQk80oerMzsZDN70MxWm9kjZlb0Gmcze6+ZrTGz58zsFjNTpqKIiADl6Vl9DbjF3ecBNwK39TzAzGYDnwcuAuYCU4H3lqFtIiIyCJQ0WJnZZOBs4HvJpjuB2WY2q8ehVwN3uftWj6uUvwpcW8q2iYjI4FHqntUMYLO7dwAkgagJ6LlW5kxgfcHjdUWOAcDMbjCzjblbS0vLwLdaREQypRzzQj3rOfVWucpTHIO73wTc9OKBZgfNbHtvx6fUAAz2qKdzyAadQ3YMhfM43s5hUm87Sh2sNgCNZlbj7h1mZkRvq6nHcU3ArILHJxY5pih3P8rKWHlmtrG3elSDhc4hG3QO2TEUzkPnkFfSYUB33wY8Cbw92XQVsM7d1/U49E7gSjObkgS0DwA/KGXbRERk8ChHNuD1wPVmthr4FEmWn5ndamZXALj7WuCzwAPAc8A2imQNiojI8ankc1buvgq4sMj263o8/jrw9VK3pxc39X9I5ukcskHnkB1D4Tx0DolBv56ViIgMfSq3JCIimadgJSIimXdcB6u0dQuzxszWmdkzZrYkub052T7ZzH6R1FhcbmYXVbqtOWb25aTdbmYLCrb32mYzqzOz75vZs8ln9KbKtP7F9vR2Dr81s7UFn8dfFOzL2jmMMLOfJG1ZkvzuZyX7BsVn0c85DKbP4n4zW5q08/dmtijZPig+h6Q9vZ3DwH8O7n7c3oD/At6d3L8aWFzpNqVs9zpgQZHt3wA+l9w/j6gKUlPp9ibtuRho7Nn2vtoM/C3wreT+bGALMC6D5/Bb4E96eU7WzmEE8Dry89UfBu4fTJ9FP+cwmD6LsQX33wg8MZg+h37OYcA/h+O2Z2Xp6xYOJn8K3Azg7o8CW4niwBXn7r9z941FdvXV5jcX7Hse+B3whtK3trg+zqEvWTuHA+5+ryffFMBDwJzk/qD4LPo5h75k5hySNjQXPBwDdCX3B8XnkLShueBh4Tn05ajO4bgNVqSvW5hVt5vZsuR6tUlmNgGocvfC0lPryPD5pGhz6pqRGfCl5PP4TzMr/OLM+jl8FPjZIP8sPgr8rODxoPkszOw7ZrYB+ALwrsH4OfQ8h4JdA/o5HM/BCtLXLcyai939TKJnuBP4drJ9MJ5Pf21OVTOywt7h7qcBZwC/B+7psT+T52BmnwZOBj6TbBp0n0WRcxhUn4W7v9PdZwB/A3wpt7nHYZn+HHo5hwH/HI7nYPVi3UIAs17rFmaOuzclP9uBfwFe7u47AcyssBBk6hqLlZCizUddM7Kc3H1D8tPd/d+AOclfyJDRczCzvwTeBLzW3dsG42fR8xxgcH4WAO7+beDS3OPB9Dnk5M7BzCaU4nM4boOVp69bmClmVm9mYws2XUucB8CPgA8lx51HLGL5h7I28Mj11ebCfbOBVwB3V6CNvTKzGjObUvD4KmBr7sufDJ6Dmd1A/Lt5VY85h0HzWRQ7h8H0WZjZaDM7oeDxlcQoyS4GyefQxznsLcnnUKkskizcgFOAxcBq4DHg9Eq3KUWb5xDBaSmwDPgpMCvZNwW4H1gDrABeUen2FrT7ZmAj0EFk/zzbX5uBeuA/gWeTz+jqrJ1D0sbHks/iKeDXwJkZPodGYgjmOWBJcnt4MH0WvZ3DYPosiFGcRwra+itg0SD7HIqeQ6k+B5VbEhGRzDtuhwFFRGTwULASEZHMU7ASEZHMU7ASEZHMU7ASEZHMU7ASEZHMU7CSsjCzWWb2/hK99iIz+3nB4zvMbLPFUh4NR/F67zazeQPbyiNnZgvN7HcWy8EsM7NbzGx4wf7zk+UXVpvZr81sWsG+Xpe/6WsJih7vf66Z3Z7cH2tmnyzReV5iZn9U8PgEM/tNid7rTDO7txSvLaWlYCXlMgsoSbAC/h74x4LHXyUuTjxa7wYqHqyAA8CH3f1U4nzGAB+HF8uD3Q78D3efB/w/4KaC534NuCXZdyNwW8G+/w085O4nA+8hiiLX9Hxzd3/M3d+WPBwLHFWwKvbaPVwCvBis3H2zu1/a++FHz92fAjrM7JJSvL6UUKWuftZtaN6AkcTV6SuJq9dz6ww9A7QR1QbuTradDPwceDQ59oMFr+PA54AHiKvcr+3l/WYC63vZ50BDH219PVEJZAmwnFim4DqgBVibbH9dcuxfElfrPwHcC8xItn8O+GGybTlRNqYk6wslbbg1uX8esKJg3yhgPzAMmAw0k18DyYiKG7OSxy3ApILnPgJcUuT9LgEeS+7/gqjcsaRg29Tk3B9Jfo//q+C564jisr8hgurU5P7jRFWGLyftWpS0bVvy2n9L/GGzo+C1XpP83pcC/w3ML2jfEuAryb+fFcC5yb5JRBWIZcnzvlnwem8Bbq/0/xXdjvDff6UboNvQugFXkgSo5PH45OeLX3zJ42oiSJ2aPK5LvlTOTh478Nnk/hxgRy5A9Hi/dwD/2Utb+gtWTwEvTe5XkSwkR4+F44C3ArcA1QXv+dPk/ueAF4ApyeOvAF/p5f3uIF8eqOftsHPr8dx6IuC/IXl8FXBvj2O2EcH7HGBlj32PEItHTgDaeuz7IfDOIu/54mfWM4Ak2+4jVgAAqCEC2pXJ43VE7y5XJWdE7rNIPvt7SMrsJL/Dfyp43Rffiwi8O4CFyeO3AcsL2tdOPkB9ALgvuf8XRM+y27/D5P5MYEul/6/odmS3/rrnIkfqKeBUM/sK8Vdwb/MDpwCnAz+IES0gegfzib+iAW4FcPe1ZvYH4OXAf/R4nUbiL/Oj8WvgX8zsDiLALunluDcC5wKPJ22tBjoL9t/j7luT+7cQX/6Hcferj6aRZjaM6K3e7+4/LXzJnocOwL60baoHLgOmFHx+DcCpBYd905PoQPwx8I/J/JgRQWgJEcD7cj6wxN2XAbj77WZ2c8H83Cp3fyy5v5jofUIsyPgXZvZ/iH+H9xW85pak3cM8Vi6QQUDBSgZUEljmE19klwM3mtmiIoca8ddzsX29vnyRbW3E0OMRc/cbzOx0YmmGb5vZ7e5+Y5FDDfiCu3/jGNpJEhTn9vKc13uyrEKP5wwjgt8LwMcKdnVbZsHMRhHB/gVirqvRzGrcvaNw+Rt332lmmNkkzy/wdzTLTFQR53leH1/4LQX3byB6dee7+wEzu4nobfXHKP77zG07ULCtk+Q7zd0XJ//uLid6oV8ws7PcvTN533YFqsFFCRYyoMyskVjG5m7ir9zcF+VeIkEgZxXQZmbvLHjuXDMbX3DMnyXbZxHLehdb7mQp3f+aP5K2nuruKzzW2/l34IJkV8+23g18MNc2MxtmZmcV7P9jM5uc3H8vUX36MO5+tbsv6uVWLFDVAD8glo14f0EvBWLuZ0RBosD1wE/cvd37X/7maJaS2QvU5ZIl3H0fsajepwrae0Ly+Rczjhh6O5AsH3FNj9ceU/xpLAYWmdlpyXu8Bdjo7n32ppOlJ1rc/YfAR4iEmVxm6GnEvxsZRNSzkoG2EPjfyV/zVcB33X1p8iW3ysyWA2vd/Qozez3wzxaL6FUD24k5iZyDZvYAMVn+kWJf6MSX7EwzG+/uuwDM7G5iFWWS91zj7pcUee4/JCnqh4ge2p8n228B/o+ZfQL4tLt/12LhuN+amRP/b24jv47Yr4Hbki/ItXRf2vtYvJlYXHAp8GQy3PaAu3/I3bvM7O3AV81sJLCJfHCCCF7fslhJd2+PNv0V8F0zW5Oc+zvcvaOvhrj7riSNfZmZtbr7ucRndZOZLUsOayHmjTYWeYkvAz8ysyVJWwsD+l3AO5J9Pwa+U/C+283sHUTGYjWROPKnfbU1cQlwg5l1Ev+2PuHue5J9rwHuTPEakiFaIkQyKQkKo9y9JcWxnyR6c1/q79iBZmafIxIH/rK/Y6XyzKyWSOx5pbvvqHR7JD0NA8pQ8H+B1ko3QgaF2cBfK1ANPupZiYhI5qlnJSIimadgJSIimadgJSIimadgJSIimadgJSIimadgJSIimff/AXNtCiepf+tqAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(f'step (1 step = {plot_every} iterations)')\n",
    "plt.ylabel('loss (negative log likelihood)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1606363743410,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "G5q-p8DwrAaN",
    "outputId": "42e0ba38-6128-4040-f923-d40205d2f3fd"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 480x320 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"264.343803pt\" version=\"1.1\" viewBox=\"0 0 385.78125 264.343803\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-11-29T02:44:54.287144</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 264.343803 \nL 385.78125 264.343803 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 226.787553 \nL 378.58125 226.787553 \nL 378.58125 9.347553 \nL 43.78125 9.347553 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m754f6ce508\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#m754f6ce508\" y=\"226.787553\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(55.818182 241.385991)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"105.115134\" xlink:href=\"#m754f6ce508\" y=\"226.787553\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(101.933884 241.385991)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"151.230837\" xlink:href=\"#m754f6ce508\" y=\"226.787553\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(144.868337 241.385991)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"197.346539\" xlink:href=\"#m754f6ce508\" y=\"226.787553\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(190.984039 241.385991)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"243.462242\" xlink:href=\"#m754f6ce508\" y=\"226.787553\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(237.099742 241.385991)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"289.577944\" xlink:href=\"#m754f6ce508\" y=\"226.787553\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(283.215444 241.385991)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"335.693647\" xlink:href=\"#m754f6ce508\" y=\"226.787553\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <g transform=\"translate(329.331147 241.385991)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- step (1 step = 14041 iterations) -->\n     <g transform=\"translate(130.698438 255.064116)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path id=\"DejaVuSans-32\"/>\n       <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n       <path d=\"M 10.59375 45.40625 \nL 73.1875 45.40625 \nL 73.1875 37.203125 \nL 10.59375 37.203125 \nz\nM 10.59375 25.484375 \nL 73.1875 25.484375 \nL 73.1875 17.1875 \nL 10.59375 17.1875 \nz\n\" id=\"DejaVuSans-61\"/>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n       <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"52.099609\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"91.308594\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"216.308594\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"248.095703\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"287.109375\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"350.732422\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"382.519531\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"434.619141\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"473.828125\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"535.351562\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"598.828125\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"630.615234\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"714.404297\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"746.191406\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"809.814453\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"873.4375\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"937.060547\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"1000.683594\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"1064.306641\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1096.09375\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"1123.876953\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"1163.085938\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"1224.609375\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"1265.722656\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"1327.001953\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"1366.210938\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"1393.994141\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"1455.175781\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"1518.554688\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"1570.654297\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"ma809472bb4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma809472bb4\" y=\"216.903917\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 220.703136)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma809472bb4\" y=\"175.722977\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 179.522196)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma809472bb4\" y=\"134.542038\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 138.341256)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma809472bb4\" y=\"93.361098\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 97.160317)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma809472bb4\" y=\"52.180158\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.8 -->\n      <g transform=\"translate(20.878125 55.979377)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma809472bb4\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- F1-scores -->\n     <g transform=\"translate(14.798438 141.965991)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 51.703125 72.90625 \nL 51.703125 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.109375 \nL 48.578125 43.109375 \nL 48.578125 34.8125 \nL 19.671875 34.8125 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-70\"/>\n       <path d=\"M 4.890625 31.390625 \nL 31.203125 31.390625 \nL 31.203125 23.390625 \nL 4.890625 23.390625 \nz\n\" id=\"DejaVuSans-45\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"121.142578\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"157.226562\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"209.326172\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"264.306641\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"325.488281\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"364.351562\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"425.875\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p6eed3e7338)\" d=\"M 58.999432 216.903917 \nL 68.222572 54.858762 \nL 77.445713 42.509447 \nL 86.668853 33.365343 \nL 95.891994 30.205252 \nL 105.115134 28.343789 \nL 114.338275 29.021847 \nL 123.561415 26.486772 \nL 132.784556 27.461076 \nL 142.007696 25.047229 \nL 151.230837 25.759537 \nL 160.453977 24.525547 \nL 169.677118 23.258525 \nL 178.900258 23.553203 \nL 188.123399 23.663321 \nL 197.346539 23.885783 \nL 206.56968 23.080216 \nL 215.79282 22.758032 \nL 225.015961 22.085051 \nL 234.239101 21.574411 \nL 243.462242 21.719775 \nL 252.685382 21.345039 \nL 261.908523 20.890526 \nL 271.131663 21.238011 \nL 280.354804 22.037029 \nL 289.577944 22.048063 \nL 298.801085 22.672079 \nL 308.024225 21.277593 \nL 317.247366 20.126527 \nL 326.470506 21.137499 \nL 335.693647 19.781094 \nL 344.916787 20.201296 \nL 354.139928 19.23119 \nL 363.363068 20.550524 \n\" style=\"fill:none;stroke:#92c6ff;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p6eed3e7338)\" d=\"M 58.999432 216.903917 \nL 68.222572 59.941939 \nL 77.445713 52.341117 \nL 86.668853 47.405659 \nL 95.891994 46.01558 \nL 105.115134 45.05931 \nL 114.338275 45.912634 \nL 123.561415 45.579168 \nL 132.784556 48.819878 \nL 142.007696 42.68636 \nL 151.230837 44.331867 \nL 160.453977 42.096662 \nL 169.677118 40.37296 \nL 178.900258 41.113446 \nL 188.123399 41.890213 \nL 197.346539 41.955198 \nL 206.56968 40.879093 \nL 215.79282 41.565466 \nL 225.015961 40.682044 \nL 234.239101 39.419467 \nL 243.462242 40.074897 \nL 252.685382 39.040996 \nL 261.908523 40.540074 \nL 271.131663 39.135829 \nL 280.354804 42.991111 \nL 289.577944 41.482515 \nL 298.801085 40.870659 \nL 308.024225 40.509475 \nL 317.247366 39.189934 \nL 326.470506 40.894378 \nL 335.693647 39.678485 \nL 344.916787 39.307078 \nL 354.139928 39.223141 \nL 363.363068 39.324161 \n\" style=\"fill:none;stroke:#97f0aa;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p6eed3e7338)\" d=\"M 58.999432 216.903917 \nL 68.222572 73.241831 \nL 77.445713 64.944319 \nL 86.668853 60.36709 \nL 95.891994 59.688078 \nL 105.115134 60.53201 \nL 114.338275 63.215574 \nL 123.561415 60.409692 \nL 132.784556 61.798685 \nL 142.007696 56.84314 \nL 151.230837 58.493049 \nL 160.453977 59.168866 \nL 169.677118 57.910287 \nL 178.900258 56.674133 \nL 188.123399 57.630853 \nL 197.346539 54.659054 \nL 206.56968 56.411496 \nL 215.79282 57.299401 \nL 225.015961 56.407716 \nL 234.239101 54.469252 \nL 243.462242 55.590986 \nL 252.685382 54.002289 \nL 261.908523 55.046616 \nL 271.131663 56.257323 \nL 280.354804 57.576256 \nL 289.577944 56.871995 \nL 298.801085 57.194428 \nL 308.024225 54.763973 \nL 317.247366 53.42064 \nL 326.470506 55.54545 \nL 335.693647 53.318802 \nL 344.916787 55.662299 \nL 354.139928 55.298986 \nL 363.363068 55.183699 \n\" style=\"fill:none;stroke:#ff9f9a;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 226.787553 \nL 43.78125 9.347553 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 226.787553 \nL 378.58125 9.347553 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 226.787553 \nL 378.58125 226.787553 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 9.347553 \nL 378.58125 9.347553 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 316.30625 221.787553 \nL 371.58125 221.787553 \nQ 373.58125 221.787553 373.58125 219.787553 \nL 373.58125 176.753178 \nQ 373.58125 174.753178 371.58125 174.753178 \nL 316.30625 174.753178 \nQ 314.30625 174.753178 314.30625 176.753178 \nL 314.30625 219.787553 \nQ 314.30625 221.787553 316.30625 221.787553 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 318.30625 182.851616 \nL 338.30625 182.851616 \n\" style=\"fill:none;stroke:#92c6ff;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"text_16\">\n     <!-- train -->\n     <g transform=\"translate(346.30625 186.351616)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 318.30625 197.529741 \nL 338.30625 197.529741 \n\" style=\"fill:none;stroke:#97f0aa;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_17\">\n     <!-- val -->\n     <g transform=\"translate(346.30625 201.029741)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n     </g>\n    </g>\n    <g id=\"line2d_21\">\n     <path d=\"M 318.30625 212.207866 \nL 338.30625 212.207866 \n\" style=\"fill:none;stroke:#ff9f9a;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_22\"/>\n    <g id=\"text_18\">\n     <!-- test -->\n     <g transform=\"translate(346.30625 215.707866)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6eed3e7338\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"9.347553\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# plot F scores\n",
    "evaluation_steps = list(range(len(all_F)))\n",
    "\n",
    "train_F_scores = []\n",
    "val_F_scores = []\n",
    "test_F_scores = []\n",
    "\n",
    "for scores in all_F:\n",
    "    train_F_scores.append(scores[0])\n",
    "    val_F_scores.append(scores[1])\n",
    "    test_F_scores.append(scores[2])\n",
    "\n",
    "plt.plot(evaluation_steps, train_F_scores, label='train')\n",
    "plt.plot(evaluation_steps, val_F_scores, label='val')\n",
    "plt.plot(evaluation_steps, test_F_scores, label='test')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(f'step (1 step = {eval_every} iterations)')\n",
    "plt.ylabel('F1-scores')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHMRkihZpgGv"
   },
   "source": [
    "### Model Testing\n",
    "\n",
    "This is where we provide our readers with some fun, they can try out how the trained model functions on the sentences that you throw at it. Feel free to play around.\n",
    "\n",
    "\n",
    "##### LIVE: PRODUCTION!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T16:00:48.722675Z",
     "start_time": "2020-11-20T16:00:48.651864Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1606363743411,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "Lah0lGMQpgGv",
    "outputId": "ac607e12-558b-4680-b720-9371e4f1a550"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction:\nword : tag\nNTU : NA\nis : NA\na : NA\nuniversity : NA\nin : NA\nSingapore : LOC\n\n\nThe : NA\nMoon : NA\ncompletes : NA\none : NA\nrevolution : NA\naround : NA\nthe : NA\nEarth : ORG\nin : NA\nabout : NA\n28 : NA\ndays : NA\n\n\nSingapore : LOC\nis : NA\npart : NA\nof : NA\nSoutheast : LOC\nAsia : NA\nand : NA\nASEAN : NA\n\n\nThe : NA\ncapital : NA\nof : NA\nEngland : LOC\nis : NA\nLondon : LOC\n\n\nLee : PER\nKuan : NA\nYew : NA\nwas : NA\nthe : NA\nfirst : NA\nprime : NA\nminister : NA\nof : NA\nSingapore : LOC\n\n\nGeorge : ORG\nWashington : NA\nwas : NA\nthe : NA\nfirst : NA\npresident : NA\nof : NA\nUSA : LOC\n\n\nBill : PER\nGates : NA\nand : NA\nhis : NA\nwife : NA\nset : NA\nup : NA\nthe : NA\nBill : ORG\n& : NA\nMelinda : NA\nGates : NA\nFoundation : NA\n\n\n"
     ]
    }
   ],
   "source": [
    "model_testing_sentences = [\n",
    "    'NTU is a university in Singapore',\n",
    "    'The Moon completes one revolution around the Earth in about 28 days',\n",
    "    'Singapore is part of Southeast Asia and ASEAN',\n",
    "    'The capital of England is London',\n",
    "    'Lee Kuan Yew was the first prime minister of Singapore',\n",
    "    'George Washington was the first president of USA',\n",
    "    'Bill Gates and his wife set up the Bill & Melinda Gates Foundation',\n",
    "]\n",
    "\n",
    "#parameters\n",
    "lower=parameters['lower']\n",
    "\n",
    "#preprocessing\n",
    "final_test_data = []\n",
    "for sentence in model_testing_sentences:\n",
    "    s=sentence.split()\n",
    "    str_words = [w for w in s]\n",
    "    words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>'] for w in str_words]\n",
    "    \n",
    "    # Skip characters that are not in the training set\n",
    "    chars = [[char_to_id[c] for c in w if c in char_to_id] for w in str_words]\n",
    "    \n",
    "    final_test_data.append({\n",
    "        'str_words': str_words,\n",
    "        'words': words,\n",
    "        'chars': chars,\n",
    "    })\n",
    "\n",
    "#prediction\n",
    "predictions = []\n",
    "print(\"Prediction:\")\n",
    "print(\"word : tag\")\n",
    "for data in final_test_data:\n",
    "    words = data['str_words']\n",
    "    chars2 = data['chars']\n",
    "\n",
    "    d = {} \n",
    "    \n",
    "    # Padding the each word to max word size of that sentence\n",
    "    chars2_length = [len(c) for c in chars2]\n",
    "    char_maxl = max(chars2_length)\n",
    "    chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "    for i, c in enumerate(chars2):\n",
    "        chars2_mask[i, :chars2_length[i]] = c\n",
    "    chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "    dwords = Variable(torch.LongTensor(data['words']))\n",
    "\n",
    "    # We are getting the predicted output from our model\n",
    "    if use_gpu:\n",
    "        val,predicted_id = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "    else:\n",
    "        val,predicted_id = model(dwords, chars2_mask, chars2_length, d)\n",
    "\n",
    "    pred_chunks = get_chunks(predicted_id,tag_to_id)\n",
    "    temp_list_tags=['NA']*len(words)\n",
    "    for p in pred_chunks:\n",
    "        temp_list_tags[p[1]]=p[0]\n",
    "        \n",
    "    for word,tag in zip(words,temp_list_tags):\n",
    "        print(word,':',tag)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zPTiqblpgGv"
   },
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBsodPyvpgGv"
   },
   "source": [
    "1) Xuezhe Ma and Eduard Hovy. 2016. ** End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF .** In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers). Association for Computational Linguistics, Berlin, Germany ** (https://arxiv.org/pdf/1603.01354.pdf) **\n",
    "\n",
    "2) Official PyTorch Tutorial : [** Advanced: Making Dynamic Decisions and the Bi-LSTM CRF **](http://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#sphx-glr-beginner-nlp-advanced-tutorial-py)\n",
    "\n",
    "3) [** Sequence Tagging with Tensorflow **](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html)  using bi-LSTM + CRF with character embeddings for NER and POS by Guillaume Genthial\n",
    "\n",
    "4) Github Repository - [** Reference Github Repository **](https://github.com/jayavardhanr/End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-Tutorial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1606363743412,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "5mD5SrUmpgGv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1606363743413,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "-werp2I6pgGv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1606363743413,
     "user": {
      "displayName": "Wong jun xiang",
      "photoUrl": "",
      "userId": "16719805263086576204"
     },
     "user_tz": -480
    },
    "id": "w5QErDSDpgGv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "q2 - 3 cnn layer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('ai_venv')",
   "language": "python",
   "name": "python38364bitaivenv118f08e36cf84beba32e8b69d70cc570"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}